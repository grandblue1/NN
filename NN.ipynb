{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T15:32:30.550598Z",
     "start_time": "2024-07-09T15:32:30.228032Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "# Example usage:\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # Subtract max for numerical stability\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "def mean_squared_error(predictions, targets):\n",
    "    return np.mean((predictions - targets) ** 2)\n",
    "\n",
    "def mean_squared_error_derivative(predictions, targets):\n",
    "    return 2 * (predictions - targets) / targets.size\n",
    "def binary_cross_entropy(output,target):\n",
    "    output = np.clip(output, 1e-9, 1 - 1e-9)\n",
    "    return -np.sum([y*np.log(p) + (1-y)*np.log(1-p) for y,p in zip(target,output)])\n",
    "def categorical_cross_entropy(predictions, targets):\n",
    "    predictions = np.clip(predictions, 1e-9, 1 - 1e-9)\n",
    "    return -np.sum(targets * np.log(predictions)) / targets.shape[0]\n",
    "def cross_entropy_derivative(output, target):\n",
    "    return output - target  # When combined with cross-entropy loss, this simplifies to this form\n",
    "\n",
    "\"\"\"\n",
    "Инициализация весов с использованием He Initialization.\n",
    "\n",
    "Аргументы:\n",
    "shape -- кортеж, определяющий размерность матрицы весов (например, (n_l, n_{l-1}))\n",
    "\n",
    "Возвращает:\n",
    "weights -- инициализированная матрица весов с размером shape, где weights(a, b), a - число нейронов в слое, b - число входов с предыдущего слоя\n",
    "\"\"\"\n",
    "def he_initialization(shape):\n",
    "    if type(shape) == int  or type(shape) == np.int64:\n",
    "        stddev = np.sqrt(2.0 / shape)\n",
    "        weights = np.random.randn(shape) * stddev\n",
    "    else:\n",
    "        stddev = np.sqrt(2.0 / shape[1])\n",
    "        weights = np.random.randn(*shape) * stddev\n",
    "\n",
    "    return weights\n",
    "def xavier_initialization(shape):\n",
    "    if type(shape) == int or type(shape) == np.int64:\n",
    "        stddev = np.sqrt(6.0)/shape\n",
    "        weights = np.random.randn(shape) * stddev\n",
    "    else:\n",
    "        stddev = np.sqrt(6.0) / np.sqrt(shape[0] + shape[1])\n",
    "        weights = np.random.randn(*shape) * stddev\n",
    "    return weights\n",
    "\n",
    "def leCun_initialization(shape):\n",
    "    if type(shape) == int or type(shape) == np.int64:\n",
    "        stddev = np.sqrt(1.0 / shape)\n",
    "        weights = np.random.randn(shape) * stddev\n",
    "    else:\n",
    "        stddev = np.sqrt(1.0 / shape[1])\n",
    "        weights = np.random.randn(*shape) * stddev\n",
    "    return weights"
   ],
   "id": "76fb398706dc7aab",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-09T15:32:30.590633Z",
     "start_time": "2024-07-09T15:32:30.552506Z"
    }
   },
   "source": [
    "import numpy as np \n",
    "class Layer:\n",
    "    def __init__(self, input_size, output_size,activation=None,derivative=None,initialize_function=np.random.randn,batch_norm=False, momentum=0.9, epsilon=1e-5):\n",
    "        self.init = initialize_function\n",
    "        self.weights = self.init(input_size,output_size)\n",
    "        self.bias = self.init(1,output_size)\n",
    "        self.input = None  \n",
    "        self.z = None \n",
    "        self.output = None \n",
    "        self.activation = activation\n",
    "        self.derivative = derivative\n",
    "        self.error = None\n",
    "        self.batch_norm = batch_norm\n",
    "        # Adding Batch Normalization parameters\n",
    "        if self.batch_norm:\n",
    "            self.momentum = momentum\n",
    "            self.gamma =  np.ones((1,output_size))\n",
    "            self.beta = np.zeros((1,output_size))\n",
    "            self.running_mean = np.zeros(output_size)\n",
    "            self.running_var = np.ones(output_size)\n",
    "            self.cache = None\n",
    "        # Adding adam optimization parameters\n",
    "        self.m_w, self.v_w = np.zeros_like(self.weights), np.zeros_like(self.weights)\n",
    "        self.m_b, self.v_b = np.zeros_like(self.bias), np.zeros_like(self.bias)\n",
    "        self.beta1, self.beta2 = 0.9,0.99\n",
    "        self.t = 0\n",
    "        self.epsilon = epsilon\n",
    "    def __call__(self,X,train=True):\n",
    "        self.input = X \n",
    "        self.z = np.dot(X, self.weights) + self.bias\n",
    "        if self.batch_norm:\n",
    "            if train:\n",
    "                mean = np.mean(self.z, axis=0)\n",
    "                var = np.var(self.z, axis=0)\n",
    "                z_norm = (self.z - mean) / np.sqrt(var + self.epsilon)\n",
    "                self.output = self.gamma * z_norm + self.beta\n",
    "                self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * mean\n",
    "                self.running_var = self.momentum * self.running_var + (1 - self.momentum) * var\n",
    "                self.cache = (self.z, z_norm, mean, var)\n",
    "            else:\n",
    "                self.z_norm = (self.z - self.running_mean) / np.sqrt(self.running_var + self.epsilon)\n",
    "                self.output = self.gamma * self.z_norm + self.beta  \n",
    "        else:\n",
    "            self.output = self.activation(self.z)\n",
    "        return self.output \n",
    "    def backward(self,error, learning_rate,L2,adam_optimize,clipping_mode='norm',clip_threshold=1.0):\n",
    "        self.error = error *  self.derivative(self.z)\n",
    "        self.t += 1\n",
    "        penalty = self.L2(L2,self.weights) if L2 is not None else 0\n",
    "        self.gradient_clipping(clipping_mode,clip_threshold)\n",
    "        if self.batch_norm:\n",
    "            self.error, self.gamma,self.beta = self.batch_normalization(self.error,self.gamma, self.beta,learning_rate)\n",
    "        if adam_optimize: \n",
    "            self.adam(learning_rate)\n",
    "        else:\n",
    "            self.weights -= learning_rate * (np.dot(self.input.T, self.error) + penalty)\n",
    "            self.bias -= learning_rate * np.sum(self.error,axis=0,keepdims=True)\n",
    "\n",
    "        propagated_error = np.dot(self.error, self.weights.T)\n",
    "        return propagated_error\n",
    "\n",
    "    def adam(self, learning_rate):\n",
    "        self.m_w = self.beta1 * self.m_w + (1 - self.beta1) * np.dot(self.input.T, self.error)\n",
    "        self.v_w = self.beta2 * self.v_w + (1 - self.beta2) * (np.dot(self.input.T, self.error) ** 2)\n",
    "        m_w_hat = self.m_w / (1 - self.beta1 ** self.t)\n",
    "        v_w_hat = self.v_w / (1 - self.beta2 ** self.t)\n",
    "        self.weights -= learning_rate * m_w_hat / (np.sqrt(v_w_hat) + self.epsilon)\n",
    "\n",
    "        self.m_b = self.beta1 * self.m_b + (1 - self.beta1) * np.sum(self.error, axis=0, keepdims=True)\n",
    "        self.v_b = self.beta2 * self.v_b + (1 - self.beta2) * (np.sum(self.error, axis=0, keepdims=True) ** 2)\n",
    "        m_b_hat = self.m_b / (1 - self.beta1 ** self.t)\n",
    "        v_b_hat = self.v_b / (1 - self.beta2 ** self.t)\n",
    "        self.bias -= learning_rate * m_b_hat / (np.sqrt(v_b_hat) + self.epsilon)\n",
    "    def gradient_clipping(self,mode,clip_threshold):\n",
    "        if mode == 'value':\n",
    "            self.error = np.clip(self.error, -clip_threshold, clip_threshold)\n",
    "        elif mode == 'norm':\n",
    "            norm = np.linalg.norm(self.error)\n",
    "            if norm > clip_threshold:\n",
    "                self.error = self.error / norm * clip_threshold\n",
    "        else:\n",
    "            return  \n",
    "    def batch_normalization(self,error,gamma,beta, learning_rate):\n",
    "        z , z_norm , mean , var = self.cache\n",
    "        m = z.shape[0]\n",
    "        dbeta = np.sum(error,axis=0)\n",
    "        dgamma = np.sum(error * z_norm,axis=0)\n",
    "        dz_norm = error * self.gamma\n",
    "        dvar = np.sum(dz_norm * (z - mean) * -0.5 * (var + self.epsilon)**-1.5, axis=0)\n",
    "        dmean = np.sum(dz_norm * -1.0 / np.sqrt(var + self.epsilon), axis=0) + dvar * np.mean(-2.0 * (z - mean), axis=0)\n",
    "\n",
    "        error = dz_norm * 1.0 / np.sqrt(var + self.epsilon) + dvar * 2.0 * (z - mean) / m + dmean / m\n",
    "        gamma -= learning_rate * dgamma\n",
    "        beta -= learning_rate * dbeta\n",
    "        return error, gamma, beta \n",
    "    def L2(self, C, weights):\n",
    "        return 2/C * weights\n",
    "    def set_initialization(self,func):\n",
    "        self.init = func\n",
    "class NeuralNetwork:\n",
    "    def __init__(self,sizes,activation_func=lambda x : x, derivative_func=lambda x: 1,init_func=np.random.randn,use_batch_norm=True):\n",
    "        self.layers = [\n",
    "            Layer(input_size=input,output_size=output,activation=activation_func,derivative=derivative_func,initialize_function=init_func,batch_norm=use_batch_norm) \n",
    "            for input,output\n",
    "            in zip(sizes[:-1],sizes[1:])\n",
    "        ]\n",
    "        self.loss = None\n",
    "        self.loss_derivative = None \n",
    "    def forward(self,X):\n",
    "        for layer in self.layers:\n",
    "            X = layer(X)\n",
    "        return X \n",
    "    def backward(self,error,learning_rate,L2,adam_optimizer,clipping_mode,clipping_threshold):\n",
    "        for layer in reversed(self.layers):\n",
    "            error = layer.backward(error,learning_rate,L2,adam_optimizer,clipping_mode,clipping_threshold)\n",
    "            \n",
    "    def train(self,X,y,*,epochs=100, learning_rate=10e-4,L2=None,adam_optimizer=False,clipping_mode='norm',clipping_threshold=1.0,batch_size=2):\n",
    "        rows = X.shape[0]\n",
    "        for epoch in range(epochs):\n",
    "            indices = np.random.permutation(rows)\n",
    "            X_permuted = X[indices]\n",
    "            y_permuted = y[indices]\n",
    "            for start in range(0,rows,batch_size):\n",
    "                end = min(start + batch_size, rows)\n",
    "                X_batch = X_permuted[start:end]\n",
    "                y_batch = y_permuted[start:end]\n",
    "                output = self.forward(X_batch)\n",
    "                error = self.loss_derivative(output,y_batch)\n",
    "                self.backward(error,learning_rate,L2,adam_optimizer,clipping_mode,clipping_threshold)\n",
    "            print(f\"Iteration: {epoch}, Loss : {self.loss(self.forward(X),y)}\")\n",
    "    def set_output_function(self,func,derivative):\n",
    "        self.layers[-1].activation =  func \n",
    "        self.layers[-1].derivative = derivative\n",
    "    def set_loss_function(self,func,derivative):\n",
    "        self.loss = func \n",
    "        self.loss_derivative = derivative\n",
    "    def predict(self,X):\n",
    "        for layer in self.layers:\n",
    "            X = layer(X,False)\n",
    "        return X "
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T15:32:32.073993Z",
     "start_time": "2024-07-09T15:32:30.592114Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "digits = load_digits()\n",
    "# One-hot encode the labels\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_onehot = encoder.fit_transform(digits.target.reshape(-1, 1))\n",
    "X_train,X_test,y_train,y_test = train_test_split(digits.data,y_onehot,test_size=0.2)"
   ],
   "id": "134ffaa41bf004aa",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T15:32:32.098333Z",
     "start_time": "2024-07-09T15:32:32.084614Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 80\n",
    "size = [64,128,32,10]\n",
    "nn = NeuralNetwork(size,relu,relu_derivative,lambda x,y: he_initialization((x,y)),use_batch_norm=True)\n",
    "nn.set_output_function(lambda x: softmax(x), lambda x : softmax(x))\n",
    "nn.set_loss_function(categorical_cross_entropy,lambda x,y : x - y)"
   ],
   "id": "acd412f984b693b6",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T15:32:37.187055Z",
     "start_time": "2024-07-09T15:32:32.100489Z"
    }
   },
   "cell_type": "code",
   "source": "nn.train(X_train,y_train,epochs=100,learning_rate=10e-3,L2=10,clipping_mode='norm',clipping_threshold=1.0,adam_optimizer=False,batch_size=batch_size)",
   "id": "46801ad6749ddb8c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Loss : 9.166242290594408\n",
      "Iteration: 1, Loss : 8.473189477169765\n",
      "Iteration: 2, Loss : 6.605324754474547\n",
      "Iteration: 3, Loss : 5.110574446929935\n",
      "Iteration: 4, Loss : 4.014747255618577\n",
      "Iteration: 5, Loss : 3.293910617415482\n",
      "Iteration: 6, Loss : 2.586001463442913\n",
      "Iteration: 7, Loss : 1.8538514958688006\n",
      "Iteration: 8, Loss : 1.32215991614892\n",
      "Iteration: 9, Loss : 1.0323369398956033\n",
      "Iteration: 10, Loss : 0.8832872392606053\n",
      "Iteration: 11, Loss : 0.6940659999341247\n",
      "Iteration: 12, Loss : 0.5710375191639917\n",
      "Iteration: 13, Loss : 0.4548438873580994\n",
      "Iteration: 14, Loss : 0.4179192875356024\n",
      "Iteration: 15, Loss : 0.3628085792230621\n",
      "Iteration: 16, Loss : 0.334867753076442\n",
      "Iteration: 17, Loss : 0.30468215169572377\n",
      "Iteration: 18, Loss : 0.2963902767562119\n",
      "Iteration: 19, Loss : 0.2825582811608798\n",
      "Iteration: 20, Loss : 0.27594984193919386\n",
      "Iteration: 21, Loss : 0.2732331174981217\n",
      "Iteration: 22, Loss : 0.26401365927796644\n",
      "Iteration: 23, Loss : 0.25039026376572904\n",
      "Iteration: 24, Loss : 0.23616041295586274\n",
      "Iteration: 25, Loss : 0.2326497839994657\n",
      "Iteration: 26, Loss : 0.2233742539746966\n",
      "Iteration: 27, Loss : 0.21201075217236007\n",
      "Iteration: 28, Loss : 0.22445971431823453\n",
      "Iteration: 29, Loss : 0.23194709070662603\n",
      "Iteration: 30, Loss : 0.23513830746013634\n",
      "Iteration: 31, Loss : 0.2581521327334039\n",
      "Iteration: 32, Loss : 0.22750679870549903\n",
      "Iteration: 33, Loss : 0.2659146202547387\n",
      "Iteration: 34, Loss : 0.23759570897136914\n",
      "Iteration: 35, Loss : 0.2348954500742056\n",
      "Iteration: 36, Loss : 0.25551911617431783\n",
      "Iteration: 37, Loss : 0.23371801107339552\n",
      "Iteration: 38, Loss : 0.2446353990737516\n",
      "Iteration: 39, Loss : 0.28681126630314685\n",
      "Iteration: 40, Loss : 0.2835578300503432\n",
      "Iteration: 41, Loss : 0.26594952098600544\n",
      "Iteration: 42, Loss : 0.23825491113672256\n",
      "Iteration: 43, Loss : 0.24588060982380675\n",
      "Iteration: 44, Loss : 0.2500393113803331\n",
      "Iteration: 45, Loss : 0.275463945508187\n",
      "Iteration: 46, Loss : 0.2649871129100483\n",
      "Iteration: 47, Loss : 0.2773839380982207\n",
      "Iteration: 48, Loss : 0.2686207212730513\n",
      "Iteration: 49, Loss : 0.27532411827856557\n",
      "Iteration: 50, Loss : 0.26475020132152555\n",
      "Iteration: 51, Loss : 0.277089277282692\n",
      "Iteration: 52, Loss : 0.27690134853839427\n",
      "Iteration: 53, Loss : 0.2882860128946717\n",
      "Iteration: 54, Loss : 0.28931101534606446\n",
      "Iteration: 55, Loss : 0.2790297933067381\n",
      "Iteration: 56, Loss : 0.2935321497615269\n",
      "Iteration: 57, Loss : 0.2788948379947632\n",
      "Iteration: 58, Loss : 0.3096432869051509\n",
      "Iteration: 59, Loss : 0.272594906618026\n",
      "Iteration: 60, Loss : 0.28609851061211505\n",
      "Iteration: 61, Loss : 0.27865004450581704\n",
      "Iteration: 62, Loss : 0.3079915889236344\n",
      "Iteration: 63, Loss : 0.2999957996039659\n",
      "Iteration: 64, Loss : 0.29148101042596425\n",
      "Iteration: 65, Loss : 0.2855941094143447\n",
      "Iteration: 66, Loss : 0.3054203188182131\n",
      "Iteration: 67, Loss : 0.3346153697029159\n",
      "Iteration: 68, Loss : 0.3015827683967636\n",
      "Iteration: 69, Loss : 0.3234202807318056\n",
      "Iteration: 70, Loss : 0.32452935918805226\n",
      "Iteration: 71, Loss : 0.32234073087689213\n",
      "Iteration: 72, Loss : 0.2947902638812792\n",
      "Iteration: 73, Loss : 0.3067724895852787\n",
      "Iteration: 74, Loss : 0.30221872516549114\n",
      "Iteration: 75, Loss : 0.30399553888181785\n",
      "Iteration: 76, Loss : 0.3046315661854379\n",
      "Iteration: 77, Loss : 0.3037857930808038\n",
      "Iteration: 78, Loss : 0.30988054362462747\n",
      "Iteration: 79, Loss : 0.3355645298987732\n",
      "Iteration: 80, Loss : 0.34566000095224486\n",
      "Iteration: 81, Loss : 0.305854051409531\n",
      "Iteration: 82, Loss : 0.3499644810571067\n",
      "Iteration: 83, Loss : 0.3344335471542447\n",
      "Iteration: 84, Loss : 0.3216300083935229\n",
      "Iteration: 85, Loss : 0.3259916746827411\n",
      "Iteration: 86, Loss : 0.31983149499562086\n",
      "Iteration: 87, Loss : 0.35084033790802877\n",
      "Iteration: 88, Loss : 0.3163066021674309\n",
      "Iteration: 89, Loss : 0.33028259477099037\n",
      "Iteration: 90, Loss : 0.30542031030655636\n",
      "Iteration: 91, Loss : 0.33069236521957057\n",
      "Iteration: 92, Loss : 0.3214796932982926\n",
      "Iteration: 93, Loss : 0.3289785649620794\n",
      "Iteration: 94, Loss : 0.28126559142953145\n",
      "Iteration: 95, Loss : 0.3087310945791986\n",
      "Iteration: 96, Loss : 0.325572208322272\n",
      "Iteration: 97, Loss : 0.31644057342848453\n",
      "Iteration: 98, Loss : 0.30506734790352363\n",
      "Iteration: 99, Loss : 0.32657493698187556\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T15:32:37.208939Z",
     "start_time": "2024-07-09T15:32:37.192471Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "# Forward method to get predictions\n",
    "predictions = nn.predict(X_test)\n",
    "# If predictions are in the form of probabilities, convert them to class labels\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "true_labels = np.argmax(y_test, axis=1)\n",
    "predicted_labels"
   ],
   "id": "7825a8083ff57713",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 5, 8, 4, 2, 0, 0, 2, 8, 3, 6, 7, 6, 9, 2, 8, 5, 8, 1, 8, 5, 5,\n",
       "       9, 8, 2, 6, 7, 4, 9, 3, 0, 0, 4, 1, 9, 8, 0, 1, 8, 2, 0, 2, 1, 5,\n",
       "       9, 6, 4, 9, 8, 9, 4, 6, 4, 2, 4, 0, 9, 5, 3, 6, 4, 7, 7, 0, 4, 3,\n",
       "       7, 8, 3, 6, 1, 5, 3, 9, 5, 9, 6, 4, 2, 3, 3, 5, 0, 5, 5, 8, 8, 2,\n",
       "       6, 6, 5, 0, 8, 9, 5, 4, 2, 6, 1, 1, 2, 5, 7, 4, 5, 0, 2, 8, 9, 4,\n",
       "       0, 2, 6, 5, 1, 9, 7, 5, 8, 2, 0, 9, 1, 3, 8, 4, 7, 8, 9, 5, 0, 7,\n",
       "       1, 2, 5, 1, 5, 4, 2, 8, 2, 6, 9, 5, 4, 6, 6, 8, 0, 8, 1, 2, 6, 3,\n",
       "       6, 0, 3, 9, 8, 0, 7, 0, 2, 6, 5, 4, 1, 3, 9, 1, 0, 9, 6, 1, 9, 4,\n",
       "       3, 0, 0, 2, 9, 3, 2, 6, 9, 5, 3, 0, 4, 1, 0, 1, 5, 4, 7, 2, 2, 1,\n",
       "       5, 1, 1, 0, 7, 0, 1, 6, 9, 8, 9, 4, 6, 8, 5, 1, 9, 3, 4, 7, 2, 4,\n",
       "       6, 5, 6, 1, 0, 2, 7, 7, 1, 7, 5, 4, 0, 6, 3, 7, 2, 8, 2, 8, 4, 8,\n",
       "       1, 3, 1, 8, 1, 9, 6, 5, 2, 2, 4, 4, 8, 4, 5, 4, 1, 8, 9, 1, 4, 2,\n",
       "       5, 9, 9, 0, 9, 1, 8, 7, 1, 3, 3, 5, 3, 1, 4, 8, 0, 8, 8, 9, 6, 8,\n",
       "       7, 2, 3, 0, 2, 7, 4, 6, 7, 7, 1, 4, 9, 5, 6, 0, 3, 2, 3, 9, 4, 8,\n",
       "       2, 7, 7, 4, 0, 8, 9, 7, 1, 8, 1, 0, 9, 7, 7, 7, 4, 9, 8, 0, 6, 1,\n",
       "       1, 0, 2, 4, 2, 4, 8, 7, 5, 8, 1, 7, 8, 7, 9, 5, 6, 4, 0, 5, 0, 3,\n",
       "       4, 3, 3, 6, 8, 0, 8, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T15:32:37.241307Z",
     "start_time": "2024-07-09T15:32:37.212013Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score,precision_score, recall_score, f1_score, confusion_matrix\n",
    "print(f\"Accuracy score: {accuracy_score(true_labels,predicted_labels)}\")\n",
    "print(precision_score(true_labels, predicted_labels, average='macro'))\n",
    "print(recall_score(true_labels, predicted_labels, average='macro'))\n",
    "print(confusion_matrix(true_labels, predicted_labels))\n",
    "print(f1_score(true_labels, predicted_labels, average='macro'))"
   ],
   "id": "be6dde8e05d1d5a3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.9305555555555556\n",
      "0.9338556115747453\n",
      "0.9381268521394572\n",
      "[[37  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 37  3  0  0  1  1  1  6  0]\n",
      " [ 0  0 33  1  0  0  0  0  0  0]\n",
      " [ 0  0  0 27  0  0  0  0  3  0]\n",
      " [ 0  0  0  0 39  0  0  1  0  0]\n",
      " [ 0  0  0  0  0 33  0  0  0  1]\n",
      " [ 0  0  0  0  1  0 32  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 29  0  0]\n",
      " [ 0  0  0  0  0  1  0  0 33  1]\n",
      " [ 1  1  0  0  0  1  0  0  1 35]]\n",
      "0.9332560261879681\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T15:32:37.752990Z",
     "start_time": "2024-07-09T15:32:37.244249Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from scipy.special import softmax\n",
    "# Define network architecture\n",
    "batch_size = 2\n",
    "sizes = [batch_size, 5,1]\n",
    "\n",
    "# Initialize network\n",
    "nn = NeuralNetwork(sizes, sigmoid, sigmoid_derivative,init_func=lambda x,y: xavier_initialization((x,y)),use_batch_norm=False)\n",
    "nn.set_loss_function(mean_squared_error, mean_squared_error_derivative)\n",
    "nn.set_output_function(sigmoid,sigmoid_derivative)\n",
    "\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y = np.array([[0],[0],[0],[1]])\n",
    "nn.train(X,y,epochs=1000,learning_rate=10e-1,batch_size=2)\n",
    "nn.forward(X)"
   ],
   "id": "11b0b19ca766da1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Loss : 0.17365165629579088\n",
      "Iteration: 1, Loss : 0.16950767810084916\n",
      "Iteration: 2, Loss : 0.16802372624373893\n",
      "Iteration: 3, Loss : 0.1669934536768269\n",
      "Iteration: 4, Loss : 0.16583306643634277\n",
      "Iteration: 5, Loss : 0.1645621249188478\n",
      "Iteration: 6, Loss : 0.16365232057875173\n",
      "Iteration: 7, Loss : 0.16268906115951975\n",
      "Iteration: 8, Loss : 0.1611837873494466\n",
      "Iteration: 9, Loss : 0.15992722724292224\n",
      "Iteration: 10, Loss : 0.15860665460252166\n",
      "Iteration: 11, Loss : 0.15722153304274658\n",
      "Iteration: 12, Loss : 0.1556069954899762\n",
      "Iteration: 13, Loss : 0.15443524874995673\n",
      "Iteration: 14, Loss : 0.15292521699315334\n",
      "Iteration: 15, Loss : 0.1516594103141545\n",
      "Iteration: 16, Loss : 0.14996873680961148\n",
      "Iteration: 17, Loss : 0.14859373539554263\n",
      "Iteration: 18, Loss : 0.14652944633324208\n",
      "Iteration: 19, Loss : 0.14540752415828898\n",
      "Iteration: 20, Loss : 0.14421491038320589\n",
      "Iteration: 21, Loss : 0.14217613626472303\n",
      "Iteration: 22, Loss : 0.1409211365640746\n",
      "Iteration: 23, Loss : 0.13837014246008988\n",
      "Iteration: 24, Loss : 0.13646769134939996\n",
      "Iteration: 25, Loss : 0.13488554275606923\n",
      "Iteration: 26, Loss : 0.1329528836727663\n",
      "Iteration: 27, Loss : 0.1311769273987098\n",
      "Iteration: 28, Loss : 0.12953036029479212\n",
      "Iteration: 29, Loss : 0.12786892043116488\n",
      "Iteration: 30, Loss : 0.12602654318816153\n",
      "Iteration: 31, Loss : 0.12415955471055248\n",
      "Iteration: 32, Loss : 0.12227511956410869\n",
      "Iteration: 33, Loss : 0.12001818862496702\n",
      "Iteration: 34, Loss : 0.1181490295108687\n",
      "Iteration: 35, Loss : 0.11622720422521024\n",
      "Iteration: 36, Loss : 0.11431596996391383\n",
      "Iteration: 37, Loss : 0.11245490889474223\n",
      "Iteration: 38, Loss : 0.11053890819092858\n",
      "Iteration: 39, Loss : 0.10863416652226843\n",
      "Iteration: 40, Loss : 0.10684445155442522\n",
      "Iteration: 41, Loss : 0.10480282950921715\n",
      "Iteration: 42, Loss : 0.1029643020579691\n",
      "Iteration: 43, Loss : 0.10119513864695079\n",
      "Iteration: 44, Loss : 0.09917578713844538\n",
      "Iteration: 45, Loss : 0.09732648507249597\n",
      "Iteration: 46, Loss : 0.09556977375553807\n",
      "Iteration: 47, Loss : 0.09358151575142956\n",
      "Iteration: 48, Loss : 0.0918481600013468\n",
      "Iteration: 49, Loss : 0.08983994487489763\n",
      "Iteration: 50, Loss : 0.08816225327844858\n",
      "Iteration: 51, Loss : 0.08626849887288088\n",
      "Iteration: 52, Loss : 0.08482483698989798\n",
      "Iteration: 53, Loss : 0.08302728248133368\n",
      "Iteration: 54, Loss : 0.08089979043964839\n",
      "Iteration: 55, Loss : 0.07927258504279255\n",
      "Iteration: 56, Loss : 0.07754862091415582\n",
      "Iteration: 57, Loss : 0.07621723957984439\n",
      "Iteration: 58, Loss : 0.07497591536578152\n",
      "Iteration: 59, Loss : 0.07297650225504448\n",
      "Iteration: 60, Loss : 0.07102697903681057\n",
      "Iteration: 61, Loss : 0.0692106841761676\n",
      "Iteration: 62, Loss : 0.06780131208098011\n",
      "Iteration: 63, Loss : 0.06618684309377769\n",
      "Iteration: 64, Loss : 0.0648216916719255\n",
      "Iteration: 65, Loss : 0.06355297471188404\n",
      "Iteration: 66, Loss : 0.06223774133939606\n",
      "Iteration: 67, Loss : 0.06087450446404888\n",
      "Iteration: 68, Loss : 0.05893524214504033\n",
      "Iteration: 69, Loss : 0.057784336426394425\n",
      "Iteration: 70, Loss : 0.056167540279757185\n",
      "Iteration: 71, Loss : 0.05505889132936677\n",
      "Iteration: 72, Loss : 0.053562950268127504\n",
      "Iteration: 73, Loss : 0.05233564661946876\n",
      "Iteration: 74, Loss : 0.05117149484624223\n",
      "Iteration: 75, Loss : 0.04990949468881452\n",
      "Iteration: 76, Loss : 0.04871047450864401\n",
      "Iteration: 77, Loss : 0.04767369434285708\n",
      "Iteration: 78, Loss : 0.046711861354508305\n",
      "Iteration: 79, Loss : 0.0454691518259865\n",
      "Iteration: 80, Loss : 0.04450935389260941\n",
      "Iteration: 81, Loss : 0.043354003883437034\n",
      "Iteration: 82, Loss : 0.042507267501811066\n",
      "Iteration: 83, Loss : 0.041364281887033005\n",
      "Iteration: 84, Loss : 0.0405317869111654\n",
      "Iteration: 85, Loss : 0.039606862299680315\n",
      "Iteration: 86, Loss : 0.03873627974504147\n",
      "Iteration: 87, Loss : 0.03788679740889625\n",
      "Iteration: 88, Loss : 0.03705269051194553\n",
      "Iteration: 89, Loss : 0.036375683255651534\n",
      "Iteration: 90, Loss : 0.035404681611847386\n",
      "Iteration: 91, Loss : 0.03469057732790741\n",
      "Iteration: 92, Loss : 0.034031562013068846\n",
      "Iteration: 93, Loss : 0.033200880539592706\n",
      "Iteration: 94, Loss : 0.03248986153376688\n",
      "Iteration: 95, Loss : 0.031830343300071264\n",
      "Iteration: 96, Loss : 0.031187438293196718\n",
      "Iteration: 97, Loss : 0.030516479898223627\n",
      "Iteration: 98, Loss : 0.02988477953832049\n",
      "Iteration: 99, Loss : 0.029322997625891196\n",
      "Iteration: 100, Loss : 0.028708711490695725\n",
      "Iteration: 101, Loss : 0.028162043594068913\n",
      "Iteration: 102, Loss : 0.027535781026220337\n",
      "Iteration: 103, Loss : 0.026996944194969986\n",
      "Iteration: 104, Loss : 0.02650802588568127\n",
      "Iteration: 105, Loss : 0.026031216925191676\n",
      "Iteration: 106, Loss : 0.025415152540828455\n",
      "Iteration: 107, Loss : 0.02494098241202642\n",
      "Iteration: 108, Loss : 0.024479525949784926\n",
      "Iteration: 109, Loss : 0.0240521661789378\n",
      "Iteration: 110, Loss : 0.023585626973097833\n",
      "Iteration: 111, Loss : 0.02310799055732051\n",
      "Iteration: 112, Loss : 0.022696275442816938\n",
      "Iteration: 113, Loss : 0.022254551512985263\n",
      "Iteration: 114, Loss : 0.02187242896057811\n",
      "Iteration: 115, Loss : 0.02146990464168545\n",
      "Iteration: 116, Loss : 0.0211081390694059\n",
      "Iteration: 117, Loss : 0.020758176153844003\n",
      "Iteration: 118, Loss : 0.02037479919049309\n",
      "Iteration: 119, Loss : 0.02002601667285075\n",
      "Iteration: 120, Loss : 0.019684861059238404\n",
      "Iteration: 121, Loss : 0.01931865516852182\n",
      "Iteration: 122, Loss : 0.018997649170863773\n",
      "Iteration: 123, Loss : 0.018684416600226524\n",
      "Iteration: 124, Loss : 0.01837114536930965\n",
      "Iteration: 125, Loss : 0.018074337598081136\n",
      "Iteration: 126, Loss : 0.017766951856417273\n",
      "Iteration: 127, Loss : 0.017468219027532095\n",
      "Iteration: 128, Loss : 0.0171853433378138\n",
      "Iteration: 129, Loss : 0.016908232285545492\n",
      "Iteration: 130, Loss : 0.01664228526354528\n",
      "Iteration: 131, Loss : 0.016373347971776406\n",
      "Iteration: 132, Loss : 0.016119020495986354\n",
      "Iteration: 133, Loss : 0.01588021688330106\n",
      "Iteration: 134, Loss : 0.015641759917965098\n",
      "Iteration: 135, Loss : 0.015407148811351946\n",
      "Iteration: 136, Loss : 0.015176852100817833\n",
      "Iteration: 137, Loss : 0.014951932929451357\n",
      "Iteration: 138, Loss : 0.014732178398515865\n",
      "Iteration: 139, Loss : 0.01449722173369909\n",
      "Iteration: 140, Loss : 0.01426791733354172\n",
      "Iteration: 141, Loss : 0.014064993625617523\n",
      "Iteration: 142, Loss : 0.013870033146118343\n",
      "Iteration: 143, Loss : 0.013679101509084433\n",
      "Iteration: 144, Loss : 0.01347992613062654\n",
      "Iteration: 145, Loss : 0.013281624387299143\n",
      "Iteration: 146, Loss : 0.013098793479685435\n",
      "Iteration: 147, Loss : 0.01291768402023253\n",
      "Iteration: 148, Loss : 0.01274533429583492\n",
      "Iteration: 149, Loss : 0.012571765059806351\n",
      "Iteration: 150, Loss : 0.012402744799285246\n",
      "Iteration: 151, Loss : 0.012238176923654587\n",
      "Iteration: 152, Loss : 0.012074933321650464\n",
      "Iteration: 153, Loss : 0.01191047053180501\n",
      "Iteration: 154, Loss : 0.011751102983970418\n",
      "Iteration: 155, Loss : 0.011597368068295689\n",
      "Iteration: 156, Loss : 0.011448608385629253\n",
      "Iteration: 157, Loss : 0.011300200506447457\n",
      "Iteration: 158, Loss : 0.011157705834178954\n",
      "Iteration: 159, Loss : 0.011009222795424016\n",
      "Iteration: 160, Loss : 0.010870156969730526\n",
      "Iteration: 161, Loss : 0.010734026244657633\n",
      "Iteration: 162, Loss : 0.010602299393053481\n",
      "Iteration: 163, Loss : 0.010473849469091141\n",
      "Iteration: 164, Loss : 0.010347683348212252\n",
      "Iteration: 165, Loss : 0.010224410586605302\n",
      "Iteration: 166, Loss : 0.010102706096426165\n",
      "Iteration: 167, Loss : 0.009983207981725254\n",
      "Iteration: 168, Loss : 0.009866748872378171\n",
      "Iteration: 169, Loss : 0.009747047305689309\n",
      "Iteration: 170, Loss : 0.00963268245830468\n",
      "Iteration: 171, Loss : 0.009519329670648235\n",
      "Iteration: 172, Loss : 0.009411107669241841\n",
      "Iteration: 173, Loss : 0.009305081752319897\n",
      "Iteration: 174, Loss : 0.009200033261831084\n",
      "Iteration: 175, Loss : 0.009095047305069489\n",
      "Iteration: 176, Loss : 0.008995862057605006\n",
      "Iteration: 177, Loss : 0.00889591433728349\n",
      "Iteration: 178, Loss : 0.008797620964393342\n",
      "Iteration: 179, Loss : 0.008697254929808464\n",
      "Iteration: 180, Loss : 0.008596719499414807\n",
      "Iteration: 181, Loss : 0.008505690914422676\n",
      "Iteration: 182, Loss : 0.008416515700804136\n",
      "Iteration: 183, Loss : 0.00832244726881273\n",
      "Iteration: 184, Loss : 0.008232802972522175\n",
      "Iteration: 185, Loss : 0.008146117242119312\n",
      "Iteration: 186, Loss : 0.008061854735522758\n",
      "Iteration: 187, Loss : 0.007977213742043437\n",
      "Iteration: 188, Loss : 0.007895694308664842\n",
      "Iteration: 189, Loss : 0.007814464270714076\n",
      "Iteration: 190, Loss : 0.007735850178866823\n",
      "Iteration: 191, Loss : 0.007655875487299767\n",
      "Iteration: 192, Loss : 0.007579936142912001\n",
      "Iteration: 193, Loss : 0.007504507805497657\n",
      "Iteration: 194, Loss : 0.007428800575110233\n",
      "Iteration: 195, Loss : 0.007353357904192351\n",
      "Iteration: 196, Loss : 0.007281974023514878\n",
      "Iteration: 197, Loss : 0.007211983440389598\n",
      "Iteration: 198, Loss : 0.00714088217492502\n",
      "Iteration: 199, Loss : 0.00707046131601783\n",
      "Iteration: 200, Loss : 0.007003551494080457\n",
      "Iteration: 201, Loss : 0.006938123652093248\n",
      "Iteration: 202, Loss : 0.006870737036899829\n",
      "Iteration: 203, Loss : 0.006807010054620975\n",
      "Iteration: 204, Loss : 0.006744383374370663\n",
      "Iteration: 205, Loss : 0.0066813286986991864\n",
      "Iteration: 206, Loss : 0.006620449886013153\n",
      "Iteration: 207, Loss : 0.006560499377765349\n",
      "Iteration: 208, Loss : 0.006500507129803654\n",
      "Iteration: 209, Loss : 0.0064410680510672055\n",
      "Iteration: 210, Loss : 0.00638330934345119\n",
      "Iteration: 211, Loss : 0.006326914260986827\n",
      "Iteration: 212, Loss : 0.006271154044640297\n",
      "Iteration: 213, Loss : 0.0062148904095602284\n",
      "Iteration: 214, Loss : 0.006160925649301733\n",
      "Iteration: 215, Loss : 0.006105820181266922\n",
      "Iteration: 216, Loss : 0.006053445969674311\n",
      "Iteration: 217, Loss : 0.006001905352893794\n",
      "Iteration: 218, Loss : 0.005951199579156722\n",
      "Iteration: 219, Loss : 0.005901288320201524\n",
      "Iteration: 220, Loss : 0.005851004282758664\n",
      "Iteration: 221, Loss : 0.005799560996622852\n",
      "Iteration: 222, Loss : 0.0057498252001603935\n",
      "Iteration: 223, Loss : 0.005701310062544863\n",
      "Iteration: 224, Loss : 0.005653724113093662\n",
      "Iteration: 225, Loss : 0.005607010563863301\n",
      "Iteration: 226, Loss : 0.005560090373825004\n",
      "Iteration: 227, Loss : 0.0055154256104205\n",
      "Iteration: 228, Loss : 0.005471042809538432\n",
      "Iteration: 229, Loss : 0.0054274083316883225\n",
      "Iteration: 230, Loss : 0.005384389872768216\n",
      "Iteration: 231, Loss : 0.005340941050245914\n",
      "Iteration: 232, Loss : 0.005298278323842127\n",
      "Iteration: 233, Loss : 0.005256269689677483\n",
      "Iteration: 234, Loss : 0.005214742893644779\n",
      "Iteration: 235, Loss : 0.005173058069406444\n",
      "Iteration: 236, Loss : 0.0051333298414525915\n",
      "Iteration: 237, Loss : 0.0050930884212277115\n",
      "Iteration: 238, Loss : 0.005054384855466325\n",
      "Iteration: 239, Loss : 0.005016147271778258\n",
      "Iteration: 240, Loss : 0.004977556210002537\n",
      "Iteration: 241, Loss : 0.004940512120560637\n",
      "Iteration: 242, Loss : 0.004903919384567353\n",
      "Iteration: 243, Loss : 0.004867056488884814\n",
      "Iteration: 244, Loss : 0.004830799449533848\n",
      "Iteration: 245, Loss : 0.004794112808864749\n",
      "Iteration: 246, Loss : 0.0047591077607439195\n",
      "Iteration: 247, Loss : 0.004724654744242782\n",
      "Iteration: 248, Loss : 0.0046904793600485925\n",
      "Iteration: 249, Loss : 0.004656762793392361\n",
      "Iteration: 250, Loss : 0.0046235365620372875\n",
      "Iteration: 251, Loss : 0.004590606753005239\n",
      "Iteration: 252, Loss : 0.004558106704017498\n",
      "Iteration: 253, Loss : 0.00452601565204269\n",
      "Iteration: 254, Loss : 0.004494310042968653\n",
      "Iteration: 255, Loss : 0.004462954348925893\n",
      "Iteration: 256, Loss : 0.0044313187361973445\n",
      "Iteration: 257, Loss : 0.004400745557800571\n",
      "Iteration: 258, Loss : 0.0043704158823598335\n",
      "Iteration: 259, Loss : 0.00434054466397636\n",
      "Iteration: 260, Loss : 0.004311025511903019\n",
      "Iteration: 261, Loss : 0.004281868496972808\n",
      "Iteration: 262, Loss : 0.004252973911598087\n",
      "Iteration: 263, Loss : 0.004224450221039093\n",
      "Iteration: 264, Loss : 0.004196258701361541\n",
      "Iteration: 265, Loss : 0.004168390468521082\n",
      "Iteration: 266, Loss : 0.004140773404190265\n",
      "Iteration: 267, Loss : 0.004113518561209489\n",
      "Iteration: 268, Loss : 0.004086576156479183\n",
      "Iteration: 269, Loss : 0.004059953167571315\n",
      "Iteration: 270, Loss : 0.004033626376196444\n",
      "Iteration: 271, Loss : 0.00400753048381246\n",
      "Iteration: 272, Loss : 0.003981244669398258\n",
      "Iteration: 273, Loss : 0.003955711454942204\n",
      "Iteration: 274, Loss : 0.003930524832130466\n",
      "Iteration: 275, Loss : 0.00390512997920372\n",
      "Iteration: 276, Loss : 0.003880476320402274\n",
      "Iteration: 277, Loss : 0.0038554564380384173\n",
      "Iteration: 278, Loss : 0.003830878216296854\n",
      "Iteration: 279, Loss : 0.0038066234894170844\n",
      "Iteration: 280, Loss : 0.003783024023527902\n",
      "Iteration: 281, Loss : 0.0037596634722234824\n",
      "Iteration: 282, Loss : 0.00373654912117565\n",
      "Iteration: 283, Loss : 0.003713562406738229\n",
      "Iteration: 284, Loss : 0.0036904140595107196\n",
      "Iteration: 285, Loss : 0.0036676030782475614\n",
      "Iteration: 286, Loss : 0.0036454125724356126\n",
      "Iteration: 287, Loss : 0.0036235117300446717\n",
      "Iteration: 288, Loss : 0.0036014323959862172\n",
      "Iteration: 289, Loss : 0.003579625869972277\n",
      "Iteration: 290, Loss : 0.0035582816858836953\n",
      "Iteration: 291, Loss : 0.0035367865098417453\n",
      "Iteration: 292, Loss : 0.0035159932119464684\n",
      "Iteration: 293, Loss : 0.0034954081254642954\n",
      "Iteration: 294, Loss : 0.0034750074780837545\n",
      "Iteration: 295, Loss : 0.003454445179592498\n",
      "Iteration: 296, Loss : 0.003434433166625452\n",
      "Iteration: 297, Loss : 0.0034142881868891614\n",
      "Iteration: 298, Loss : 0.003394740577013664\n",
      "Iteration: 299, Loss : 0.003375396155130531\n",
      "Iteration: 300, Loss : 0.0033562577656579533\n",
      "Iteration: 301, Loss : 0.003337190914621002\n",
      "Iteration: 302, Loss : 0.0033183318771414796\n",
      "Iteration: 303, Loss : 0.0032996716199196353\n",
      "Iteration: 304, Loss : 0.0032812408214347518\n",
      "Iteration: 305, Loss : 0.0032626869954216105\n",
      "Iteration: 306, Loss : 0.0032446256200172885\n",
      "Iteration: 307, Loss : 0.003226736542870723\n",
      "Iteration: 308, Loss : 0.003208880056648088\n",
      "Iteration: 309, Loss : 0.003191338881452014\n",
      "Iteration: 310, Loss : 0.00317396238081702\n",
      "Iteration: 311, Loss : 0.0031566078158830977\n",
      "Iteration: 312, Loss : 0.0031394433857079825\n",
      "Iteration: 313, Loss : 0.0031225494244174054\n",
      "Iteration: 314, Loss : 0.003105362514670887\n",
      "Iteration: 315, Loss : 0.003088776330821291\n",
      "Iteration: 316, Loss : 0.003072348380062731\n",
      "Iteration: 317, Loss : 0.003055983204914418\n",
      "Iteration: 318, Loss : 0.003039782887040384\n",
      "Iteration: 319, Loss : 0.003023741331298316\n",
      "Iteration: 320, Loss : 0.0030075702339437665\n",
      "Iteration: 321, Loss : 0.0029918507318619407\n",
      "Iteration: 322, Loss : 0.0029759921796606546\n",
      "Iteration: 323, Loss : 0.0029605726276562398\n",
      "Iteration: 324, Loss : 0.0029449749210051024\n",
      "Iteration: 325, Loss : 0.002929816238861094\n",
      "Iteration: 326, Loss : 0.0029147958736021147\n",
      "Iteration: 327, Loss : 0.002899911287976378\n",
      "Iteration: 328, Loss : 0.002885166747300786\n",
      "Iteration: 329, Loss : 0.00287056561805854\n",
      "Iteration: 330, Loss : 0.0028557532430448724\n",
      "Iteration: 331, Loss : 0.002841114809676219\n",
      "Iteration: 332, Loss : 0.00282686333308068\n",
      "Iteration: 333, Loss : 0.002812746525929516\n",
      "Iteration: 334, Loss : 0.0027985086194292335\n",
      "Iteration: 335, Loss : 0.0027846225376957096\n",
      "Iteration: 336, Loss : 0.002770858891785204\n",
      "Iteration: 337, Loss : 0.0027572169958608277\n",
      "Iteration: 338, Loss : 0.002743685188986622\n",
      "Iteration: 339, Loss : 0.002730279868097458\n",
      "Iteration: 340, Loss : 0.0027169898722566\n",
      "Iteration: 341, Loss : 0.002703805267008086\n",
      "Iteration: 342, Loss : 0.002690518970290631\n",
      "Iteration: 343, Loss : 0.002677367000505088\n",
      "Iteration: 344, Loss : 0.0026645023914482185\n",
      "Iteration: 345, Loss : 0.0026515800972417318\n",
      "Iteration: 346, Loss : 0.0026389632626201325\n",
      "Iteration: 347, Loss : 0.0026264069201162132\n",
      "Iteration: 348, Loss : 0.0026137506570513523\n",
      "Iteration: 349, Loss : 0.002601421047716722\n",
      "Iteration: 350, Loss : 0.002589202347459996\n",
      "Iteration: 351, Loss : 0.0025770780627438054\n",
      "Iteration: 352, Loss : 0.002565054003781344\n",
      "Iteration: 353, Loss : 0.002552955886805132\n",
      "Iteration: 354, Loss : 0.002541118129028793\n",
      "Iteration: 355, Loss : 0.0025293865225538854\n",
      "Iteration: 356, Loss : 0.0025175642377558774\n",
      "Iteration: 357, Loss : 0.002506025377710248\n",
      "Iteration: 358, Loss : 0.002494582243302967\n",
      "Iteration: 359, Loss : 0.002483236270685689\n",
      "Iteration: 360, Loss : 0.0024719654753341644\n",
      "Iteration: 361, Loss : 0.0024607864095611736\n",
      "Iteration: 362, Loss : 0.002449525279320028\n",
      "Iteration: 363, Loss : 0.0024385190180266835\n",
      "Iteration: 364, Loss : 0.0024274403816888163\n",
      "Iteration: 365, Loss : 0.002416616448233624\n",
      "Iteration: 366, Loss : 0.0024058691650123845\n",
      "Iteration: 367, Loss : 0.002395053825519645\n",
      "Iteration: 368, Loss : 0.002384334839493665\n",
      "Iteration: 369, Loss : 0.002373707724821218\n",
      "Iteration: 370, Loss : 0.0023632761189234545\n",
      "Iteration: 371, Loss : 0.002352936031043514\n",
      "Iteration: 372, Loss : 0.0023425156789816212\n",
      "Iteration: 373, Loss : 0.0023321909808189005\n",
      "Iteration: 374, Loss : 0.0023220937978446453\n",
      "Iteration: 375, Loss : 0.0023120684460893326\n",
      "Iteration: 376, Loss : 0.002301984580763334\n",
      "Iteration: 377, Loss : 0.0022921212984302435\n",
      "Iteration: 378, Loss : 0.002282180380574641\n",
      "Iteration: 379, Loss : 0.0022724659530941787\n",
      "Iteration: 380, Loss : 0.0022626786185056848\n",
      "Iteration: 381, Loss : 0.0022531082141875143\n",
      "Iteration: 382, Loss : 0.002243609070950745\n",
      "Iteration: 383, Loss : 0.002234180332937756\n",
      "Iteration: 384, Loss : 0.0022246961032842106\n",
      "Iteration: 385, Loss : 0.0022154016607088996\n",
      "Iteration: 386, Loss : 0.0022061602268872637\n",
      "Iteration: 387, Loss : 0.0021970061703015845\n",
      "Iteration: 388, Loss : 0.002187910456201366\n",
      "Iteration: 389, Loss : 0.0021788863992079663\n",
      "Iteration: 390, Loss : 0.0021699285906119836\n",
      "Iteration: 391, Loss : 0.0021610323271979304\n",
      "Iteration: 392, Loss : 0.0021522083008148458\n",
      "Iteration: 393, Loss : 0.002143445434115282\n",
      "Iteration: 394, Loss : 0.002134736800074057\n",
      "Iteration: 395, Loss : 0.002126086745537092\n",
      "Iteration: 396, Loss : 0.002117383359226645\n",
      "Iteration: 397, Loss : 0.0021088610680472133\n",
      "Iteration: 398, Loss : 0.0021003995995785294\n",
      "Iteration: 399, Loss : 0.002091892844961697\n",
      "Iteration: 400, Loss : 0.002083555920971856\n",
      "Iteration: 401, Loss : 0.002075264216748843\n",
      "Iteration: 402, Loss : 0.002067035674082952\n",
      "Iteration: 403, Loss : 0.002058867033807871\n",
      "Iteration: 404, Loss : 0.0020506542763677944\n",
      "Iteration: 405, Loss : 0.0020426045147198843\n",
      "Iteration: 406, Loss : 0.0020346103607349238\n",
      "Iteration: 407, Loss : 0.002026659547788619\n",
      "Iteration: 408, Loss : 0.0020186677714785096\n",
      "Iteration: 409, Loss : 0.002010738056300007\n",
      "Iteration: 410, Loss : 0.002002868127527842\n",
      "Iteration: 411, Loss : 0.0019950560791738414\n",
      "Iteration: 412, Loss : 0.0019872667555681854\n",
      "Iteration: 413, Loss : 0.0019796455242198025\n",
      "Iteration: 414, Loss : 0.0019720851249563492\n",
      "Iteration: 415, Loss : 0.001964565879708176\n",
      "Iteration: 416, Loss : 0.0019570143430206683\n",
      "Iteration: 417, Loss : 0.0019495904883213742\n",
      "Iteration: 418, Loss : 0.0019422324077462947\n",
      "Iteration: 419, Loss : 0.0019348352535108562\n",
      "Iteration: 420, Loss : 0.0019275562856034406\n",
      "Iteration: 421, Loss : 0.0019203319400990433\n",
      "Iteration: 422, Loss : 0.0019131647866627282\n",
      "Iteration: 423, Loss : 0.0019059635293727489\n",
      "Iteration: 424, Loss : 0.0018988118272043168\n",
      "Iteration: 425, Loss : 0.001891771009445609\n",
      "Iteration: 426, Loss : 0.0018847952562631122\n",
      "Iteration: 427, Loss : 0.0018777845767052718\n",
      "Iteration: 428, Loss : 0.0018708748940864952\n",
      "Iteration: 429, Loss : 0.0018639202314129817\n",
      "Iteration: 430, Loss : 0.0018570446532046943\n",
      "Iteration: 431, Loss : 0.0018502783285169063\n",
      "Iteration: 432, Loss : 0.0018435702226626707\n",
      "Iteration: 433, Loss : 0.0018368870615242305\n",
      "Iteration: 434, Loss : 0.0018302644233157755\n",
      "Iteration: 435, Loss : 0.0018236842047053637\n",
      "Iteration: 436, Loss : 0.001817041305594035\n",
      "Iteration: 437, Loss : 0.0018104486198595227\n",
      "Iteration: 438, Loss : 0.0018039041414983834\n",
      "Iteration: 439, Loss : 0.0017974840685551449\n",
      "Iteration: 440, Loss : 0.0017910282663585225\n",
      "Iteration: 441, Loss : 0.0017846870781215683\n",
      "Iteration: 442, Loss : 0.001778383098098271\n",
      "Iteration: 443, Loss : 0.0017721236443101463\n",
      "Iteration: 444, Loss : 0.0017659003609465508\n",
      "Iteration: 445, Loss : 0.0017597159204791823\n",
      "Iteration: 446, Loss : 0.0017535704073122307\n",
      "Iteration: 447, Loss : 0.0017473940963664372\n",
      "Iteration: 448, Loss : 0.001741252470107692\n",
      "Iteration: 449, Loss : 0.0017352202455162419\n",
      "Iteration: 450, Loss : 0.001729223864507762\n",
      "Iteration: 451, Loss : 0.001723266144789173\n",
      "Iteration: 452, Loss : 0.0017173452540873103\n",
      "Iteration: 453, Loss : 0.001711463132670112\n",
      "Iteration: 454, Loss : 0.0017056107916009924\n",
      "Iteration: 455, Loss : 0.001699797163306567\n",
      "Iteration: 456, Loss : 0.0016940162688365945\n",
      "Iteration: 457, Loss : 0.0016882705515015245\n",
      "Iteration: 458, Loss : 0.0016825626141923648\n",
      "Iteration: 459, Loss : 0.0016768900870806603\n",
      "Iteration: 460, Loss : 0.0016712523893025032\n",
      "Iteration: 461, Loss : 0.0016656489961966903\n",
      "Iteration: 462, Loss : 0.001660070270397314\n",
      "Iteration: 463, Loss : 0.0016544625146054114\n",
      "Iteration: 464, Loss : 0.0016489550990243286\n",
      "Iteration: 465, Loss : 0.0016434156272407763\n",
      "Iteration: 466, Loss : 0.001637973420705579\n",
      "Iteration: 467, Loss : 0.0016325609832824532\n",
      "Iteration: 468, Loss : 0.0016271237132864768\n",
      "Iteration: 469, Loss : 0.0016217766649294155\n",
      "Iteration: 470, Loss : 0.0016164017890769444\n",
      "Iteration: 471, Loss : 0.0016111157586813677\n",
      "Iteration: 472, Loss : 0.0016058040265970201\n",
      "Iteration: 473, Loss : 0.001600525881205459\n",
      "Iteration: 474, Loss : 0.001595280520437549\n",
      "Iteration: 475, Loss : 0.0015901216772289842\n",
      "Iteration: 476, Loss : 0.0015849391665712666\n",
      "Iteration: 477, Loss : 0.0015797876680878618\n",
      "Iteration: 478, Loss : 0.0015746666704039396\n",
      "Iteration: 479, Loss : 0.001569555052756777\n",
      "Iteration: 480, Loss : 0.0015645439846211182\n",
      "Iteration: 481, Loss : 0.0015595614383941614\n",
      "Iteration: 482, Loss : 0.0015545982129824777\n",
      "Iteration: 483, Loss : 0.0015496640305290121\n",
      "Iteration: 484, Loss : 0.0015447016381423331\n",
      "Iteration: 485, Loss : 0.0015398232873231183\n",
      "Iteration: 486, Loss : 0.0015349769240154776\n",
      "Iteration: 487, Loss : 0.0015301581063500958\n",
      "Iteration: 488, Loss : 0.001525308436059863\n",
      "Iteration: 489, Loss : 0.0015205391942901014\n",
      "Iteration: 490, Loss : 0.001515747577658936\n",
      "Iteration: 491, Loss : 0.0015110345998736044\n",
      "Iteration: 492, Loss : 0.001506347845696537\n",
      "Iteration: 493, Loss : 0.001501687291886411\n",
      "Iteration: 494, Loss : 0.0014970032684637168\n",
      "Iteration: 495, Loss : 0.0014923461440810477\n",
      "Iteration: 496, Loss : 0.0014877141341176452\n",
      "Iteration: 497, Loss : 0.0014831557206542098\n",
      "Iteration: 498, Loss : 0.0014786222456461152\n",
      "Iteration: 499, Loss : 0.0014741126078678204\n",
      "Iteration: 500, Loss : 0.0014696265455037246\n",
      "Iteration: 501, Loss : 0.0014651642057274148\n",
      "Iteration: 502, Loss : 0.0014607280177689566\n",
      "Iteration: 503, Loss : 0.0014563161952347416\n",
      "Iteration: 504, Loss : 0.0014519270614894364\n",
      "Iteration: 505, Loss : 0.0014475204766063602\n",
      "Iteration: 506, Loss : 0.001443181300910268\n",
      "Iteration: 507, Loss : 0.0014388154603148444\n",
      "Iteration: 508, Loss : 0.0014345223660011557\n",
      "Iteration: 509, Loss : 0.0014302044108803515\n",
      "Iteration: 510, Loss : 0.0014259115258077773\n",
      "Iteration: 511, Loss : 0.0014216835087347023\n",
      "Iteration: 512, Loss : 0.0014174797434510897\n",
      "Iteration: 513, Loss : 0.001413256035103992\n",
      "Iteration: 514, Loss : 0.0014090949990982374\n",
      "Iteration: 515, Loss : 0.0014049575068798702\n",
      "Iteration: 516, Loss : 0.0014008003270895145\n",
      "Iteration: 517, Loss : 0.0013967048769773205\n",
      "Iteration: 518, Loss : 0.0013926328628212211\n",
      "Iteration: 519, Loss : 0.0013885788820778705\n",
      "Iteration: 520, Loss : 0.0013845106193299266\n",
      "Iteration: 521, Loss : 0.0013804984805669612\n",
      "Iteration: 522, Loss : 0.0013765109658137602\n",
      "Iteration: 523, Loss : 0.0013725443112218702\n",
      "Iteration: 524, Loss : 0.0013685613477222428\n",
      "Iteration: 525, Loss : 0.0013646320160292926\n",
      "Iteration: 526, Loss : 0.0013607266835204595\n",
      "Iteration: 527, Loss : 0.0013568400965989318\n",
      "Iteration: 528, Loss : 0.0013529747742280986\n",
      "Iteration: 529, Loss : 0.0013491278711618934\n",
      "Iteration: 530, Loss : 0.0013453005525543113\n",
      "Iteration: 531, Loss : 0.0013414939094590964\n",
      "Iteration: 532, Loss : 0.001337700630801752\n",
      "Iteration: 533, Loss : 0.0013338877198263155\n",
      "Iteration: 534, Loss : 0.001330132899719927\n",
      "Iteration: 535, Loss : 0.001326401112378097\n",
      "Iteration: 536, Loss : 0.0013226894030976244\n",
      "Iteration: 537, Loss : 0.0013189950965856302\n",
      "Iteration: 538, Loss : 0.0013153204970765193\n",
      "Iteration: 539, Loss : 0.0013116642151767613\n",
      "Iteration: 540, Loss : 0.0013079940116446824\n",
      "Iteration: 541, Loss : 0.0013043305122350888\n",
      "Iteration: 542, Loss : 0.0013007241238581733\n",
      "Iteration: 543, Loss : 0.001297140061963433\n",
      "Iteration: 544, Loss : 0.001293542305900014\n",
      "Iteration: 545, Loss : 0.0012899936546171531\n",
      "Iteration: 546, Loss : 0.0012864624049126112\n",
      "Iteration: 547, Loss : 0.0012829484235614727\n",
      "Iteration: 548, Loss : 0.0012794444628502055\n",
      "Iteration: 549, Loss : 0.0012759582475873061\n",
      "Iteration: 550, Loss : 0.001272491004107186\n",
      "Iteration: 551, Loss : 0.0012690396268584948\n",
      "Iteration: 552, Loss : 0.0012656092179566508\n",
      "Iteration: 553, Loss : 0.0012621965615969956\n",
      "Iteration: 554, Loss : 0.001258799316574185\n",
      "Iteration: 555, Loss : 0.0012554151297580677\n",
      "Iteration: 556, Loss : 0.0012520146306781823\n",
      "Iteration: 557, Loss : 0.0012486661555330494\n",
      "Iteration: 558, Loss : 0.001245330222159888\n",
      "Iteration: 559, Loss : 0.0012420137189662224\n",
      "Iteration: 560, Loss : 0.0012386792830155432\n",
      "Iteration: 561, Loss : 0.0012353953699596504\n",
      "Iteration: 562, Loss : 0.0012321231966710782\n",
      "Iteration: 563, Loss : 0.001228866806454967\n",
      "Iteration: 564, Loss : 0.0012255973524971202\n",
      "Iteration: 565, Loss : 0.0012223736406324023\n",
      "Iteration: 566, Loss : 0.0012191640811975483\n",
      "Iteration: 567, Loss : 0.0012159697364564777\n",
      "Iteration: 568, Loss : 0.0012127911326461592\n",
      "Iteration: 569, Loss : 0.001209628706298133\n",
      "Iteration: 570, Loss : 0.0012064799087509182\n",
      "Iteration: 571, Loss : 0.0012033202974297782\n",
      "Iteration: 572, Loss : 0.0012001996226692901\n",
      "Iteration: 573, Loss : 0.0011970936148402872\n",
      "Iteration: 574, Loss : 0.0011939781943919592\n",
      "Iteration: 575, Loss : 0.0011909036275805926\n",
      "Iteration: 576, Loss : 0.001187842442147893\n",
      "Iteration: 577, Loss : 0.0011847964029147737\n",
      "Iteration: 578, Loss : 0.0011817619261234188\n",
      "Iteration: 579, Loss : 0.0011787405096813995\n",
      "Iteration: 580, Loss : 0.0011757331551854834\n",
      "Iteration: 581, Loss : 0.0011727424598236415\n",
      "Iteration: 582, Loss : 0.0011697635709439166\n",
      "Iteration: 583, Loss : 0.0011667984137443432\n",
      "Iteration: 584, Loss : 0.0011638458501194652\n",
      "Iteration: 585, Loss : 0.0011609090767519931\n",
      "Iteration: 586, Loss : 0.001157958606067796\n",
      "Iteration: 587, Loss : 0.0011550474329162068\n",
      "Iteration: 588, Loss : 0.0011521482050757077\n",
      "Iteration: 589, Loss : 0.001149238373280615\n",
      "Iteration: 590, Loss : 0.0011463424094348743\n",
      "Iteration: 591, Loss : 0.0011434837763385783\n",
      "Iteration: 592, Loss : 0.0011406143009194368\n",
      "Iteration: 593, Loss : 0.001137758318516062\n",
      "Iteration: 594, Loss : 0.0011349139491264619\n",
      "Iteration: 595, Loss : 0.0011321053502840739\n",
      "Iteration: 596, Loss : 0.001129309535053266\n",
      "Iteration: 597, Loss : 0.0011265257718234309\n",
      "Iteration: 598, Loss : 0.0011237324219774334\n",
      "Iteration: 599, Loss : 0.0011209743135059\n",
      "Iteration: 600, Loss : 0.0011182059042391637\n",
      "Iteration: 601, Loss : 0.0011154720569087108\n",
      "Iteration: 602, Loss : 0.0011127503530202467\n",
      "Iteration: 603, Loss : 0.0011100397647395561\n",
      "Iteration: 604, Loss : 0.0011073410800770274\n",
      "Iteration: 605, Loss : 0.0011046540092794453\n",
      "Iteration: 606, Loss : 0.0011019581007547599\n",
      "Iteration: 607, Loss : 0.001099273771976064\n",
      "Iteration: 608, Loss : 0.0010966017174570764\n",
      "Iteration: 609, Loss : 0.0010939417828108589\n",
      "Iteration: 610, Loss : 0.0010912938246915513\n",
      "Iteration: 611, Loss : 0.0010886773976449826\n",
      "Iteration: 612, Loss : 0.0010860723444419055\n",
      "Iteration: 613, Loss : 0.001083479295061093\n",
      "Iteration: 614, Loss : 0.0010808967193214087\n",
      "Iteration: 615, Loss : 0.001078302158738594\n",
      "Iteration: 616, Loss : 0.0010757416794115794\n",
      "Iteration: 617, Loss : 0.0010731903586195485\n",
      "Iteration: 618, Loss : 0.001070632949137072\n",
      "Iteration: 619, Loss : 0.001068105947602973\n",
      "Iteration: 620, Loss : 0.001065566620396894\n",
      "Iteration: 621, Loss : 0.0010630419619166828\n",
      "Iteration: 622, Loss : 0.0010605471431131808\n",
      "Iteration: 623, Loss : 0.0010580622281445063\n",
      "Iteration: 624, Loss : 0.0010555864050790843\n",
      "Iteration: 625, Loss : 0.0010531211727466735\n",
      "Iteration: 626, Loss : 0.0010506681338726216\n",
      "Iteration: 627, Loss : 0.0010482235855869867\n",
      "Iteration: 628, Loss : 0.001045790406403437\n",
      "Iteration: 629, Loss : 0.0010433680823210448\n",
      "Iteration: 630, Loss : 0.0010409340137803131\n",
      "Iteration: 631, Loss : 0.0010385295979281578\n",
      "Iteration: 632, Loss : 0.0010361373965081162\n",
      "Iteration: 633, Loss : 0.0010337374651006026\n",
      "Iteration: 634, Loss : 0.0010313645998045948\n",
      "Iteration: 635, Loss : 0.0010290015728102017\n",
      "Iteration: 636, Loss : 0.0010266317064917428\n",
      "Iteration: 637, Loss : 0.0010242882310380111\n",
      "Iteration: 638, Loss : 0.0010219543959070682\n",
      "Iteration: 639, Loss : 0.0010196283560026064\n",
      "Iteration: 640, Loss : 0.0010173113157914693\n",
      "Iteration: 641, Loss : 0.0010150039553990737\n",
      "Iteration: 642, Loss : 0.0010127061962798894\n",
      "Iteration: 643, Loss : 0.0010104036787582135\n",
      "Iteration: 644, Loss : 0.0010081271539796831\n",
      "Iteration: 645, Loss : 0.0010058593119070076\n",
      "Iteration: 646, Loss : 0.0010035992235126736\n",
      "Iteration: 647, Loss : 0.0010013497228238092\n",
      "Iteration: 648, Loss : 0.0009991093221869102\n",
      "Iteration: 649, Loss : 0.0009968757954082609\n",
      "Iteration: 650, Loss : 0.000994634327705674\n",
      "Iteration: 651, Loss : 0.0009924025405623625\n",
      "Iteration: 652, Loss : 0.0009901973882498582\n",
      "Iteration: 653, Loss : 0.0009880011558026979\n",
      "Iteration: 654, Loss : 0.0009857970701777367\n",
      "Iteration: 655, Loss : 0.0009836192987862362\n",
      "Iteration: 656, Loss : 0.0009814330783791529\n",
      "Iteration: 657, Loss : 0.0009792720551193151\n",
      "Iteration: 658, Loss : 0.000977103464749894\n",
      "Iteration: 659, Loss : 0.0009749597291236506\n",
      "Iteration: 660, Loss : 0.0009728248252931471\n",
      "Iteration: 661, Loss : 0.0009706822688839655\n",
      "Iteration: 662, Loss : 0.0009685633942189621\n",
      "Iteration: 663, Loss : 0.0009664536117591297\n",
      "Iteration: 664, Loss : 0.0009643513565656685\n",
      "Iteration: 665, Loss : 0.0009622432209064776\n",
      "Iteration: 666, Loss : 0.0009601574274397184\n",
      "Iteration: 667, Loss : 0.0009580802252804007\n",
      "Iteration: 668, Loss : 0.0009559961201428321\n",
      "Iteration: 669, Loss : 0.000953920501595838\n",
      "Iteration: 670, Loss : 0.0009518674072660203\n",
      "Iteration: 671, Loss : 0.000949808086164978\n",
      "Iteration: 672, Loss : 0.0009477714624525418\n",
      "Iteration: 673, Loss : 0.0009457284559093156\n",
      "Iteration: 674, Loss : 0.0009437068684360127\n",
      "Iteration: 675, Loss : 0.0009416933283466439\n",
      "Iteration: 676, Loss : 0.0009396879227381385\n",
      "Iteration: 677, Loss : 0.0009376763470969423\n",
      "Iteration: 678, Loss : 0.0009356727607843499\n",
      "Iteration: 679, Loss : 0.0009336770921162645\n",
      "Iteration: 680, Loss : 0.0009317026331929756\n",
      "Iteration: 681, Loss : 0.0009297352275297044\n",
      "Iteration: 682, Loss : 0.0009277613951721414\n",
      "Iteration: 683, Loss : 0.000925809503949183\n",
      "Iteration: 684, Loss : 0.0009238650371929734\n",
      "Iteration: 685, Loss : 0.0009219275244623146\n",
      "Iteration: 686, Loss : 0.0009199840796267024\n",
      "Iteration: 687, Loss : 0.0009180484519901659\n",
      "Iteration: 688, Loss : 0.0009161203696949251\n",
      "Iteration: 689, Loss : 0.0009142116744071055\n",
      "Iteration: 690, Loss : 0.0009123103155103444\n",
      "Iteration: 691, Loss : 0.0009104037012069471\n",
      "Iteration: 692, Loss : 0.0009085169283392556\n",
      "Iteration: 693, Loss : 0.0009066370900404628\n",
      "Iteration: 694, Loss : 0.0009047649302582176\n",
      "Iteration: 695, Loss : 0.0009028990999245726\n",
      "Iteration: 696, Loss : 0.0009010402543328794\n",
      "Iteration: 697, Loss : 0.0008991883531533271\n",
      "Iteration: 698, Loss : 0.0008973314536867117\n",
      "Iteration: 699, Loss : 0.0008954932900098679\n",
      "Iteration: 700, Loss : 0.0008936499333747057\n",
      "Iteration: 701, Loss : 0.00089181369553559\n",
      "Iteration: 702, Loss : 0.0008899959740435569\n",
      "Iteration: 703, Loss : 0.0008881855661849207\n",
      "Iteration: 704, Loss : 0.0008863818931676803\n",
      "Iteration: 705, Loss : 0.0008845845619507705\n",
      "Iteration: 706, Loss : 0.0008827934002500963\n",
      "Iteration: 707, Loss : 0.000881009506679715\n",
      "Iteration: 708, Loss : 0.0008792204777532086\n",
      "Iteration: 709, Loss : 0.0008774371858354267\n",
      "Iteration: 710, Loss : 0.0008756724859665687\n",
      "Iteration: 711, Loss : 0.0008739146735902781\n",
      "Iteration: 712, Loss : 0.0008721508646164802\n",
      "Iteration: 713, Loss : 0.0008703943899051048\n",
      "Iteration: 714, Loss : 0.000868655301936248\n",
      "Iteration: 715, Loss : 0.0008669108429219258\n",
      "Iteration: 716, Loss : 0.0008651729327048838\n",
      "Iteration: 717, Loss : 0.0008634525460343866\n",
      "Iteration: 718, Loss : 0.0008617382077073039\n",
      "Iteration: 719, Loss : 0.000860030467126406\n",
      "Iteration: 720, Loss : 0.0008583177009991193\n",
      "Iteration: 721, Loss : 0.0008566218507359072\n",
      "Iteration: 722, Loss : 0.0008549215874702452\n",
      "Iteration: 723, Loss : 0.000853238441670005\n",
      "Iteration: 724, Loss : 0.0008515499812344164\n",
      "Iteration: 725, Loss : 0.0008498788172757467\n",
      "Iteration: 726, Loss : 0.0008482024892206856\n",
      "Iteration: 727, Loss : 0.0008465428060658697\n",
      "Iteration: 728, Loss : 0.0008448888585915734\n",
      "Iteration: 729, Loss : 0.00084323048840062\n",
      "Iteration: 730, Loss : 0.000841588319592772\n",
      "Iteration: 731, Loss : 0.0008399418466297641\n",
      "Iteration: 732, Loss : 0.0008383113760194426\n",
      "Iteration: 733, Loss : 0.0008366870648106016\n",
      "Iteration: 734, Loss : 0.0008350681461273075\n",
      "Iteration: 735, Loss : 0.0008334549548198447\n",
      "Iteration: 736, Loss : 0.0008318470511010129\n",
      "Iteration: 737, Loss : 0.0008302450769383953\n",
      "Iteration: 738, Loss : 0.0008286490906971094\n",
      "Iteration: 739, Loss : 0.0008270587019585414\n",
      "Iteration: 740, Loss : 0.0008254735329509857\n",
      "Iteration: 741, Loss : 0.0008238933301399846\n",
      "Iteration: 742, Loss : 0.0008223186485308372\n",
      "Iteration: 743, Loss : 0.0008207502503723616\n",
      "Iteration: 744, Loss : 0.0008191776595980773\n",
      "Iteration: 745, Loss : 0.0008176196016410473\n",
      "Iteration: 746, Loss : 0.0008160674762493104\n",
      "Iteration: 747, Loss : 0.0008145112464072808\n",
      "Iteration: 748, Loss : 0.0008129698600309528\n",
      "Iteration: 749, Loss : 0.0008114326911154968\n",
      "Iteration: 750, Loss : 0.0008098912130931773\n",
      "Iteration: 751, Loss : 0.0008083647390680169\n",
      "Iteration: 752, Loss : 0.0008068435194047584\n",
      "Iteration: 753, Loss : 0.0008053282954823172\n",
      "Iteration: 754, Loss : 0.000803809092064349\n",
      "Iteration: 755, Loss : 0.0008022940045754193\n",
      "Iteration: 756, Loss : 0.000800784283345988\n",
      "Iteration: 757, Loss : 0.0007992798825596028\n",
      "Iteration: 758, Loss : 0.0007977900450887015\n",
      "Iteration: 759, Loss : 0.0007962964081747795\n",
      "Iteration: 760, Loss : 0.0007948079449095744\n",
      "Iteration: 761, Loss : 0.0007933331898507814\n",
      "Iteration: 762, Loss : 0.0007918540598201103\n",
      "Iteration: 763, Loss : 0.0007903800923461096\n",
      "Iteration: 764, Loss : 0.0007889112458061833\n",
      "Iteration: 765, Loss : 0.0007874561598304736\n",
      "Iteration: 766, Loss : 0.0007859975763649146\n",
      "Iteration: 767, Loss : 0.0007845434691983452\n",
      "Iteration: 768, Loss : 0.0007831031793384595\n",
      "Iteration: 769, Loss : 0.0007816678240072507\n",
      "Iteration: 770, Loss : 0.0007802369837189042\n",
      "Iteration: 771, Loss : 0.0007788112334554507\n",
      "Iteration: 772, Loss : 0.0007773902435096844\n",
      "Iteration: 773, Loss : 0.0007759733216419562\n",
      "Iteration: 774, Loss : 0.000774561444109113\n",
      "Iteration: 775, Loss : 0.0007731542585978676\n",
      "Iteration: 776, Loss : 0.0007717516194074249\n",
      "Iteration: 777, Loss : 0.0007703536254008371\n",
      "Iteration: 778, Loss : 0.0007689599398064623\n",
      "Iteration: 779, Loss : 0.0007675627067254543\n",
      "Iteration: 780, Loss : 0.0007661785606858753\n",
      "Iteration: 781, Loss : 0.0007647986590796623\n",
      "Iteration: 782, Loss : 0.0007634156544793652\n",
      "Iteration: 783, Loss : 0.0007620368227337134\n",
      "Iteration: 784, Loss : 0.0007606626300940433\n",
      "Iteration: 785, Loss : 0.000759301081105538\n",
      "Iteration: 786, Loss : 0.0007579359838573276\n",
      "Iteration: 787, Loss : 0.0007565829887855328\n",
      "Iteration: 788, Loss : 0.000755234432832993\n",
      "Iteration: 789, Loss : 0.0007538905652981615\n",
      "Iteration: 790, Loss : 0.0007525510802578698\n",
      "Iteration: 791, Loss : 0.0007512159529666118\n",
      "Iteration: 792, Loss : 0.0007498846538067242\n",
      "Iteration: 793, Loss : 0.0007485501002129737\n",
      "Iteration: 794, Loss : 0.0007472201905095231\n",
      "Iteration: 795, Loss : 0.0007459020493985175\n",
      "Iteration: 796, Loss : 0.0007445882992861005\n",
      "Iteration: 797, Loss : 0.0007432712888867114\n",
      "Iteration: 798, Loss : 0.0007419580285834106\n",
      "Iteration: 799, Loss : 0.0007406566506639321\n",
      "Iteration: 800, Loss : 0.000739351850174779\n",
      "Iteration: 801, Loss : 0.0007380590211837781\n",
      "Iteration: 802, Loss : 0.0007367703296279706\n",
      "Iteration: 803, Loss : 0.0007354854975603338\n",
      "Iteration: 804, Loss : 0.0007341972067925696\n",
      "Iteration: 805, Loss : 0.0007329202923200071\n",
      "Iteration: 806, Loss : 0.0007316478758893892\n",
      "Iteration: 807, Loss : 0.0007303792850202369\n",
      "Iteration: 808, Loss : 0.0007291147077609364\n",
      "Iteration: 809, Loss : 0.0007278471976859017\n",
      "Iteration: 810, Loss : 0.0007265903047751355\n",
      "Iteration: 811, Loss : 0.0007253304060613392\n",
      "Iteration: 812, Loss : 0.0007240745766071411\n",
      "Iteration: 813, Loss : 0.0007228296671488573\n",
      "Iteration: 814, Loss : 0.0007215885644639515\n",
      "Iteration: 815, Loss : 0.0007203516049564661\n",
      "Iteration: 816, Loss : 0.0007191182561899238\n",
      "Iteration: 817, Loss : 0.0007178887581927797\n",
      "Iteration: 818, Loss : 0.0007166633689646509\n",
      "Iteration: 819, Loss : 0.0007154418178046222\n",
      "Iteration: 820, Loss : 0.0007142237223058077\n",
      "Iteration: 821, Loss : 0.0007130096598389981\n",
      "Iteration: 822, Loss : 0.0007117991130272839\n",
      "Iteration: 823, Loss : 0.0007105925605733515\n",
      "Iteration: 824, Loss : 0.0007093830369015123\n",
      "Iteration: 825, Loss : 0.0007081839305336661\n",
      "Iteration: 826, Loss : 0.0007069883140905845\n",
      "Iteration: 827, Loss : 0.00070579007623561\n",
      "Iteration: 828, Loss : 0.0007046018843437306\n",
      "Iteration: 829, Loss : 0.0007034111140547575\n",
      "Iteration: 830, Loss : 0.0007022302634867672\n",
      "Iteration: 831, Loss : 0.0007010527781018368\n",
      "Iteration: 832, Loss : 0.0006998789030691448\n",
      "Iteration: 833, Loss : 0.0006987086212292008\n",
      "Iteration: 834, Loss : 0.0006975419156284969\n",
      "Iteration: 835, Loss : 0.0006963725399234983\n",
      "Iteration: 836, Loss : 0.0006952070018963993\n",
      "Iteration: 837, Loss : 0.0006940511024376783\n",
      "Iteration: 838, Loss : 0.0006928923633220196\n",
      "Iteration: 839, Loss : 0.0006917436720670471\n",
      "Iteration: 840, Loss : 0.0006905983182620879\n",
      "Iteration: 841, Loss : 0.0006894564279067523\n",
      "Iteration: 842, Loss : 0.0006883177591750262\n",
      "Iteration: 843, Loss : 0.0006871828449518747\n",
      "Iteration: 844, Loss : 0.0006860511497398802\n",
      "Iteration: 845, Loss : 0.0006849170596252071\n",
      "Iteration: 846, Loss : 0.0006837922034453008\n",
      "Iteration: 847, Loss : 0.0006826649524917649\n",
      "Iteration: 848, Loss : 0.0006815406477916552\n",
      "Iteration: 849, Loss : 0.000680425899947816\n",
      "Iteration: 850, Loss : 0.0006793146402302274\n",
      "Iteration: 851, Loss : 0.0006782062985438877\n",
      "Iteration: 852, Loss : 0.0006771014435059964\n",
      "Iteration: 853, Loss : 0.0006760000868685871\n",
      "Iteration: 854, Loss : 0.0006749017878573951\n",
      "Iteration: 855, Loss : 0.0006738007856352346\n",
      "Iteration: 856, Loss : 0.0006727091809425818\n",
      "Iteration: 857, Loss : 0.0006716206161684366\n",
      "Iteration: 858, Loss : 0.0006705354748386267\n",
      "Iteration: 859, Loss : 0.0006694533453937841\n",
      "Iteration: 860, Loss : 0.0006683741378263588\n",
      "Iteration: 861, Loss : 0.0006672924878100865\n",
      "Iteration: 862, Loss : 0.000666219882657162\n",
      "Iteration: 863, Loss : 0.0006651502219801989\n",
      "Iteration: 864, Loss : 0.0006640837030145849\n",
      "Iteration: 865, Loss : 0.0006630148528872425\n",
      "Iteration: 866, Loss : 0.0006619548844490012\n",
      "Iteration: 867, Loss : 0.0006608923157650172\n",
      "Iteration: 868, Loss : 0.0006598330487285227\n",
      "Iteration: 869, Loss : 0.0006587821724619121\n",
      "Iteration: 870, Loss : 0.0006577344233728082\n",
      "Iteration: 871, Loss : 0.0006566844500166246\n",
      "Iteration: 872, Loss : 0.000655642771808789\n",
      "Iteration: 873, Loss : 0.000654598920871929\n",
      "Iteration: 874, Loss : 0.0006535634389610638\n",
      "Iteration: 875, Loss : 0.0006525309561667501\n",
      "Iteration: 876, Loss : 0.0006515012764655749\n",
      "Iteration: 877, Loss : 0.0006504745705727688\n",
      "Iteration: 878, Loss : 0.0006494508249293642\n",
      "Iteration: 879, Loss : 0.0006484250499287098\n",
      "Iteration: 880, Loss : 0.000647402255654878\n",
      "Iteration: 881, Loss : 0.0006463817122937137\n",
      "Iteration: 882, Loss : 0.00064536959122398\n",
      "Iteration: 883, Loss : 0.0006443606497768043\n",
      "Iteration: 884, Loss : 0.0006433543015044066\n",
      "Iteration: 885, Loss : 0.0006423460790306295\n",
      "Iteration: 886, Loss : 0.0006413407640867081\n",
      "Iteration: 887, Loss : 0.000640342786911108\n",
      "Iteration: 888, Loss : 0.0006393478556043473\n",
      "Iteration: 889, Loss : 0.0006383505138391867\n",
      "Iteration: 890, Loss : 0.0006373610877784483\n",
      "Iteration: 891, Loss : 0.0006363746622091462\n",
      "Iteration: 892, Loss : 0.0006353908475456773\n",
      "Iteration: 893, Loss : 0.0006344098208479418\n",
      "Iteration: 894, Loss : 0.0006334266979723716\n",
      "Iteration: 895, Loss : 0.0006324514187316442\n",
      "Iteration: 896, Loss : 0.000631478707141455\n",
      "Iteration: 897, Loss : 0.0006305090145839199\n",
      "Iteration: 898, Loss : 0.000629541930628085\n",
      "Iteration: 899, Loss : 0.0006285726517968178\n",
      "Iteration: 900, Loss : 0.0006276110820666141\n",
      "Iteration: 901, Loss : 0.0006266472591223122\n",
      "Iteration: 902, Loss : 0.0006256909986164217\n",
      "Iteration: 903, Loss : 0.0006247327271907395\n",
      "Iteration: 904, Loss : 0.0006237816168008831\n",
      "Iteration: 905, Loss : 0.0006228332342890163\n",
      "Iteration: 906, Loss : 0.0006218875000548196\n",
      "Iteration: 907, Loss : 0.0006209442788001646\n",
      "Iteration: 908, Loss : 0.0006199991221009138\n",
      "Iteration: 909, Loss : 0.0006190613210606424\n",
      "Iteration: 910, Loss : 0.0006181215775541055\n",
      "Iteration: 911, Loss : 0.0006171887848303965\n",
      "Iteration: 912, Loss : 0.0006162587370401585\n",
      "Iteration: 913, Loss : 0.0006153266484127927\n",
      "Iteration: 914, Loss : 0.0006144016112684082\n",
      "Iteration: 915, Loss : 0.0006134746750780514\n",
      "Iteration: 916, Loss : 0.0006125548733976161\n",
      "Iteration: 917, Loss : 0.0006116330480671228\n",
      "Iteration: 918, Loss : 0.0006107182714186293\n",
      "Iteration: 919, Loss : 0.0006098058361256355\n",
      "Iteration: 920, Loss : 0.000608896055024658\n",
      "Iteration: 921, Loss : 0.0006079843323260393\n",
      "Iteration: 922, Loss : 0.0006070751437381029\n",
      "Iteration: 923, Loss : 0.0006061728334183743\n",
      "Iteration: 924, Loss : 0.0006052728353350913\n",
      "Iteration: 925, Loss : 0.0006043751952034854\n",
      "Iteration: 926, Loss : 0.0006034799939112678\n",
      "Iteration: 927, Loss : 0.000602587444826378\n",
      "Iteration: 928, Loss : 0.0006016972360324269\n",
      "Iteration: 929, Loss : 0.000600805263625753\n",
      "Iteration: 930, Loss : 0.0005999198117038846\n",
      "Iteration: 931, Loss : 0.000599032452764266\n",
      "Iteration: 932, Loss : 0.0005981477225896566\n",
      "Iteration: 933, Loss : 0.0005972651699772576\n",
      "Iteration: 934, Loss : 0.0005963850406622627\n",
      "Iteration: 935, Loss : 0.0005955113836966556\n",
      "Iteration: 936, Loss : 0.0005946361280143597\n",
      "Iteration: 937, Loss : 0.0005937632549585642\n",
      "Iteration: 938, Loss : 0.0005928925494124398\n",
      "Iteration: 939, Loss : 0.0005920282272729475\n",
      "Iteration: 940, Loss : 0.0005911664202967847\n",
      "Iteration: 941, Loss : 0.0005903066949086244\n",
      "Iteration: 942, Loss : 0.0005894454383941876\n",
      "Iteration: 943, Loss : 0.0005885863178737694\n",
      "Iteration: 944, Loss : 0.0005877296574898439\n",
      "Iteration: 945, Loss : 0.0005868792001562815\n",
      "Iteration: 946, Loss : 0.0005860271062733259\n",
      "Iteration: 947, Loss : 0.0005851811463770361\n",
      "Iteration: 948, Loss : 0.0005843372793232773\n",
      "Iteration: 949, Loss : 0.0005834957884389986\n",
      "Iteration: 950, Loss : 0.0005826525284710748\n",
      "Iteration: 951, Loss : 0.000581815345062198\n",
      "Iteration: 952, Loss : 0.0005809765744180725\n",
      "Iteration: 953, Loss : 0.0005801400784842144\n",
      "Iteration: 954, Loss : 0.0005793096743844425\n",
      "Iteration: 955, Loss : 0.0005784814026795601\n",
      "Iteration: 956, Loss : 0.0005776552129634709\n",
      "Iteration: 957, Loss : 0.0005768313259991984\n",
      "Iteration: 958, Loss : 0.0005760094665993489\n",
      "Iteration: 959, Loss : 0.0005751896585497642\n",
      "Iteration: 960, Loss : 0.0005743721297091023\n",
      "Iteration: 961, Loss : 0.0005735565932161997\n",
      "Iteration: 962, Loss : 0.0005727396496017143\n",
      "Iteration: 963, Loss : 0.0005719283475087366\n",
      "Iteration: 964, Loss : 0.0005711192900262091\n",
      "Iteration: 965, Loss : 0.0005703087115254571\n",
      "Iteration: 966, Loss : 0.0005695038025833271\n",
      "Iteration: 967, Loss : 0.0005686972381484321\n",
      "Iteration: 968, Loss : 0.0005678927999873516\n",
      "Iteration: 969, Loss : 0.0005670942329041813\n",
      "Iteration: 970, Loss : 0.0005662941114735688\n",
      "Iteration: 971, Loss : 0.0005654995881344304\n",
      "Iteration: 972, Loss : 0.0005647071908044131\n",
      "Iteration: 973, Loss : 0.0005639166067325141\n",
      "Iteration: 974, Loss : 0.0005631280541964655\n",
      "Iteration: 975, Loss : 0.0005623416094189907\n",
      "Iteration: 976, Loss : 0.0005615535852069159\n",
      "Iteration: 977, Loss : 0.0005607710860246794\n",
      "Iteration: 978, Loss : 0.000559990652735364\n",
      "Iteration: 979, Loss : 0.0005592088417159192\n",
      "Iteration: 980, Loss : 0.0005584325269311986\n",
      "Iteration: 981, Loss : 0.0005576547313678699\n",
      "Iteration: 982, Loss : 0.0005568786841271922\n",
      "Iteration: 983, Loss : 0.0005561046495190739\n",
      "Iteration: 984, Loss : 0.0005553326175312548\n",
      "Iteration: 985, Loss : 0.0005545661163291699\n",
      "Iteration: 986, Loss : 0.0005537980133272589\n",
      "Iteration: 987, Loss : 0.0005530318875941903\n",
      "Iteration: 988, Loss : 0.0005522711545617326\n",
      "Iteration: 989, Loss : 0.0005515123591523491\n",
      "Iteration: 990, Loss : 0.0005507553658839608\n",
      "Iteration: 991, Loss : 0.0005500003563598112\n",
      "Iteration: 992, Loss : 0.0005492471237050116\n",
      "Iteration: 993, Loss : 0.0005484957838439972\n",
      "Iteration: 994, Loss : 0.0005477464498703183\n",
      "Iteration: 995, Loss : 0.0005469957062535309\n",
      "Iteration: 996, Loss : 0.0005462500463170549\n",
      "Iteration: 997, Loss : 0.000545502984198165\n",
      "Iteration: 998, Loss : 0.0005447578200283392\n",
      "Iteration: 999, Loss : 0.000544017826509021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.26336616e-04],\n",
       "       [2.28771669e-02],\n",
       "       [2.33513428e-02],\n",
       "       [9.66722299e-01]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T15:32:37.797773Z",
     "start_time": "2024-07-09T15:32:37.754394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 1\n",
    "X = np.array([[-1],[-2],[-3],[1],[2],[3]])\n",
    "y = np.array([[1],[2],[3],[-1],[-2],[-3]])\n",
    "size = [1,2,1]\n",
    "nn = NeuralNetwork(size,lambda x: x,lambda x: 1,init_func=lambda x,y: xavier_initialization((x,y)),use_batch_norm=False)\n",
    "nn.set_loss_function(mean_squared_error, mean_squared_error_derivative)\n",
    "nn.train(X,y,epochs=100,learning_rate=0.01, batch_size=3)\n",
    "nn.forward([[-25],[5],[-4],[4]])"
   ],
   "id": "4553d76d7dbaf899",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Loss : 4.6256037596674915\n",
      "Iteration: 1, Loss : 3.141815460215979\n",
      "Iteration: 2, Loss : 2.092505233622914\n",
      "Iteration: 3, Loss : 1.3166255515480165\n",
      "Iteration: 4, Loss : 0.7967779476806705\n",
      "Iteration: 5, Loss : 0.4655553478348294\n",
      "Iteration: 6, Loss : 0.2894731666914195\n",
      "Iteration: 7, Loss : 0.191451458174326\n",
      "Iteration: 8, Loss : 0.1288762906705466\n",
      "Iteration: 9, Loss : 0.08452455767479518\n",
      "Iteration: 10, Loss : 0.05761672905320724\n",
      "Iteration: 11, Loss : 0.037987432208739944\n",
      "Iteration: 12, Loss : 0.02503346082537029\n",
      "Iteration: 13, Loss : 0.016873306370959542\n",
      "Iteration: 14, Loss : 0.011381623919114528\n",
      "Iteration: 15, Loss : 0.007664189935640949\n",
      "Iteration: 16, Loss : 0.005091499444436252\n",
      "Iteration: 17, Loss : 0.0033698303616038357\n",
      "Iteration: 18, Loss : 0.0022501270643953356\n",
      "Iteration: 19, Loss : 0.0014576617698522046\n",
      "Iteration: 20, Loss : 0.0009734078843813263\n",
      "Iteration: 21, Loss : 0.000650290137643332\n",
      "Iteration: 22, Loss : 0.000435692508433223\n",
      "Iteration: 23, Loss : 0.00028791034728484444\n",
      "Iteration: 24, Loss : 0.0001920321413088998\n",
      "Iteration: 25, Loss : 0.00012803972533678444\n",
      "Iteration: 26, Loss : 8.570702740062207e-05\n",
      "Iteration: 27, Loss : 5.6604408626491695e-05\n",
      "Iteration: 28, Loss : 3.765316990512103e-05\n",
      "Iteration: 29, Loss : 2.5185186486630673e-05\n",
      "Iteration: 30, Loss : 1.6627093788126393e-05\n",
      "Iteration: 31, Loss : 1.1114188852164744e-05\n",
      "Iteration: 32, Loss : 7.402196085756756e-06\n",
      "Iteration: 33, Loss : 4.949040613866954e-06\n",
      "Iteration: 34, Loss : 3.3102670517480473e-06\n",
      "Iteration: 35, Loss : 2.187174655006823e-06\n",
      "Iteration: 36, Loss : 1.4555356273043275e-06\n",
      "Iteration: 37, Loss : 9.589876383671529e-07\n",
      "Iteration: 38, Loss : 6.340159880631157e-07\n",
      "Iteration: 39, Loss : 4.2369266840824933e-07\n",
      "Iteration: 40, Loss : 2.8024491204360304e-07\n",
      "Iteration: 41, Loss : 1.8700993861345736e-07\n",
      "Iteration: 42, Loss : 1.2502058526184048e-07\n",
      "Iteration: 43, Loss : 8.232349607390986e-08\n",
      "Iteration: 44, Loss : 5.308542928407902e-08\n",
      "Iteration: 45, Loss : 3.530737118914232e-08\n",
      "Iteration: 46, Loss : 2.3292864551976347e-08\n",
      "Iteration: 47, Loss : 1.5530304349657174e-08\n",
      "Iteration: 48, Loss : 1.0343728689075468e-08\n",
      "Iteration: 49, Loss : 6.887525990080346e-09\n",
      "Iteration: 50, Loss : 4.600405822929949e-09\n",
      "Iteration: 51, Loss : 3.075619882408608e-09\n",
      "Iteration: 52, Loss : 2.056841468178721e-09\n",
      "Iteration: 53, Loss : 1.3760957708674467e-09\n",
      "Iteration: 54, Loss : 9.204128277015507e-10\n",
      "Iteration: 55, Loss : 6.1255986377379e-10\n",
      "Iteration: 56, Loss : 4.0832408496556007e-10\n",
      "Iteration: 57, Loss : 2.635163981881047e-10\n",
      "Iteration: 58, Loss : 1.7352366627089286e-10\n",
      "Iteration: 59, Loss : 1.1606359183345315e-10\n",
      "Iteration: 60, Loss : 7.764538369148733e-11\n",
      "Iteration: 61, Loss : 5.1752843921576023e-11\n",
      "Iteration: 62, Loss : 3.449777161710927e-11\n",
      "Iteration: 63, Loss : 2.2713369087634792e-11\n",
      "Iteration: 64, Loss : 1.511520071762544e-11\n",
      "Iteration: 65, Loss : 1.0075269771557666e-11\n",
      "Iteration: 66, Loss : 6.715688456464311e-12\n",
      "Iteration: 67, Loss : 4.473997927245835e-12\n",
      "Iteration: 68, Loss : 2.94743133194031e-12\n",
      "Iteration: 69, Loss : 1.947068049010767e-12\n",
      "Iteration: 70, Loss : 1.3023545115007848e-12\n",
      "Iteration: 71, Loss : 8.601781809664559e-13\n",
      "Iteration: 72, Loss : 5.748864256195953e-13\n",
      "Iteration: 73, Loss : 3.7967439673702825e-13\n",
      "Iteration: 74, Loss : 2.5293343238415073e-13\n",
      "Iteration: 75, Loss : 1.6324400723646742e-13\n",
      "Iteration: 76, Loss : 1.0920270735676764e-13\n",
      "Iteration: 77, Loss : 7.278460996713798e-14\n",
      "Iteration: 78, Loss : 4.8692169409660485e-14\n",
      "Iteration: 79, Loss : 3.257943940183851e-14\n",
      "Iteration: 80, Loss : 2.179081618037506e-14\n",
      "Iteration: 81, Loss : 1.4398252999018824e-14\n",
      "Iteration: 82, Loss : 9.593829890524973e-15\n",
      "Iteration: 83, Loss : 6.3946726977433306e-15\n",
      "Iteration: 84, Loss : 4.133661067840673e-15\n",
      "Iteration: 85, Loss : 2.748103424790064e-15\n",
      "Iteration: 86, Loss : 1.7752330175982703e-15\n",
      "Iteration: 87, Loss : 1.1831406874137268e-15\n",
      "Iteration: 88, Loss : 7.911551990613302e-16\n",
      "Iteration: 89, Loss : 5.273397756446665e-16\n",
      "Iteration: 90, Loss : 3.527558120451576e-16\n",
      "Iteration: 91, Loss : 2.3261501403522043e-16\n",
      "Iteration: 92, Loss : 1.5485578818881615e-16\n",
      "Iteration: 93, Loss : 1.0198162501119285e-16\n",
      "Iteration: 94, Loss : 6.797872754331607e-17\n",
      "Iteration: 95, Loss : 4.408124884120094e-17\n",
      "Iteration: 96, Loss : 2.898242513746125e-17\n",
      "Iteration: 97, Loss : 1.935873423957493e-17\n",
      "Iteration: 98, Loss : 1.2778282432570212e-17\n",
      "Iteration: 99, Loss : 8.403881495954702e-18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[24.99999998],\n",
       "       [-5.        ],\n",
       "       [ 4.        ],\n",
       "       [-4.        ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T15:33:44.800675Z",
     "start_time": "2024-07-09T15:33:44.139320Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 1 \n",
    "size = [2,3,1]\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y = np.array([[0],[1],[1],[0]])\n",
    "nn = NeuralNetwork(size,sigmoid,sigmoid_derivative,use_batch_norm=False)\n",
    "nn.set_loss_function(mean_squared_error,mean_squared_error_derivative)\n",
    "nn.train(X,y,epochs=1000,learning_rate=10e-3,L2=None,clipping_mode='norm',adam_optimizer=True,batch_size=batch_size)\n",
    "nn.forward(X)"
   ],
   "id": "8926d131faf82493",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Loss : 0.3273192813941069\n",
      "Iteration: 1, Loss : 0.3231620457619328\n",
      "Iteration: 2, Loss : 0.31847123627134194\n",
      "Iteration: 3, Loss : 0.31358156521103636\n",
      "Iteration: 4, Loss : 0.3092275355094654\n",
      "Iteration: 5, Loss : 0.3036810266499065\n",
      "Iteration: 6, Loss : 0.2991717185124966\n",
      "Iteration: 7, Loss : 0.2942886192030614\n",
      "Iteration: 8, Loss : 0.29004389345731557\n",
      "Iteration: 9, Loss : 0.2875134780849379\n",
      "Iteration: 10, Loss : 0.2841809256441531\n",
      "Iteration: 11, Loss : 0.2805468531976735\n",
      "Iteration: 12, Loss : 0.27776684040649385\n",
      "Iteration: 13, Loss : 0.2759967293962527\n",
      "Iteration: 14, Loss : 0.2730880384465447\n",
      "Iteration: 15, Loss : 0.2716348755939501\n",
      "Iteration: 16, Loss : 0.2700635307896075\n",
      "Iteration: 17, Loss : 0.2679957896384892\n",
      "Iteration: 18, Loss : 0.2663799644503365\n",
      "Iteration: 19, Loss : 0.26428193523530746\n",
      "Iteration: 20, Loss : 0.26306270837359724\n",
      "Iteration: 21, Loss : 0.26170355635380943\n",
      "Iteration: 22, Loss : 0.26088993749106487\n",
      "Iteration: 23, Loss : 0.260213684144212\n",
      "Iteration: 24, Loss : 0.2593237838827837\n",
      "Iteration: 25, Loss : 0.2585002767339008\n",
      "Iteration: 26, Loss : 0.25756039039709605\n",
      "Iteration: 27, Loss : 0.2569078893955381\n",
      "Iteration: 28, Loss : 0.256189592937506\n",
      "Iteration: 29, Loss : 0.25557426255059945\n",
      "Iteration: 30, Loss : 0.25502316143308684\n",
      "Iteration: 31, Loss : 0.25479690386948045\n",
      "Iteration: 32, Loss : 0.2541775693054381\n",
      "Iteration: 33, Loss : 0.25388420157199065\n",
      "Iteration: 34, Loss : 0.25360534751716496\n",
      "Iteration: 35, Loss : 0.25324296983568656\n",
      "Iteration: 36, Loss : 0.25293013714000856\n",
      "Iteration: 37, Loss : 0.2526808136383133\n",
      "Iteration: 38, Loss : 0.2522848433772677\n",
      "Iteration: 39, Loss : 0.25221150673578363\n",
      "Iteration: 40, Loss : 0.25218460941793025\n",
      "Iteration: 41, Loss : 0.25204250950823504\n",
      "Iteration: 42, Loss : 0.2518374539872043\n",
      "Iteration: 43, Loss : 0.2517800520311399\n",
      "Iteration: 44, Loss : 0.2515220320624141\n",
      "Iteration: 45, Loss : 0.2513314243548752\n",
      "Iteration: 46, Loss : 0.2512837659552549\n",
      "Iteration: 47, Loss : 0.25118225745103123\n",
      "Iteration: 48, Loss : 0.25099109914534096\n",
      "Iteration: 49, Loss : 0.2509401687086075\n",
      "Iteration: 50, Loss : 0.25084179091445463\n",
      "Iteration: 51, Loss : 0.2508072936155276\n",
      "Iteration: 52, Loss : 0.25077059799804485\n",
      "Iteration: 53, Loss : 0.2507604577329337\n",
      "Iteration: 54, Loss : 0.2507045502688334\n",
      "Iteration: 55, Loss : 0.25064725029392393\n",
      "Iteration: 56, Loss : 0.25066607483672443\n",
      "Iteration: 57, Loss : 0.25059631268223337\n",
      "Iteration: 58, Loss : 0.25050989707562854\n",
      "Iteration: 59, Loss : 0.2504212169688043\n",
      "Iteration: 60, Loss : 0.2504321939493599\n",
      "Iteration: 61, Loss : 0.25035785806015737\n",
      "Iteration: 62, Loss : 0.25032475774800683\n",
      "Iteration: 63, Loss : 0.250298143294084\n",
      "Iteration: 64, Loss : 0.25033377434846227\n",
      "Iteration: 65, Loss : 0.25030935353177336\n",
      "Iteration: 66, Loss : 0.25024471841293006\n",
      "Iteration: 67, Loss : 0.2502722419346191\n",
      "Iteration: 68, Loss : 0.2502087726501309\n",
      "Iteration: 69, Loss : 0.25021371320066393\n",
      "Iteration: 70, Loss : 0.25023002224331176\n",
      "Iteration: 71, Loss : 0.250215735396444\n",
      "Iteration: 72, Loss : 0.250218901854308\n",
      "Iteration: 73, Loss : 0.2501906689485051\n",
      "Iteration: 74, Loss : 0.2501868338001429\n",
      "Iteration: 75, Loss : 0.25012114117221496\n",
      "Iteration: 76, Loss : 0.25012532176195457\n",
      "Iteration: 77, Loss : 0.25007717699050397\n",
      "Iteration: 78, Loss : 0.25006789380603256\n",
      "Iteration: 79, Loss : 0.25004399547564626\n",
      "Iteration: 80, Loss : 0.2500491131119674\n",
      "Iteration: 81, Loss : 0.25001647463290627\n",
      "Iteration: 82, Loss : 0.25000780209287204\n",
      "Iteration: 83, Loss : 0.24997492878681188\n",
      "Iteration: 84, Loss : 0.24995241215832142\n",
      "Iteration: 85, Loss : 0.24994057889369295\n",
      "Iteration: 86, Loss : 0.2499302666003636\n",
      "Iteration: 87, Loss : 0.249919036521226\n",
      "Iteration: 88, Loss : 0.249903620234539\n",
      "Iteration: 89, Loss : 0.24990587786714702\n",
      "Iteration: 90, Loss : 0.24988916357473825\n",
      "Iteration: 91, Loss : 0.249860687758349\n",
      "Iteration: 92, Loss : 0.24985155874781909\n",
      "Iteration: 93, Loss : 0.2498434643368606\n",
      "Iteration: 94, Loss : 0.2498204551234598\n",
      "Iteration: 95, Loss : 0.24980605264816316\n",
      "Iteration: 96, Loss : 0.24979278380404535\n",
      "Iteration: 97, Loss : 0.24977563595907218\n",
      "Iteration: 98, Loss : 0.24976524387335536\n",
      "Iteration: 99, Loss : 0.24975633432935107\n",
      "Iteration: 100, Loss : 0.24974030526593455\n",
      "Iteration: 101, Loss : 0.2497266203466384\n",
      "Iteration: 102, Loss : 0.24971232464677268\n",
      "Iteration: 103, Loss : 0.24970153937675293\n",
      "Iteration: 104, Loss : 0.2496952694659662\n",
      "Iteration: 105, Loss : 0.24967243257028765\n",
      "Iteration: 106, Loss : 0.2496568332029503\n",
      "Iteration: 107, Loss : 0.24964126254159152\n",
      "Iteration: 108, Loss : 0.24962449969979728\n",
      "Iteration: 109, Loss : 0.24961602768346192\n",
      "Iteration: 110, Loss : 0.24960302440353066\n",
      "Iteration: 111, Loss : 0.24958369115962742\n",
      "Iteration: 112, Loss : 0.24956387132497293\n",
      "Iteration: 113, Loss : 0.2495458770981289\n",
      "Iteration: 114, Loss : 0.24953476512505318\n",
      "Iteration: 115, Loss : 0.24951879970087876\n",
      "Iteration: 116, Loss : 0.24950303041591354\n",
      "Iteration: 117, Loss : 0.2494844970005542\n",
      "Iteration: 118, Loss : 0.2494697574923455\n",
      "Iteration: 119, Loss : 0.24945599324242984\n",
      "Iteration: 120, Loss : 0.24944941584846314\n",
      "Iteration: 121, Loss : 0.2494182077007989\n",
      "Iteration: 122, Loss : 0.24939676606860153\n",
      "Iteration: 123, Loss : 0.2493784684371949\n",
      "Iteration: 124, Loss : 0.2493636629454526\n",
      "Iteration: 125, Loss : 0.24935277300970932\n",
      "Iteration: 126, Loss : 0.2493318384358325\n",
      "Iteration: 127, Loss : 0.24931048738401285\n",
      "Iteration: 128, Loss : 0.24928985437980497\n",
      "Iteration: 129, Loss : 0.24925891953527424\n",
      "Iteration: 130, Loss : 0.249240398616601\n",
      "Iteration: 131, Loss : 0.24921835219172397\n",
      "Iteration: 132, Loss : 0.24918992754630437\n",
      "Iteration: 133, Loss : 0.2491706547919888\n",
      "Iteration: 134, Loss : 0.24914364553556523\n",
      "Iteration: 135, Loss : 0.24912301568476597\n",
      "Iteration: 136, Loss : 0.24910269355315107\n",
      "Iteration: 137, Loss : 0.24908700323411276\n",
      "Iteration: 138, Loss : 0.24904957066170857\n",
      "Iteration: 139, Loss : 0.2490219115653337\n",
      "Iteration: 140, Loss : 0.24900049765505727\n",
      "Iteration: 141, Loss : 0.24897259292337937\n",
      "Iteration: 142, Loss : 0.24894810191647926\n",
      "Iteration: 143, Loss : 0.2489135837295836\n",
      "Iteration: 144, Loss : 0.24889197469567453\n",
      "Iteration: 145, Loss : 0.24887050228357283\n",
      "Iteration: 146, Loss : 0.24883499217271238\n",
      "Iteration: 147, Loss : 0.24880490078865528\n",
      "Iteration: 148, Loss : 0.24877389678423342\n",
      "Iteration: 149, Loss : 0.24873935475846187\n",
      "Iteration: 150, Loss : 0.2486989216787496\n",
      "Iteration: 151, Loss : 0.24866718986989417\n",
      "Iteration: 152, Loss : 0.24863011144137276\n",
      "Iteration: 153, Loss : 0.24860166448061463\n",
      "Iteration: 154, Loss : 0.24855198066342882\n",
      "Iteration: 155, Loss : 0.24852292501408152\n",
      "Iteration: 156, Loss : 0.248487447546521\n",
      "Iteration: 157, Loss : 0.24845591798815025\n",
      "Iteration: 158, Loss : 0.24840062344373204\n",
      "Iteration: 159, Loss : 0.24836419007552307\n",
      "Iteration: 160, Loss : 0.24832492479573093\n",
      "Iteration: 161, Loss : 0.24828538025132124\n",
      "Iteration: 162, Loss : 0.24824421306800815\n",
      "Iteration: 163, Loss : 0.24819702906522584\n",
      "Iteration: 164, Loss : 0.24813903961590886\n",
      "Iteration: 165, Loss : 0.2480845375343149\n",
      "Iteration: 166, Loss : 0.24804143589459127\n",
      "Iteration: 167, Loss : 0.2479984855073702\n",
      "Iteration: 168, Loss : 0.24795137891586255\n",
      "Iteration: 169, Loss : 0.24790095261835315\n",
      "Iteration: 170, Loss : 0.24783085809922664\n",
      "Iteration: 171, Loss : 0.24778327374386672\n",
      "Iteration: 172, Loss : 0.24773505831668663\n",
      "Iteration: 173, Loss : 0.24765761002657555\n",
      "Iteration: 174, Loss : 0.24760642785967346\n",
      "Iteration: 175, Loss : 0.24754464555623973\n",
      "Iteration: 176, Loss : 0.2474723647257152\n",
      "Iteration: 177, Loss : 0.24741503136970622\n",
      "Iteration: 178, Loss : 0.24735705618559367\n",
      "Iteration: 179, Loss : 0.24728613514239545\n",
      "Iteration: 180, Loss : 0.2472171425621391\n",
      "Iteration: 181, Loss : 0.24714631002295445\n",
      "Iteration: 182, Loss : 0.24707384559577553\n",
      "Iteration: 183, Loss : 0.2469827290014635\n",
      "Iteration: 184, Loss : 0.246895721632041\n",
      "Iteration: 185, Loss : 0.2468032483925643\n",
      "Iteration: 186, Loss : 0.2467183427683849\n",
      "Iteration: 187, Loss : 0.24664390391401037\n",
      "Iteration: 188, Loss : 0.24656396987056337\n",
      "Iteration: 189, Loss : 0.24648289556138336\n",
      "Iteration: 190, Loss : 0.24638946467573714\n",
      "Iteration: 191, Loss : 0.24629414789685555\n",
      "Iteration: 192, Loss : 0.24617353374452514\n",
      "Iteration: 193, Loss : 0.24605859824239246\n",
      "Iteration: 194, Loss : 0.2459386263299866\n",
      "Iteration: 195, Loss : 0.2458212103808517\n",
      "Iteration: 196, Loss : 0.24571946867457617\n",
      "Iteration: 197, Loss : 0.24563488483970686\n",
      "Iteration: 198, Loss : 0.24554157933130377\n",
      "Iteration: 199, Loss : 0.24544056847745377\n",
      "Iteration: 200, Loss : 0.24531153406815254\n",
      "Iteration: 201, Loss : 0.2451983566517697\n",
      "Iteration: 202, Loss : 0.24504093356736473\n",
      "Iteration: 203, Loss : 0.2448972572451915\n",
      "Iteration: 204, Loss : 0.24475571358869563\n",
      "Iteration: 205, Loss : 0.24463737611762074\n",
      "Iteration: 206, Loss : 0.24448285754621019\n",
      "Iteration: 207, Loss : 0.24433763916257528\n",
      "Iteration: 208, Loss : 0.2441880692256193\n",
      "Iteration: 209, Loss : 0.24401594685512887\n",
      "Iteration: 210, Loss : 0.2438506159968971\n",
      "Iteration: 211, Loss : 0.24371864634948373\n",
      "Iteration: 212, Loss : 0.24357800862683704\n",
      "Iteration: 213, Loss : 0.24337252706365492\n",
      "Iteration: 214, Loss : 0.2431694028765254\n",
      "Iteration: 215, Loss : 0.24297123915015723\n",
      "Iteration: 216, Loss : 0.24282762239184363\n",
      "Iteration: 217, Loss : 0.24266468981292136\n",
      "Iteration: 218, Loss : 0.24244435844641182\n",
      "Iteration: 219, Loss : 0.24227260709471077\n",
      "Iteration: 220, Loss : 0.2420881888336877\n",
      "Iteration: 221, Loss : 0.24185775687814898\n",
      "Iteration: 222, Loss : 0.24162146002662227\n",
      "Iteration: 223, Loss : 0.241374943202687\n",
      "Iteration: 224, Loss : 0.24114041605382563\n",
      "Iteration: 225, Loss : 0.24095021058843336\n",
      "Iteration: 226, Loss : 0.24075320880939938\n",
      "Iteration: 227, Loss : 0.24048664900930825\n",
      "Iteration: 228, Loss : 0.2402310587326952\n",
      "Iteration: 229, Loss : 0.23994901235731364\n",
      "Iteration: 230, Loss : 0.23970837481532672\n",
      "Iteration: 231, Loss : 0.23949335543981143\n",
      "Iteration: 232, Loss : 0.23920189306031017\n",
      "Iteration: 233, Loss : 0.23890468320277425\n",
      "Iteration: 234, Loss : 0.23868467155495504\n",
      "Iteration: 235, Loss : 0.2384113744443449\n",
      "Iteration: 236, Loss : 0.238104993495327\n",
      "Iteration: 237, Loss : 0.2378058634269259\n",
      "Iteration: 238, Loss : 0.23753834862290912\n",
      "Iteration: 239, Loss : 0.2372236230310869\n",
      "Iteration: 240, Loss : 0.23693374823134755\n",
      "Iteration: 241, Loss : 0.23662485491638846\n",
      "Iteration: 242, Loss : 0.2362369424697751\n",
      "Iteration: 243, Loss : 0.23592411709260186\n",
      "Iteration: 244, Loss : 0.23557313968752824\n",
      "Iteration: 245, Loss : 0.23525322025152606\n",
      "Iteration: 246, Loss : 0.23489103860726712\n",
      "Iteration: 247, Loss : 0.23451583843632695\n",
      "Iteration: 248, Loss : 0.2341746802460012\n",
      "Iteration: 249, Loss : 0.23372945233713888\n",
      "Iteration: 250, Loss : 0.2333349448213284\n",
      "Iteration: 251, Loss : 0.23298720411951507\n",
      "Iteration: 252, Loss : 0.2325427049541181\n",
      "Iteration: 253, Loss : 0.23213437493019956\n",
      "Iteration: 254, Loss : 0.23174868286652783\n",
      "Iteration: 255, Loss : 0.23135328021514193\n",
      "Iteration: 256, Loss : 0.23088734023530338\n",
      "Iteration: 257, Loss : 0.23053966310785373\n",
      "Iteration: 258, Loss : 0.23017770130234633\n",
      "Iteration: 259, Loss : 0.229696829198497\n",
      "Iteration: 260, Loss : 0.2292776230593547\n",
      "Iteration: 261, Loss : 0.22878423655595656\n",
      "Iteration: 262, Loss : 0.22828876103780568\n",
      "Iteration: 263, Loss : 0.22787899107243187\n",
      "Iteration: 264, Loss : 0.22745209148321643\n",
      "Iteration: 265, Loss : 0.22695028507456727\n",
      "Iteration: 266, Loss : 0.22650020679939464\n",
      "Iteration: 267, Loss : 0.22597467822482512\n",
      "Iteration: 268, Loss : 0.2255405965329363\n",
      "Iteration: 269, Loss : 0.22501733817657166\n",
      "Iteration: 270, Loss : 0.22449960320263002\n",
      "Iteration: 271, Loss : 0.2239811627556677\n",
      "Iteration: 272, Loss : 0.22352151174641252\n",
      "Iteration: 273, Loss : 0.22295214045305167\n",
      "Iteration: 274, Loss : 0.22247750162411473\n",
      "Iteration: 275, Loss : 0.22198715753354567\n",
      "Iteration: 276, Loss : 0.22137943880477998\n",
      "Iteration: 277, Loss : 0.22084394332854732\n",
      "Iteration: 278, Loss : 0.22028976280667323\n",
      "Iteration: 279, Loss : 0.21981052626496195\n",
      "Iteration: 280, Loss : 0.21927063528516444\n",
      "Iteration: 281, Loss : 0.2187997672418659\n",
      "Iteration: 282, Loss : 0.21820766051148707\n",
      "Iteration: 283, Loss : 0.21769445549626737\n",
      "Iteration: 284, Loss : 0.21718762761694585\n",
      "Iteration: 285, Loss : 0.2166472073975821\n",
      "Iteration: 286, Loss : 0.2160888635447269\n",
      "Iteration: 287, Loss : 0.21553187036722005\n",
      "Iteration: 288, Loss : 0.21503024319653824\n",
      "Iteration: 289, Loss : 0.2144815140624107\n",
      "Iteration: 290, Loss : 0.2138013909049952\n",
      "Iteration: 291, Loss : 0.21317790150378868\n",
      "Iteration: 292, Loss : 0.212597266792181\n",
      "Iteration: 293, Loss : 0.21210412382847968\n",
      "Iteration: 294, Loss : 0.21158048753256006\n",
      "Iteration: 295, Loss : 0.2109875115729254\n",
      "Iteration: 296, Loss : 0.210410970112426\n",
      "Iteration: 297, Loss : 0.2098125875610261\n",
      "Iteration: 298, Loss : 0.20927869084056333\n",
      "Iteration: 299, Loss : 0.20867055162001674\n",
      "Iteration: 300, Loss : 0.20815477507958122\n",
      "Iteration: 301, Loss : 0.2076837312205304\n",
      "Iteration: 302, Loss : 0.2071498750591099\n",
      "Iteration: 303, Loss : 0.20652205405917926\n",
      "Iteration: 304, Loss : 0.20603342477171294\n",
      "Iteration: 305, Loss : 0.2055482088186577\n",
      "Iteration: 306, Loss : 0.20498962760611616\n",
      "Iteration: 307, Loss : 0.20433340658514187\n",
      "Iteration: 308, Loss : 0.20372150105257303\n",
      "Iteration: 309, Loss : 0.2032242119400411\n",
      "Iteration: 310, Loss : 0.2027395403360855\n",
      "Iteration: 311, Loss : 0.2022165945243493\n",
      "Iteration: 312, Loss : 0.20171351918495445\n",
      "Iteration: 313, Loss : 0.20110448600902892\n",
      "Iteration: 314, Loss : 0.2005627364695057\n",
      "Iteration: 315, Loss : 0.20000644677985746\n",
      "Iteration: 316, Loss : 0.19945360615794655\n",
      "Iteration: 317, Loss : 0.19898443179913206\n",
      "Iteration: 318, Loss : 0.1984679654869222\n",
      "Iteration: 319, Loss : 0.1980007990065362\n",
      "Iteration: 320, Loss : 0.19749579421211155\n",
      "Iteration: 321, Loss : 0.1970307222713482\n",
      "Iteration: 322, Loss : 0.19657314350432525\n",
      "Iteration: 323, Loss : 0.19613354110533818\n",
      "Iteration: 324, Loss : 0.19565093447660126\n",
      "Iteration: 325, Loss : 0.1951725961205888\n",
      "Iteration: 326, Loss : 0.1946417043312526\n",
      "Iteration: 327, Loss : 0.19414735875633066\n",
      "Iteration: 328, Loss : 0.19368767392654965\n",
      "Iteration: 329, Loss : 0.19324085195302992\n",
      "Iteration: 330, Loss : 0.1927858028346351\n",
      "Iteration: 331, Loss : 0.19233449941294423\n",
      "Iteration: 332, Loss : 0.19185226895105606\n",
      "Iteration: 333, Loss : 0.19134380791571542\n",
      "Iteration: 334, Loss : 0.1909417352120541\n",
      "Iteration: 335, Loss : 0.19051598552750712\n",
      "Iteration: 336, Loss : 0.19013290713269745\n",
      "Iteration: 337, Loss : 0.18970260852095533\n",
      "Iteration: 338, Loss : 0.1893098441032026\n",
      "Iteration: 339, Loss : 0.18890629830383884\n",
      "Iteration: 340, Loss : 0.18838765103414024\n",
      "Iteration: 341, Loss : 0.18790859614657351\n",
      "Iteration: 342, Loss : 0.1874663742625904\n",
      "Iteration: 343, Loss : 0.18714350992839723\n",
      "Iteration: 344, Loss : 0.18676789344961622\n",
      "Iteration: 345, Loss : 0.18641584652653162\n",
      "Iteration: 346, Loss : 0.18601754378151292\n",
      "Iteration: 347, Loss : 0.18562457472607452\n",
      "Iteration: 348, Loss : 0.18527445968538697\n",
      "Iteration: 349, Loss : 0.18491951632772832\n",
      "Iteration: 350, Loss : 0.18448799952523381\n",
      "Iteration: 351, Loss : 0.18404390492553638\n",
      "Iteration: 352, Loss : 0.18363942809760972\n",
      "Iteration: 353, Loss : 0.18333771722701725\n",
      "Iteration: 354, Loss : 0.18299756579238505\n",
      "Iteration: 355, Loss : 0.1826226025033157\n",
      "Iteration: 356, Loss : 0.1822624266175653\n",
      "Iteration: 357, Loss : 0.18190516099838655\n",
      "Iteration: 358, Loss : 0.18157810246535955\n",
      "Iteration: 359, Loss : 0.1811944595543707\n",
      "Iteration: 360, Loss : 0.18090679028533685\n",
      "Iteration: 361, Loss : 0.18055724264728096\n",
      "Iteration: 362, Loss : 0.18018606972729634\n",
      "Iteration: 363, Loss : 0.17991198441685752\n",
      "Iteration: 364, Loss : 0.17957827005607296\n",
      "Iteration: 365, Loss : 0.1793035329405463\n",
      "Iteration: 366, Loss : 0.17898997070684164\n",
      "Iteration: 367, Loss : 0.17870443175916417\n",
      "Iteration: 368, Loss : 0.17835497736484207\n",
      "Iteration: 369, Loss : 0.1779801605973961\n",
      "Iteration: 370, Loss : 0.1776423744302798\n",
      "Iteration: 371, Loss : 0.17734643253529922\n",
      "Iteration: 372, Loss : 0.17708469053305906\n",
      "Iteration: 373, Loss : 0.17679876341863976\n",
      "Iteration: 374, Loss : 0.17648394183690652\n",
      "Iteration: 375, Loss : 0.17615454846677583\n",
      "Iteration: 376, Loss : 0.17584438293678525\n",
      "Iteration: 377, Loss : 0.1755924980130884\n",
      "Iteration: 378, Loss : 0.17530268520022702\n",
      "Iteration: 379, Loss : 0.175022575214347\n",
      "Iteration: 380, Loss : 0.17476038830254853\n",
      "Iteration: 381, Loss : 0.17446529996919696\n",
      "Iteration: 382, Loss : 0.17420099205093825\n",
      "Iteration: 383, Loss : 0.1739182657255472\n",
      "Iteration: 384, Loss : 0.17361973229559177\n",
      "Iteration: 385, Loss : 0.17335815305918462\n",
      "Iteration: 386, Loss : 0.17308072133605876\n",
      "Iteration: 387, Loss : 0.17282752919135358\n",
      "Iteration: 388, Loss : 0.17255907202176163\n",
      "Iteration: 389, Loss : 0.17228476615036098\n",
      "Iteration: 390, Loss : 0.17197150611223833\n",
      "Iteration: 391, Loss : 0.1717216674879123\n",
      "Iteration: 392, Loss : 0.17145902929333012\n",
      "Iteration: 393, Loss : 0.17114953795300272\n",
      "Iteration: 394, Loss : 0.1708781483863585\n",
      "Iteration: 395, Loss : 0.1706270477311438\n",
      "Iteration: 396, Loss : 0.17036169492242587\n",
      "Iteration: 397, Loss : 0.17007895822439867\n",
      "Iteration: 398, Loss : 0.16983250043795187\n",
      "Iteration: 399, Loss : 0.16955312235022274\n",
      "Iteration: 400, Loss : 0.16929273024374653\n",
      "Iteration: 401, Loss : 0.16904936392980963\n",
      "Iteration: 402, Loss : 0.16877297835317678\n",
      "Iteration: 403, Loss : 0.1685215698571353\n",
      "Iteration: 404, Loss : 0.16826816856864452\n",
      "Iteration: 405, Loss : 0.16801799846272267\n",
      "Iteration: 406, Loss : 0.1677460760173867\n",
      "Iteration: 407, Loss : 0.16748901736863225\n",
      "Iteration: 408, Loss : 0.16724112697321628\n",
      "Iteration: 409, Loss : 0.1669707424646688\n",
      "Iteration: 410, Loss : 0.16671195874265307\n",
      "Iteration: 411, Loss : 0.16646489858981597\n",
      "Iteration: 412, Loss : 0.16620525065238875\n",
      "Iteration: 413, Loss : 0.16596401788151957\n",
      "Iteration: 414, Loss : 0.16569494172334104\n",
      "Iteration: 415, Loss : 0.16545528738241738\n",
      "Iteration: 416, Loss : 0.16520668773844427\n",
      "Iteration: 417, Loss : 0.16492271906454498\n",
      "Iteration: 418, Loss : 0.16465448189814\n",
      "Iteration: 419, Loss : 0.16440785839047117\n",
      "Iteration: 420, Loss : 0.164149020132886\n",
      "Iteration: 421, Loss : 0.16388676797111565\n",
      "Iteration: 422, Loss : 0.1636287350091336\n",
      "Iteration: 423, Loss : 0.16336913789195787\n",
      "Iteration: 424, Loss : 0.16311574143060262\n",
      "Iteration: 425, Loss : 0.1628523372580768\n",
      "Iteration: 426, Loss : 0.16259694664939786\n",
      "Iteration: 427, Loss : 0.16233640070864286\n",
      "Iteration: 428, Loss : 0.16207723110017289\n",
      "Iteration: 429, Loss : 0.1618182067676383\n",
      "Iteration: 430, Loss : 0.16154210242826167\n",
      "Iteration: 431, Loss : 0.1612809783052891\n",
      "Iteration: 432, Loss : 0.1610141843522066\n",
      "Iteration: 433, Loss : 0.16074430778909982\n",
      "Iteration: 434, Loss : 0.16047975360648767\n",
      "Iteration: 435, Loss : 0.1602157097695857\n",
      "Iteration: 436, Loss : 0.15994798352947243\n",
      "Iteration: 437, Loss : 0.1596816132283328\n",
      "Iteration: 438, Loss : 0.15941513552391798\n",
      "Iteration: 439, Loss : 0.15915207226928357\n",
      "Iteration: 440, Loss : 0.1588855889208788\n",
      "Iteration: 441, Loss : 0.15859498389674803\n",
      "Iteration: 442, Loss : 0.1583200713538972\n",
      "Iteration: 443, Loss : 0.15805690114138368\n",
      "Iteration: 444, Loss : 0.1577798914149858\n",
      "Iteration: 445, Loss : 0.1575039038745305\n",
      "Iteration: 446, Loss : 0.1572194575965541\n",
      "Iteration: 447, Loss : 0.1569430933198619\n",
      "Iteration: 448, Loss : 0.15665692736145406\n",
      "Iteration: 449, Loss : 0.1563835959831022\n",
      "Iteration: 450, Loss : 0.15610231002481656\n",
      "Iteration: 451, Loss : 0.1558100211627928\n",
      "Iteration: 452, Loss : 0.155514582464121\n",
      "Iteration: 453, Loss : 0.15522904551816735\n",
      "Iteration: 454, Loss : 0.15495898809576011\n",
      "Iteration: 455, Loss : 0.15467818528174065\n",
      "Iteration: 456, Loss : 0.15438725120549157\n",
      "Iteration: 457, Loss : 0.15408117621170686\n",
      "Iteration: 458, Loss : 0.15380050414184307\n",
      "Iteration: 459, Loss : 0.1534991678264866\n",
      "Iteration: 460, Loss : 0.1532081791162402\n",
      "Iteration: 461, Loss : 0.15289629304008473\n",
      "Iteration: 462, Loss : 0.15259124299496585\n",
      "Iteration: 463, Loss : 0.1523079417841132\n",
      "Iteration: 464, Loss : 0.15200612845379644\n",
      "Iteration: 465, Loss : 0.15169578452523957\n",
      "Iteration: 466, Loss : 0.15141346706319397\n",
      "Iteration: 467, Loss : 0.1510996841599393\n",
      "Iteration: 468, Loss : 0.15079791240468798\n",
      "Iteration: 469, Loss : 0.15050626176135878\n",
      "Iteration: 470, Loss : 0.15018537915482177\n",
      "Iteration: 471, Loss : 0.14985366806456454\n",
      "Iteration: 472, Loss : 0.14953995137551288\n",
      "Iteration: 473, Loss : 0.14922710294412977\n",
      "Iteration: 474, Loss : 0.14892727938150574\n",
      "Iteration: 475, Loss : 0.14860561864362612\n",
      "Iteration: 476, Loss : 0.14831003313870347\n",
      "Iteration: 477, Loss : 0.14800495310443557\n",
      "Iteration: 478, Loss : 0.14765713836875022\n",
      "Iteration: 479, Loss : 0.14735755517864163\n",
      "Iteration: 480, Loss : 0.14701168392035788\n",
      "Iteration: 481, Loss : 0.14665821015582606\n",
      "Iteration: 482, Loss : 0.14635593653700626\n",
      "Iteration: 483, Loss : 0.1460099348542701\n",
      "Iteration: 484, Loss : 0.1456900237043908\n",
      "Iteration: 485, Loss : 0.14537752404620602\n",
      "Iteration: 486, Loss : 0.1450377404492436\n",
      "Iteration: 487, Loss : 0.14468152833422307\n",
      "Iteration: 488, Loss : 0.14434960526237794\n",
      "Iteration: 489, Loss : 0.1440304399060003\n",
      "Iteration: 490, Loss : 0.1436789627996295\n",
      "Iteration: 491, Loss : 0.1433541014298932\n",
      "Iteration: 492, Loss : 0.1429957618505278\n",
      "Iteration: 493, Loss : 0.14266505204404034\n",
      "Iteration: 494, Loss : 0.14232536332754722\n",
      "Iteration: 495, Loss : 0.14193572325307602\n",
      "Iteration: 496, Loss : 0.14159924129153478\n",
      "Iteration: 497, Loss : 0.14121741512456198\n",
      "Iteration: 498, Loss : 0.1408619200553911\n",
      "Iteration: 499, Loss : 0.1404768289585185\n",
      "Iteration: 500, Loss : 0.14014197312496082\n",
      "Iteration: 501, Loss : 0.13976392592043949\n",
      "Iteration: 502, Loss : 0.13939711947967257\n",
      "Iteration: 503, Loss : 0.13904284716674853\n",
      "Iteration: 504, Loss : 0.1386749779389286\n",
      "Iteration: 505, Loss : 0.13828165868213685\n",
      "Iteration: 506, Loss : 0.13792202390970457\n",
      "Iteration: 507, Loss : 0.13755093614641756\n",
      "Iteration: 508, Loss : 0.13720661900979972\n",
      "Iteration: 509, Loss : 0.13681448932916412\n",
      "Iteration: 510, Loss : 0.13644129780223374\n",
      "Iteration: 511, Loss : 0.13603008181053922\n",
      "Iteration: 512, Loss : 0.13568043974269256\n",
      "Iteration: 513, Loss : 0.13532418830816884\n",
      "Iteration: 514, Loss : 0.13490936764036898\n",
      "Iteration: 515, Loss : 0.13454951212812777\n",
      "Iteration: 516, Loss : 0.13415395513025843\n",
      "Iteration: 517, Loss : 0.1337251322030056\n",
      "Iteration: 518, Loss : 0.13334352828319007\n",
      "Iteration: 519, Loss : 0.13294950387928478\n",
      "Iteration: 520, Loss : 0.1325265860845703\n",
      "Iteration: 521, Loss : 0.1321363339864397\n",
      "Iteration: 522, Loss : 0.131738899177606\n",
      "Iteration: 523, Loss : 0.13132960607705746\n",
      "Iteration: 524, Loss : 0.1309651204875759\n",
      "Iteration: 525, Loss : 0.13058830352637557\n",
      "Iteration: 526, Loss : 0.1302006790801412\n",
      "Iteration: 527, Loss : 0.12980493960217357\n",
      "Iteration: 528, Loss : 0.12937520269020392\n",
      "Iteration: 529, Loss : 0.12894648596559932\n",
      "Iteration: 530, Loss : 0.12848828263360879\n",
      "Iteration: 531, Loss : 0.12805846007262084\n",
      "Iteration: 532, Loss : 0.12765795530992585\n",
      "Iteration: 533, Loss : 0.1272236807209796\n",
      "Iteration: 534, Loss : 0.12684190673212017\n",
      "Iteration: 535, Loss : 0.12644330710146612\n",
      "Iteration: 536, Loss : 0.1259948833890335\n",
      "Iteration: 537, Loss : 0.12554745755783026\n",
      "Iteration: 538, Loss : 0.12508967450492448\n",
      "Iteration: 539, Loss : 0.12466324891044314\n",
      "Iteration: 540, Loss : 0.12426663195359111\n",
      "Iteration: 541, Loss : 0.12384078568322543\n",
      "Iteration: 542, Loss : 0.1233735837236613\n",
      "Iteration: 543, Loss : 0.12295884110991347\n",
      "Iteration: 544, Loss : 0.12252345780749428\n",
      "Iteration: 545, Loss : 0.1220906791914296\n",
      "Iteration: 546, Loss : 0.12168433707982018\n",
      "Iteration: 547, Loss : 0.12126683785365086\n",
      "Iteration: 548, Loss : 0.12076942489735834\n",
      "Iteration: 549, Loss : 0.12030517016036206\n",
      "Iteration: 550, Loss : 0.1198963545283175\n",
      "Iteration: 551, Loss : 0.1194267448497438\n",
      "Iteration: 552, Loss : 0.11896024101123154\n",
      "Iteration: 553, Loss : 0.11849571886706796\n",
      "Iteration: 554, Loss : 0.11804065973983428\n",
      "Iteration: 555, Loss : 0.1175676785733369\n",
      "Iteration: 556, Loss : 0.11716650197612197\n",
      "Iteration: 557, Loss : 0.11668292567585545\n",
      "Iteration: 558, Loss : 0.1162312276352401\n",
      "Iteration: 559, Loss : 0.11578462796873984\n",
      "Iteration: 560, Loss : 0.11534797245040751\n",
      "Iteration: 561, Loss : 0.11489448053665763\n",
      "Iteration: 562, Loss : 0.1143955125499729\n",
      "Iteration: 563, Loss : 0.11395446822872274\n",
      "Iteration: 564, Loss : 0.11351501300163969\n",
      "Iteration: 565, Loss : 0.11304303395801985\n",
      "Iteration: 566, Loss : 0.11259498376394474\n",
      "Iteration: 567, Loss : 0.11209511541494527\n",
      "Iteration: 568, Loss : 0.11159775551888709\n",
      "Iteration: 569, Loss : 0.1111427501315492\n",
      "Iteration: 570, Loss : 0.11065777561472596\n",
      "Iteration: 571, Loss : 0.11016326924143925\n",
      "Iteration: 572, Loss : 0.10972023146085062\n",
      "Iteration: 573, Loss : 0.10930721705532336\n",
      "Iteration: 574, Loss : 0.1088820868821371\n",
      "Iteration: 575, Loss : 0.1084397288836474\n",
      "Iteration: 576, Loss : 0.10788105694184577\n",
      "Iteration: 577, Loss : 0.10741021834985412\n",
      "Iteration: 578, Loss : 0.10697292736723817\n",
      "Iteration: 579, Loss : 0.10643844745843038\n",
      "Iteration: 580, Loss : 0.10592428838351513\n",
      "Iteration: 581, Loss : 0.10549543819864243\n",
      "Iteration: 582, Loss : 0.10498994692467625\n",
      "Iteration: 583, Loss : 0.10455798784340511\n",
      "Iteration: 584, Loss : 0.10404645044099001\n",
      "Iteration: 585, Loss : 0.10358114475848074\n",
      "Iteration: 586, Loss : 0.10313583311507077\n",
      "Iteration: 587, Loss : 0.10267388670763594\n",
      "Iteration: 588, Loss : 0.10211588020651705\n",
      "Iteration: 589, Loss : 0.101673175177417\n",
      "Iteration: 590, Loss : 0.10116329363699776\n",
      "Iteration: 591, Loss : 0.10064627205630039\n",
      "Iteration: 592, Loss : 0.10020536239853217\n",
      "Iteration: 593, Loss : 0.09974426829569932\n",
      "Iteration: 594, Loss : 0.09928056374531631\n",
      "Iteration: 595, Loss : 0.09871631114823858\n",
      "Iteration: 596, Loss : 0.09820273176129425\n",
      "Iteration: 597, Loss : 0.09771453274795851\n",
      "Iteration: 598, Loss : 0.09721059105410362\n",
      "Iteration: 599, Loss : 0.09671209780512197\n",
      "Iteration: 600, Loss : 0.09622747471258733\n",
      "Iteration: 601, Loss : 0.09577716812061216\n",
      "Iteration: 602, Loss : 0.09528282110500064\n",
      "Iteration: 603, Loss : 0.09483188441703969\n",
      "Iteration: 604, Loss : 0.0943252701858733\n",
      "Iteration: 605, Loss : 0.093865898745115\n",
      "Iteration: 606, Loss : 0.09335130334983172\n",
      "Iteration: 607, Loss : 0.09285266735140864\n",
      "Iteration: 608, Loss : 0.09239398713890465\n",
      "Iteration: 609, Loss : 0.09193171232672889\n",
      "Iteration: 610, Loss : 0.0914673067735036\n",
      "Iteration: 611, Loss : 0.0909914931998367\n",
      "Iteration: 612, Loss : 0.09045041484135412\n",
      "Iteration: 613, Loss : 0.08992414012494769\n",
      "Iteration: 614, Loss : 0.08944392551084773\n",
      "Iteration: 615, Loss : 0.08896456566715877\n",
      "Iteration: 616, Loss : 0.08845930996650159\n",
      "Iteration: 617, Loss : 0.08799137547291437\n",
      "Iteration: 618, Loss : 0.0875288313196845\n",
      "Iteration: 619, Loss : 0.08703546080193279\n",
      "Iteration: 620, Loss : 0.08650396787328418\n",
      "Iteration: 621, Loss : 0.085976570933048\n",
      "Iteration: 622, Loss : 0.08553421587361455\n",
      "Iteration: 623, Loss : 0.08508027370694951\n",
      "Iteration: 624, Loss : 0.08461977214749308\n",
      "Iteration: 625, Loss : 0.08414369934002784\n",
      "Iteration: 626, Loss : 0.08358567850280357\n",
      "Iteration: 627, Loss : 0.08311326640291145\n",
      "Iteration: 628, Loss : 0.08260229632580018\n",
      "Iteration: 629, Loss : 0.08213183115827148\n",
      "Iteration: 630, Loss : 0.08162804163702976\n",
      "Iteration: 631, Loss : 0.08116066476313638\n",
      "Iteration: 632, Loss : 0.08066734834624839\n",
      "Iteration: 633, Loss : 0.08017917681100586\n",
      "Iteration: 634, Loss : 0.07969330287301114\n",
      "Iteration: 635, Loss : 0.07920709270329135\n",
      "Iteration: 636, Loss : 0.07870571176888114\n",
      "Iteration: 637, Loss : 0.07827253093333639\n",
      "Iteration: 638, Loss : 0.07781501133609602\n",
      "Iteration: 639, Loss : 0.07730454827511739\n",
      "Iteration: 640, Loss : 0.07684515837961453\n",
      "Iteration: 641, Loss : 0.0764098423088064\n",
      "Iteration: 642, Loss : 0.07596927536030357\n",
      "Iteration: 643, Loss : 0.0754564229298414\n",
      "Iteration: 644, Loss : 0.0749746840658253\n",
      "Iteration: 645, Loss : 0.07450439457983228\n",
      "Iteration: 646, Loss : 0.07400491015549832\n",
      "Iteration: 647, Loss : 0.07350939647716453\n",
      "Iteration: 648, Loss : 0.07305312906073465\n",
      "Iteration: 649, Loss : 0.07259115025915674\n",
      "Iteration: 650, Loss : 0.072168699683713\n",
      "Iteration: 651, Loss : 0.0716689701627813\n",
      "Iteration: 652, Loss : 0.07119790601695548\n",
      "Iteration: 653, Loss : 0.07071141734610342\n",
      "Iteration: 654, Loss : 0.07026247432804897\n",
      "Iteration: 655, Loss : 0.0697864236188266\n",
      "Iteration: 656, Loss : 0.06931637951809022\n",
      "Iteration: 657, Loss : 0.06885093276621211\n",
      "Iteration: 658, Loss : 0.0684360434981941\n",
      "Iteration: 659, Loss : 0.06794858628092638\n",
      "Iteration: 660, Loss : 0.06748472830005424\n",
      "Iteration: 661, Loss : 0.06705279815451509\n",
      "Iteration: 662, Loss : 0.0666303621675779\n",
      "Iteration: 663, Loss : 0.06621116194611709\n",
      "Iteration: 664, Loss : 0.06574283653773566\n",
      "Iteration: 665, Loss : 0.06528270731266975\n",
      "Iteration: 666, Loss : 0.06478888959469692\n",
      "Iteration: 667, Loss : 0.06432965336465873\n",
      "Iteration: 668, Loss : 0.06389831409878997\n",
      "Iteration: 669, Loss : 0.06344385063554638\n",
      "Iteration: 670, Loss : 0.06301166472028888\n",
      "Iteration: 671, Loss : 0.06258792380632362\n",
      "Iteration: 672, Loss : 0.06217122080128298\n",
      "Iteration: 673, Loss : 0.06169967780118342\n",
      "Iteration: 674, Loss : 0.061258138711740656\n",
      "Iteration: 675, Loss : 0.06083126608820759\n",
      "Iteration: 676, Loss : 0.060369969898928344\n",
      "Iteration: 677, Loss : 0.059973600549503396\n",
      "Iteration: 678, Loss : 0.059569687979670896\n",
      "Iteration: 679, Loss : 0.05909892854854122\n",
      "Iteration: 680, Loss : 0.05869779946240912\n",
      "Iteration: 681, Loss : 0.05826455907999283\n",
      "Iteration: 682, Loss : 0.05781851321128601\n",
      "Iteration: 683, Loss : 0.0573545766266611\n",
      "Iteration: 684, Loss : 0.056957248867956\n",
      "Iteration: 685, Loss : 0.056503276853316964\n",
      "Iteration: 686, Loss : 0.05610581348727377\n",
      "Iteration: 687, Loss : 0.05567876793054588\n",
      "Iteration: 688, Loss : 0.055280817894576076\n",
      "Iteration: 689, Loss : 0.05485296894208968\n",
      "Iteration: 690, Loss : 0.054408848341325804\n",
      "Iteration: 691, Loss : 0.053963280235680755\n",
      "Iteration: 692, Loss : 0.05354158583333319\n",
      "Iteration: 693, Loss : 0.053125988445996766\n",
      "Iteration: 694, Loss : 0.052718155479464124\n",
      "Iteration: 695, Loss : 0.052305760115634356\n",
      "Iteration: 696, Loss : 0.05189626207894231\n",
      "Iteration: 697, Loss : 0.05152525135473953\n",
      "Iteration: 698, Loss : 0.05108824780984032\n",
      "Iteration: 699, Loss : 0.05072240632994855\n",
      "Iteration: 700, Loss : 0.050345195973290346\n",
      "Iteration: 701, Loss : 0.04993097103732351\n",
      "Iteration: 702, Loss : 0.04949682001744143\n",
      "Iteration: 703, Loss : 0.04909787128717369\n",
      "Iteration: 704, Loss : 0.048683809926042695\n",
      "Iteration: 705, Loss : 0.04827397482441595\n",
      "Iteration: 706, Loss : 0.047876518274190205\n",
      "Iteration: 707, Loss : 0.04749649142407808\n",
      "Iteration: 708, Loss : 0.047102323400191805\n",
      "Iteration: 709, Loss : 0.04665036683822578\n",
      "Iteration: 710, Loss : 0.04626642778162453\n",
      "Iteration: 711, Loss : 0.0458326613370978\n",
      "Iteration: 712, Loss : 0.045408942797097035\n",
      "Iteration: 713, Loss : 0.04497991268385196\n",
      "Iteration: 714, Loss : 0.0445819309810377\n",
      "Iteration: 715, Loss : 0.04417906610284505\n",
      "Iteration: 716, Loss : 0.043745785820503875\n",
      "Iteration: 717, Loss : 0.043315688628876046\n",
      "Iteration: 718, Loss : 0.04292121031390583\n",
      "Iteration: 719, Loss : 0.04251718421046902\n",
      "Iteration: 720, Loss : 0.042104181501576854\n",
      "Iteration: 721, Loss : 0.04164279134471002\n",
      "Iteration: 722, Loss : 0.04122348772206029\n",
      "Iteration: 723, Loss : 0.040804401534442404\n",
      "Iteration: 724, Loss : 0.040381586964643224\n",
      "Iteration: 725, Loss : 0.0399403045438948\n",
      "Iteration: 726, Loss : 0.03946631408501941\n",
      "Iteration: 727, Loss : 0.03905866991149484\n",
      "Iteration: 728, Loss : 0.03857460244701537\n",
      "Iteration: 729, Loss : 0.03812120041866774\n",
      "Iteration: 730, Loss : 0.03764663160862497\n",
      "Iteration: 731, Loss : 0.03718747605730415\n",
      "Iteration: 732, Loss : 0.036726192217922685\n",
      "Iteration: 733, Loss : 0.036268422400340894\n",
      "Iteration: 734, Loss : 0.03579605395495284\n",
      "Iteration: 735, Loss : 0.035323415216649764\n",
      "Iteration: 736, Loss : 0.03489236449732736\n",
      "Iteration: 737, Loss : 0.034432714299142325\n",
      "Iteration: 738, Loss : 0.03400799745859136\n",
      "Iteration: 739, Loss : 0.03351181594965723\n",
      "Iteration: 740, Loss : 0.033052884124157994\n",
      "Iteration: 741, Loss : 0.032589426035358596\n",
      "Iteration: 742, Loss : 0.032089341285035045\n",
      "Iteration: 743, Loss : 0.031606254987696064\n",
      "Iteration: 744, Loss : 0.031115112306500003\n",
      "Iteration: 745, Loss : 0.03062496842311549\n",
      "Iteration: 746, Loss : 0.03020129846610634\n",
      "Iteration: 747, Loss : 0.029735154072547725\n",
      "Iteration: 748, Loss : 0.029254056689796196\n",
      "Iteration: 749, Loss : 0.028821256319782954\n",
      "Iteration: 750, Loss : 0.028356064537730614\n",
      "Iteration: 751, Loss : 0.02791849937628818\n",
      "Iteration: 752, Loss : 0.027447792482536874\n",
      "Iteration: 753, Loss : 0.026961475426186765\n",
      "Iteration: 754, Loss : 0.026511105170106016\n",
      "Iteration: 755, Loss : 0.02605538085818381\n",
      "Iteration: 756, Loss : 0.02562581862355385\n",
      "Iteration: 757, Loss : 0.025236316721273664\n",
      "Iteration: 758, Loss : 0.024851628363535716\n",
      "Iteration: 759, Loss : 0.024436679935035288\n",
      "Iteration: 760, Loss : 0.024016350840532802\n",
      "Iteration: 761, Loss : 0.023638250276466568\n",
      "Iteration: 762, Loss : 0.023251181810744807\n",
      "Iteration: 763, Loss : 0.022853190587626013\n",
      "Iteration: 764, Loss : 0.022488977886279037\n",
      "Iteration: 765, Loss : 0.022153016290564775\n",
      "Iteration: 766, Loss : 0.02178077831801943\n",
      "Iteration: 767, Loss : 0.021456492572830103\n",
      "Iteration: 768, Loss : 0.021097281779291654\n",
      "Iteration: 769, Loss : 0.02075240039830172\n",
      "Iteration: 770, Loss : 0.020415411216256427\n",
      "Iteration: 771, Loss : 0.020102237624477083\n",
      "Iteration: 772, Loss : 0.019803724634121736\n",
      "Iteration: 773, Loss : 0.0194967765051131\n",
      "Iteration: 774, Loss : 0.019184423563259796\n",
      "Iteration: 775, Loss : 0.018875630833237206\n",
      "Iteration: 776, Loss : 0.0185833510039771\n",
      "Iteration: 777, Loss : 0.01832804530740284\n",
      "Iteration: 778, Loss : 0.018024982063166457\n",
      "Iteration: 779, Loss : 0.0177355849162819\n",
      "Iteration: 780, Loss : 0.017494565496604865\n",
      "Iteration: 781, Loss : 0.017211471358890278\n",
      "Iteration: 782, Loss : 0.016966280701459745\n",
      "Iteration: 783, Loss : 0.01669123377975578\n",
      "Iteration: 784, Loss : 0.01645407366465829\n",
      "Iteration: 785, Loss : 0.016223801425764465\n",
      "Iteration: 786, Loss : 0.015953421221211943\n",
      "Iteration: 787, Loss : 0.015733114583551536\n",
      "Iteration: 788, Loss : 0.015494200806473794\n",
      "Iteration: 789, Loss : 0.015273022214722907\n",
      "Iteration: 790, Loss : 0.015029571263325162\n",
      "Iteration: 791, Loss : 0.014812291336077353\n",
      "Iteration: 792, Loss : 0.014561019546452187\n",
      "Iteration: 793, Loss : 0.014356382543085067\n",
      "Iteration: 794, Loss : 0.01412735093895716\n",
      "Iteration: 795, Loss : 0.013902105669034468\n",
      "Iteration: 796, Loss : 0.013685569082441024\n",
      "Iteration: 797, Loss : 0.013471392520815669\n",
      "Iteration: 798, Loss : 0.013261403782803407\n",
      "Iteration: 799, Loss : 0.013070570079130919\n",
      "Iteration: 800, Loss : 0.01286981163606581\n",
      "Iteration: 801, Loss : 0.012679541732022405\n",
      "Iteration: 802, Loss : 0.012479255364073124\n",
      "Iteration: 803, Loss : 0.012301990410026869\n",
      "Iteration: 804, Loss : 0.012126326218774046\n",
      "Iteration: 805, Loss : 0.011927475480610454\n",
      "Iteration: 806, Loss : 0.011737253027069737\n",
      "Iteration: 807, Loss : 0.011571157511237764\n",
      "Iteration: 808, Loss : 0.011388500042392913\n",
      "Iteration: 809, Loss : 0.01121007402221261\n",
      "Iteration: 810, Loss : 0.01104594543903546\n",
      "Iteration: 811, Loss : 0.010875933087543636\n",
      "Iteration: 812, Loss : 0.010696015144303999\n",
      "Iteration: 813, Loss : 0.010537341167290293\n",
      "Iteration: 814, Loss : 0.01036310566081893\n",
      "Iteration: 815, Loss : 0.010211717614140251\n",
      "Iteration: 816, Loss : 0.01004524866332339\n",
      "Iteration: 817, Loss : 0.00988458048451027\n",
      "Iteration: 818, Loss : 0.009727222542043012\n",
      "Iteration: 819, Loss : 0.009585797672429185\n",
      "Iteration: 820, Loss : 0.009433972895262387\n",
      "Iteration: 821, Loss : 0.009291783007246987\n",
      "Iteration: 822, Loss : 0.009138118528692198\n",
      "Iteration: 823, Loss : 0.008993134097731956\n",
      "Iteration: 824, Loss : 0.008864369860505646\n",
      "Iteration: 825, Loss : 0.008719034865385543\n",
      "Iteration: 826, Loss : 0.008593771022007209\n",
      "Iteration: 827, Loss : 0.008467876170885413\n",
      "Iteration: 828, Loss : 0.008330437096324774\n",
      "Iteration: 829, Loss : 0.00820075632896361\n",
      "Iteration: 830, Loss : 0.008065066927614037\n",
      "Iteration: 831, Loss : 0.007942721926399509\n",
      "Iteration: 832, Loss : 0.007823434168468254\n",
      "Iteration: 833, Loss : 0.007701138845928386\n",
      "Iteration: 834, Loss : 0.007584484998838502\n",
      "Iteration: 835, Loss : 0.007459695450833467\n",
      "Iteration: 836, Loss : 0.007333795404293984\n",
      "Iteration: 837, Loss : 0.007213903519262199\n",
      "Iteration: 838, Loss : 0.007099167842478776\n",
      "Iteration: 839, Loss : 0.0069868320405736765\n",
      "Iteration: 840, Loss : 0.006882314068403044\n",
      "Iteration: 841, Loss : 0.006770522799607619\n",
      "Iteration: 842, Loss : 0.0066676802895649645\n",
      "Iteration: 843, Loss : 0.006560948541066938\n",
      "Iteration: 844, Loss : 0.006453981726325755\n",
      "Iteration: 845, Loss : 0.006347597336133777\n",
      "Iteration: 846, Loss : 0.006252561171129334\n",
      "Iteration: 847, Loss : 0.006147641486933831\n",
      "Iteration: 848, Loss : 0.0060577307066349865\n",
      "Iteration: 849, Loss : 0.005957533440245654\n",
      "Iteration: 850, Loss : 0.005864770957267073\n",
      "Iteration: 851, Loss : 0.005771411128309565\n",
      "Iteration: 852, Loss : 0.005677507719822195\n",
      "Iteration: 853, Loss : 0.005586692704135949\n",
      "Iteration: 854, Loss : 0.005497645804971293\n",
      "Iteration: 855, Loss : 0.005414689353456232\n",
      "Iteration: 856, Loss : 0.005330457570834635\n",
      "Iteration: 857, Loss : 0.005245990725500493\n",
      "Iteration: 858, Loss : 0.005158328593324459\n",
      "Iteration: 859, Loss : 0.00507612549861545\n",
      "Iteration: 860, Loss : 0.00499669036444255\n",
      "Iteration: 861, Loss : 0.004922783163148049\n",
      "Iteration: 862, Loss : 0.004839553880131287\n",
      "Iteration: 863, Loss : 0.004766543812195791\n",
      "Iteration: 864, Loss : 0.004690971306832594\n",
      "Iteration: 865, Loss : 0.004610274575720695\n",
      "Iteration: 866, Loss : 0.004538762223461789\n",
      "Iteration: 867, Loss : 0.0044631660182130155\n",
      "Iteration: 868, Loss : 0.004397225474238384\n",
      "Iteration: 869, Loss : 0.004330798893186868\n",
      "Iteration: 870, Loss : 0.004256182812790589\n",
      "Iteration: 871, Loss : 0.004187322661750887\n",
      "Iteration: 872, Loss : 0.004116782229069417\n",
      "Iteration: 873, Loss : 0.00405108293421789\n",
      "Iteration: 874, Loss : 0.0039869323783493435\n",
      "Iteration: 875, Loss : 0.003921746842037822\n",
      "Iteration: 876, Loss : 0.003862429898601472\n",
      "Iteration: 877, Loss : 0.0037990468477174678\n",
      "Iteration: 878, Loss : 0.003743056362621607\n",
      "Iteration: 879, Loss : 0.0036816589004730173\n",
      "Iteration: 880, Loss : 0.0036203651415373456\n",
      "Iteration: 881, Loss : 0.0035637338630817206\n",
      "Iteration: 882, Loss : 0.003503802242019408\n",
      "Iteration: 883, Loss : 0.0034460085787352446\n",
      "Iteration: 884, Loss : 0.0033928516793755867\n",
      "Iteration: 885, Loss : 0.0033378210137879737\n",
      "Iteration: 886, Loss : 0.0032879732212553097\n",
      "Iteration: 887, Loss : 0.0032324273631851985\n",
      "Iteration: 888, Loss : 0.0031775048743551324\n",
      "Iteration: 889, Loss : 0.0031258570000445964\n",
      "Iteration: 890, Loss : 0.0030763117967770874\n",
      "Iteration: 891, Loss : 0.0030254099325587426\n",
      "Iteration: 892, Loss : 0.002977992082122147\n",
      "Iteration: 893, Loss : 0.0029302209357882645\n",
      "Iteration: 894, Loss : 0.00288788991148798\n",
      "Iteration: 895, Loss : 0.002844119613587608\n",
      "Iteration: 896, Loss : 0.0028007072577957083\n",
      "Iteration: 897, Loss : 0.0027504921556733004\n",
      "Iteration: 898, Loss : 0.0027083223900762204\n",
      "Iteration: 899, Loss : 0.0026646290541521836\n",
      "Iteration: 900, Loss : 0.002619723496517278\n",
      "Iteration: 901, Loss : 0.002576521274722296\n",
      "Iteration: 902, Loss : 0.002534482247577478\n",
      "Iteration: 903, Loss : 0.0024954432178175123\n",
      "Iteration: 904, Loss : 0.0024578353251045433\n",
      "Iteration: 905, Loss : 0.002417414864786948\n",
      "Iteration: 906, Loss : 0.0023801858758910575\n",
      "Iteration: 907, Loss : 0.0023418263166446805\n",
      "Iteration: 908, Loss : 0.0023013751140114245\n",
      "Iteration: 909, Loss : 0.0022623678200118045\n",
      "Iteration: 910, Loss : 0.0022272545800164022\n",
      "Iteration: 911, Loss : 0.0021932067758637565\n",
      "Iteration: 912, Loss : 0.0021581011187331175\n",
      "Iteration: 913, Loss : 0.0021227012908320328\n",
      "Iteration: 914, Loss : 0.0020864152307864233\n",
      "Iteration: 915, Loss : 0.0020522562927817976\n",
      "Iteration: 916, Loss : 0.002018638055005077\n",
      "Iteration: 917, Loss : 0.0019845768805188757\n",
      "Iteration: 918, Loss : 0.0019553404276069844\n",
      "Iteration: 919, Loss : 0.001921828247350517\n",
      "Iteration: 920, Loss : 0.0018903506482256139\n",
      "Iteration: 921, Loss : 0.0018613834351419373\n",
      "Iteration: 922, Loss : 0.0018313448791057994\n",
      "Iteration: 923, Loss : 0.0018008067812850588\n",
      "Iteration: 924, Loss : 0.0017715036683123497\n",
      "Iteration: 925, Loss : 0.0017425597168704615\n",
      "Iteration: 926, Loss : 0.0017155912454619728\n",
      "Iteration: 927, Loss : 0.0016888009639364912\n",
      "Iteration: 928, Loss : 0.0016634846214507338\n",
      "Iteration: 929, Loss : 0.0016366378072192006\n",
      "Iteration: 930, Loss : 0.0016090111646182722\n",
      "Iteration: 931, Loss : 0.0015840972714560339\n",
      "Iteration: 932, Loss : 0.0015579735970923823\n",
      "Iteration: 933, Loss : 0.0015333468506006946\n",
      "Iteration: 934, Loss : 0.0015065946385184456\n",
      "Iteration: 935, Loss : 0.001483598326888921\n",
      "Iteration: 936, Loss : 0.0014601808640246018\n",
      "Iteration: 937, Loss : 0.0014351826474300526\n",
      "Iteration: 938, Loss : 0.0014116122470167083\n",
      "Iteration: 939, Loss : 0.0013905125773449277\n",
      "Iteration: 940, Loss : 0.0013670788065102651\n",
      "Iteration: 941, Loss : 0.0013461460951493189\n",
      "Iteration: 942, Loss : 0.001323760800382433\n",
      "Iteration: 943, Loss : 0.0013027917569053095\n",
      "Iteration: 944, Loss : 0.0012805318218647613\n",
      "Iteration: 945, Loss : 0.0012591605440402096\n",
      "Iteration: 946, Loss : 0.0012380604139568523\n",
      "Iteration: 947, Loss : 0.0012196666651672588\n",
      "Iteration: 948, Loss : 0.0012002932936461604\n",
      "Iteration: 949, Loss : 0.0011813456011513955\n",
      "Iteration: 950, Loss : 0.0011611512584190184\n",
      "Iteration: 951, Loss : 0.0011414193671251006\n",
      "Iteration: 952, Loss : 0.001123181491099884\n",
      "Iteration: 953, Loss : 0.001105193927917892\n",
      "Iteration: 954, Loss : 0.0010874013141343416\n",
      "Iteration: 955, Loss : 0.0010689841855488777\n",
      "Iteration: 956, Loss : 0.0010527705929303478\n",
      "Iteration: 957, Loss : 0.0010364753947720716\n",
      "Iteration: 958, Loss : 0.0010197820987420262\n",
      "Iteration: 959, Loss : 0.0010046044872455621\n",
      "Iteration: 960, Loss : 0.000986762908233193\n",
      "Iteration: 961, Loss : 0.0009720039046305814\n",
      "Iteration: 962, Loss : 0.0009559419289507478\n",
      "Iteration: 963, Loss : 0.0009412947182221294\n",
      "Iteration: 964, Loss : 0.0009255960054290174\n",
      "Iteration: 965, Loss : 0.0009099091193852648\n",
      "Iteration: 966, Loss : 0.0008947729190412234\n",
      "Iteration: 967, Loss : 0.000881308745994213\n",
      "Iteration: 968, Loss : 0.0008660006033549873\n",
      "Iteration: 969, Loss : 0.0008519893122527081\n",
      "Iteration: 970, Loss : 0.0008390904987048533\n",
      "Iteration: 971, Loss : 0.000826119726718823\n",
      "Iteration: 972, Loss : 0.0008121230313527148\n",
      "Iteration: 973, Loss : 0.0007983577626272159\n",
      "Iteration: 974, Loss : 0.0007858009001751626\n",
      "Iteration: 975, Loss : 0.0007733164555567434\n",
      "Iteration: 976, Loss : 0.0007609600986794175\n",
      "Iteration: 977, Loss : 0.0007478731930810555\n",
      "Iteration: 978, Loss : 0.0007357702645812425\n",
      "Iteration: 979, Loss : 0.0007242063805892416\n",
      "Iteration: 980, Loss : 0.0007131278215858491\n",
      "Iteration: 981, Loss : 0.0007010581682556851\n",
      "Iteration: 982, Loss : 0.000690082281230097\n",
      "Iteration: 983, Loss : 0.0006785687022928877\n",
      "Iteration: 984, Loss : 0.0006673261456710966\n",
      "Iteration: 985, Loss : 0.0006561544491429955\n",
      "Iteration: 986, Loss : 0.0006463794769721553\n",
      "Iteration: 987, Loss : 0.0006362736518605513\n",
      "Iteration: 988, Loss : 0.0006257670268733243\n",
      "Iteration: 989, Loss : 0.0006158458999562188\n",
      "Iteration: 990, Loss : 0.0006055333122640104\n",
      "Iteration: 991, Loss : 0.0005962635451772275\n",
      "Iteration: 992, Loss : 0.0005868546480713025\n",
      "Iteration: 993, Loss : 0.0005771298912308185\n",
      "Iteration: 994, Loss : 0.0005683963098479458\n",
      "Iteration: 995, Loss : 0.0005591072857411177\n",
      "Iteration: 996, Loss : 0.0005501754030733296\n",
      "Iteration: 997, Loss : 0.0005410949497282866\n",
      "Iteration: 998, Loss : 0.0005319667971063011\n",
      "Iteration: 999, Loss : 0.0005238796288863231\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.00636636],\n",
       "       [0.97875838],\n",
       "       [0.97868883],\n",
       "       [0.03390598]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
