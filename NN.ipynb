{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T09:52:36.933057Z",
     "start_time": "2024-07-10T09:52:36.851492Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "def mean_squared_error(predictions, targets):\n",
    "    return np.mean((predictions - targets) ** 2)\n",
    "\n",
    "def mean_squared_error_derivative(predictions, targets):\n",
    "    return 2 * (predictions - targets) / targets.size\n",
    "def binary_cross_entropy(output,target):\n",
    "    output = np.clip(output, 1e-9, 1 - 1e-9)\n",
    "    return -np.sum([y*np.log(p) + (1-y)*np.log(1-p) for y,p in zip(target,output)])\n",
    "def categorical_cross_entropy(predictions, targets):\n",
    "    predictions = np.clip(predictions, 1e-9, 1 - 1e-9)\n",
    "    return -np.sum(targets * np.log(predictions)) / targets.shape[0]\n",
    "def cross_entropy_derivative(output, target):\n",
    "    return output - target  \n",
    "\n",
    "\"\"\"\n",
    "Инициализация весов с использованием He Initialization.\n",
    "\n",
    "Аргументы:\n",
    "shape -- кортеж, определяющий размерность матрицы весов (например, (n_l, n_{l-1}))\n",
    "\n",
    "Возвращает:\n",
    "weights -- инициализированная матрица весов с размером shape, где weights(a, b), a - число нейронов в слое, b - число входов с предыдущего слоя\n",
    "\"\"\"\n",
    "def he_initialization(shape):\n",
    "    if type(shape) == int  or type(shape) == np.int64:\n",
    "        stddev = np.sqrt(2.0 / shape)\n",
    "        weights = np.random.randn(shape) * stddev\n",
    "    else:\n",
    "        stddev = np.sqrt(2.0 / shape[1])\n",
    "        weights = np.random.randn(*shape) * stddev\n",
    "\n",
    "    return weights\n",
    "def xavier_initialization(shape):\n",
    "    if type(shape) == int or type(shape) == np.int64:\n",
    "        stddev = np.sqrt(6.0)/shape\n",
    "        weights = np.random.randn(shape) * stddev\n",
    "    else:\n",
    "        stddev = np.sqrt(6.0) / np.sqrt(shape[0] + shape[1])\n",
    "        weights = np.random.randn(*shape) * stddev\n",
    "    return weights\n",
    "\n",
    "def leCun_initialization(shape):\n",
    "    if type(shape) == int or type(shape) == np.int64:\n",
    "        stddev = np.sqrt(1.0 / shape)\n",
    "        weights = np.random.randn(shape) * stddev\n",
    "    else:\n",
    "        stddev = np.sqrt(1.0 / shape[1])\n",
    "        weights = np.random.randn(*shape) * stddev\n",
    "    return weights"
   ],
   "id": "76fb398706dc7aab",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-10T09:52:36.956603Z",
     "start_time": "2024-07-10T09:52:36.934741Z"
    }
   },
   "source": [
    "import numpy as np \n",
    "class Layer:\n",
    "    def __init__(self, input_size, output_size,activation=None,derivative=None,initialize_function=np.random.randn,batch_norm=False, momentum=0.9, epsilon=1e-5):\n",
    "        self.init = initialize_function\n",
    "        self.weights = self.init(input_size,output_size)\n",
    "        self.bias = self.init(1,output_size)\n",
    "        self.input = None  \n",
    "        self.z = None \n",
    "        self.output = None \n",
    "        self.activation = activation\n",
    "        self.derivative = derivative\n",
    "        self.error = None\n",
    "        self.batch_norm = batch_norm\n",
    "        # Adding Batch Normalization parameters\n",
    "        if self.batch_norm:\n",
    "            self.momentum = momentum\n",
    "            self.gamma =  np.ones((1,output_size))\n",
    "            self.beta = np.zeros((1,output_size))\n",
    "            self.running_mean = np.zeros(output_size)\n",
    "            self.running_var = np.ones(output_size)\n",
    "            self.cache = None\n",
    "        # Adding adam optimization parameters\n",
    "        self.m_w, self.v_w = np.zeros_like(self.weights), np.zeros_like(self.weights)\n",
    "        self.m_b, self.v_b = np.zeros_like(self.bias), np.zeros_like(self.bias)\n",
    "        self.beta1, self.beta2 = 0.9,0.99\n",
    "        self.t = 0\n",
    "        self.epsilon = epsilon\n",
    "    def __call__(self,X,train=True):\n",
    "        self.input = X \n",
    "        self.z = np.dot(X, self.weights) + self.bias\n",
    "        if self.batch_norm:\n",
    "            if train:\n",
    "                mean = np.mean(self.z, axis=0)\n",
    "                var = np.var(self.z, axis=0)\n",
    "                z_norm = (self.z - mean) / np.sqrt(var + self.epsilon)\n",
    "                self.output = self.gamma * z_norm + self.beta\n",
    "                self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * mean\n",
    "                self.running_var = self.momentum * self.running_var + (1 - self.momentum) * var\n",
    "                self.cache = (self.z, z_norm, mean, var)\n",
    "            else:\n",
    "                self.z_norm = (self.z - self.running_mean) / np.sqrt(self.running_var + self.epsilon)\n",
    "                self.output = self.gamma * self.z_norm + self.beta  \n",
    "        else:\n",
    "            self.output = self.activation(self.z)\n",
    "        return self.output \n",
    "    def backward(self,error, learning_rate,L2,adam_optimize,clipping_mode='norm',clip_threshold=1.0):\n",
    "        self.error = error *  self.derivative(self.z)\n",
    "        self.t += 1\n",
    "        penalty = self.L2(L2,self.weights) if L2 is not None else 0\n",
    "        self.gradient_clipping(clipping_mode,clip_threshold)\n",
    "        if self.batch_norm:\n",
    "            self.error, self.gamma,self.beta = self.batch_normalization(self.error,self.gamma, self.beta,learning_rate)\n",
    "        if adam_optimize: \n",
    "            self.adam(learning_rate)\n",
    "        else:\n",
    "            self.weights -= learning_rate * (np.dot(self.input.T, self.error) + penalty)\n",
    "            self.bias -= learning_rate * np.sum(self.error,axis=0,keepdims=True)\n",
    "\n",
    "        propagated_error = np.dot(self.error, self.weights.T)\n",
    "        return propagated_error\n",
    "\n",
    "    def adam(self, learning_rate):\n",
    "        self.m_w = self.beta1 * self.m_w + (1 - self.beta1) * np.dot(self.input.T, self.error)\n",
    "        self.v_w = self.beta2 * self.v_w + (1 - self.beta2) * (np.dot(self.input.T, self.error) ** 2)\n",
    "        m_w_hat = self.m_w / (1 - self.beta1 ** self.t)\n",
    "        v_w_hat = self.v_w / (1 - self.beta2 ** self.t)\n",
    "        self.weights -= learning_rate * m_w_hat / (np.sqrt(v_w_hat) + self.epsilon)\n",
    "\n",
    "        self.m_b = self.beta1 * self.m_b + (1 - self.beta1) * np.sum(self.error, axis=0, keepdims=True)\n",
    "        self.v_b = self.beta2 * self.v_b + (1 - self.beta2) * (np.sum(self.error, axis=0, keepdims=True) ** 2)\n",
    "        m_b_hat = self.m_b / (1 - self.beta1 ** self.t)\n",
    "        v_b_hat = self.v_b / (1 - self.beta2 ** self.t)\n",
    "        self.bias -= learning_rate * m_b_hat / (np.sqrt(v_b_hat) + self.epsilon)\n",
    "    def gradient_clipping(self,mode,clip_threshold):\n",
    "        if mode == 'value':\n",
    "            self.error = np.clip(self.error, -clip_threshold, clip_threshold)\n",
    "        elif mode == 'norm':\n",
    "            norm = np.linalg.norm(self.error)\n",
    "            if norm > clip_threshold:\n",
    "                self.error = self.error / norm * clip_threshold\n",
    "        else:\n",
    "            return  \n",
    "    def batch_normalization(self,error,gamma,beta, learning_rate):\n",
    "        z , z_norm , mean , var = self.cache\n",
    "        m = z.shape[0]\n",
    "        dbeta = np.sum(error,axis=0)\n",
    "        dgamma = np.sum(error * z_norm,axis=0)\n",
    "        dz_norm = error * self.gamma\n",
    "        dvar = np.sum(dz_norm * (z - mean) * -0.5 * (var + self.epsilon)**-1.5, axis=0)\n",
    "        dmean = np.sum(dz_norm * -1.0 / np.sqrt(var + self.epsilon), axis=0) + dvar * np.mean(-2.0 * (z - mean), axis=0)\n",
    "\n",
    "        error = dz_norm * 1.0 / np.sqrt(var + self.epsilon) + dvar * 2.0 * (z - mean) / m + dmean / m\n",
    "        gamma -= learning_rate * dgamma\n",
    "        beta -= learning_rate * dbeta\n",
    "        return error, gamma, beta \n",
    "    def L2(self, C, weights):\n",
    "        return 2/C * weights\n",
    "    def set_initialization(self,func):\n",
    "        self.init = func\n",
    "class NeuralNetwork:\n",
    "    def __init__(self,sizes,activation_func=lambda x : x, derivative_func=lambda x: 1,init_func=np.random.randn,use_batch_norm=True):\n",
    "        self.layers = [\n",
    "            Layer(input_size=input,output_size=output,activation=activation_func,derivative=derivative_func,initialize_function=init_func,batch_norm=use_batch_norm) \n",
    "            for input,output\n",
    "            in zip(sizes[:-1],sizes[1:])\n",
    "        ]\n",
    "        self.loss = None\n",
    "        self.loss_derivative = None \n",
    "    def forward(self,X):\n",
    "        for layer in self.layers:\n",
    "            X = layer(X)\n",
    "        return X \n",
    "    def backward(self,error,learning_rate,L2,adam_optimizer,clipping_mode,clipping_threshold):\n",
    "        for layer in reversed(self.layers):\n",
    "            error = layer.backward(error,learning_rate,L2,adam_optimizer,clipping_mode,clipping_threshold)\n",
    "            \n",
    "    def train(self,X,y,*,epochs=100, learning_rate=10e-4,L2=None,adam_optimizer=False,clipping_mode='norm',clipping_threshold=1.0,batch_size=2):\n",
    "        rows = X.shape[0]\n",
    "        for epoch in range(epochs):\n",
    "            indices = np.random.permutation(rows)\n",
    "            X_permuted = X[indices]\n",
    "            y_permuted = y[indices]\n",
    "            for start in range(0,rows,batch_size):\n",
    "                end = min(start + batch_size, rows)\n",
    "                X_batch = X_permuted[start:end]\n",
    "                y_batch = y_permuted[start:end]\n",
    "                output = self.forward(X_batch)\n",
    "                error = self.loss_derivative(output,y_batch)\n",
    "                self.backward(error,learning_rate,L2,adam_optimizer,clipping_mode,clipping_threshold)\n",
    "            print(f\"Iteration: {epoch}, Loss : {self.loss(self.forward(X),y)}\")\n",
    "    def set_output_function(self,func,derivative):\n",
    "        self.layers[-1].activation =  func \n",
    "        self.layers[-1].derivative = derivative\n",
    "    def set_loss_function(self,func,derivative):\n",
    "        self.loss = func \n",
    "        self.loss_derivative = derivative\n",
    "    def predict(self,X):\n",
    "        for layer in self.layers:\n",
    "            X = layer(X,False)\n",
    "        return X "
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T09:53:10.940648Z",
     "start_time": "2024-07-10T09:53:10.918546Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy.special import softmax\n",
    "\n",
    "digits = load_digits()\n",
    "# One-hot encode the labels\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_onehot = encoder.fit_transform(digits.target.reshape(-1, 1))\n",
    "X_train,X_test,y_train,y_test = train_test_split(digits.data,y_onehot,test_size=0.2)"
   ],
   "id": "134ffaa41bf004aa",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T09:53:12.161025Z",
     "start_time": "2024-07-10T09:53:12.156954Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 80\n",
    "size = [64,128,32,10]\n",
    "nn = NeuralNetwork(size,relu,relu_derivative,lambda x,y: he_initialization((x,y)),use_batch_norm=True)\n",
    "nn.set_output_function(lambda x: softmax(x), lambda x : softmax(x))\n",
    "nn.set_loss_function(categorical_cross_entropy,lambda x,y : x - y)"
   ],
   "id": "acd412f984b693b6",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T09:53:16.325309Z",
     "start_time": "2024-07-10T09:53:12.995389Z"
    }
   },
   "cell_type": "code",
   "source": "nn.train(X_train,y_train,epochs=100,learning_rate=10e-3,L2=10,clipping_mode='norm',clipping_threshold=1.0,adam_optimizer=False,batch_size=batch_size)",
   "id": "46801ad6749ddb8c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Loss : 11.437662176427967\n",
      "Iteration: 1, Loss : 11.326383092076966\n",
      "Iteration: 2, Loss : 11.37032818915962\n",
      "Iteration: 3, Loss : 11.341968381382609\n",
      "Iteration: 4, Loss : 11.25681875436386\n",
      "Iteration: 5, Loss : 11.085937901312835\n",
      "Iteration: 6, Loss : 11.059317175752588\n",
      "Iteration: 7, Loss : 11.016567629328689\n",
      "Iteration: 8, Loss : 10.949255342055165\n",
      "Iteration: 9, Loss : 10.847183044275962\n",
      "Iteration: 10, Loss : 10.769190866215359\n",
      "Iteration: 11, Loss : 10.672937229209689\n",
      "Iteration: 12, Loss : 10.550011013109621\n",
      "Iteration: 13, Loss : 10.499053840336162\n",
      "Iteration: 14, Loss : 10.402743306124018\n",
      "Iteration: 15, Loss : 10.306485521243722\n",
      "Iteration: 16, Loss : 10.254527176514689\n",
      "Iteration: 17, Loss : 10.210400655136658\n",
      "Iteration: 18, Loss : 10.0195288760453\n",
      "Iteration: 19, Loss : 9.908534949486164\n",
      "Iteration: 20, Loss : 9.732933290498188\n",
      "Iteration: 21, Loss : 9.600185630573346\n",
      "Iteration: 22, Loss : 9.449245227798956\n",
      "Iteration: 23, Loss : 9.297664845306008\n",
      "Iteration: 24, Loss : 9.147879483304024\n",
      "Iteration: 25, Loss : 8.970124635507506\n",
      "Iteration: 26, Loss : 8.877295175417222\n",
      "Iteration: 27, Loss : 8.731592548959735\n",
      "Iteration: 28, Loss : 8.47590160094251\n",
      "Iteration: 29, Loss : 8.334565190272144\n",
      "Iteration: 30, Loss : 8.11320872662789\n",
      "Iteration: 31, Loss : 7.9210310248451545\n",
      "Iteration: 32, Loss : 7.7448568544218235\n",
      "Iteration: 33, Loss : 7.546457457741174\n",
      "Iteration: 34, Loss : 7.277596760170595\n",
      "Iteration: 35, Loss : 7.11620221170113\n",
      "Iteration: 36, Loss : 6.821050343253793\n",
      "Iteration: 37, Loss : 6.613720439254626\n",
      "Iteration: 38, Loss : 6.41932823693827\n",
      "Iteration: 39, Loss : 6.075239512918183\n",
      "Iteration: 40, Loss : 5.688882514437222\n",
      "Iteration: 41, Loss : 5.377900465071006\n",
      "Iteration: 42, Loss : 5.0905002445716345\n",
      "Iteration: 43, Loss : 4.788128016167204\n",
      "Iteration: 44, Loss : 4.409125999762766\n",
      "Iteration: 45, Loss : 4.06935636989454\n",
      "Iteration: 46, Loss : 3.63662320244223\n",
      "Iteration: 47, Loss : 3.393468996773624\n",
      "Iteration: 48, Loss : 3.1666968361717256\n",
      "Iteration: 49, Loss : 2.800424875212637\n",
      "Iteration: 50, Loss : 2.5339116569960973\n",
      "Iteration: 51, Loss : 2.2709204968888352\n",
      "Iteration: 52, Loss : 2.0461060724038598\n",
      "Iteration: 53, Loss : 1.9270111458397299\n",
      "Iteration: 54, Loss : 1.767299661129819\n",
      "Iteration: 55, Loss : 1.626394773885611\n",
      "Iteration: 56, Loss : 1.5314872394248438\n",
      "Iteration: 57, Loss : 1.4048896003665154\n",
      "Iteration: 58, Loss : 1.2825340884752585\n",
      "Iteration: 59, Loss : 1.1810342284718\n",
      "Iteration: 60, Loss : 1.0876184311401935\n",
      "Iteration: 61, Loss : 1.0164411446289905\n",
      "Iteration: 62, Loss : 0.9484354632414984\n",
      "Iteration: 63, Loss : 0.8704041318936824\n",
      "Iteration: 64, Loss : 0.8191990530848903\n",
      "Iteration: 65, Loss : 0.7889829281530257\n",
      "Iteration: 66, Loss : 0.7635207002040535\n",
      "Iteration: 67, Loss : 0.7175446500353527\n",
      "Iteration: 68, Loss : 0.7030508251486558\n",
      "Iteration: 69, Loss : 0.6623595017609821\n",
      "Iteration: 70, Loss : 0.6374932276964452\n",
      "Iteration: 71, Loss : 0.6383617437995993\n",
      "Iteration: 72, Loss : 0.6199813427345294\n",
      "Iteration: 73, Loss : 0.5901280772729494\n",
      "Iteration: 74, Loss : 0.5952495731570561\n",
      "Iteration: 75, Loss : 0.5781642116720895\n",
      "Iteration: 76, Loss : 0.5834979718452041\n",
      "Iteration: 77, Loss : 0.5562187853957183\n",
      "Iteration: 78, Loss : 0.563117408525951\n",
      "Iteration: 79, Loss : 0.5603915478064191\n",
      "Iteration: 80, Loss : 0.5461125270922543\n",
      "Iteration: 81, Loss : 0.5425927288722621\n",
      "Iteration: 82, Loss : 0.5407094621288899\n",
      "Iteration: 83, Loss : 0.5035754098967395\n",
      "Iteration: 84, Loss : 0.4890017314821794\n",
      "Iteration: 85, Loss : 0.4633738833587944\n",
      "Iteration: 86, Loss : 0.4721277498506231\n",
      "Iteration: 87, Loss : 0.4641586215924674\n",
      "Iteration: 88, Loss : 0.44888193487794703\n",
      "Iteration: 89, Loss : 0.4485732201288082\n",
      "Iteration: 90, Loss : 0.4354991116239282\n",
      "Iteration: 91, Loss : 0.4457407745149448\n",
      "Iteration: 92, Loss : 0.43678830012184033\n",
      "Iteration: 93, Loss : 0.4379517769336337\n",
      "Iteration: 94, Loss : 0.43267869497175315\n",
      "Iteration: 95, Loss : 0.4358117248688015\n",
      "Iteration: 96, Loss : 0.4350971890261856\n",
      "Iteration: 97, Loss : 0.42463350972180136\n",
      "Iteration: 98, Loss : 0.4113669759805968\n",
      "Iteration: 99, Loss : 0.44837792023356215\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T09:53:22.563034Z",
     "start_time": "2024-07-10T09:53:22.552820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "# Forward method to get predictions\n",
    "predictions = nn.predict(X_test)\n",
    "# If predictions are in the form of probabilities, convert them to class labels\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "true_labels = np.argmax(y_test, axis=1)\n",
    "predicted_labels"
   ],
   "id": "7825a8083ff57713",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 4, 0, 0, 0, 0, 8, 9, 5, 3, 3, 2, 2, 4, 0, 1, 3, 2, 8, 8, 0, 3,\n",
       "       5, 3, 1, 4, 4, 9, 3, 4, 0, 9, 7, 7, 7, 4, 7, 0, 5, 4, 0, 7, 2, 3,\n",
       "       6, 7, 9, 1, 4, 7, 7, 7, 4, 2, 8, 1, 1, 4, 2, 0, 7, 6, 0, 1, 2, 0,\n",
       "       2, 6, 6, 3, 1, 0, 6, 4, 2, 4, 9, 6, 2, 2, 8, 6, 7, 2, 1, 0, 3, 2,\n",
       "       5, 7, 1, 2, 8, 7, 9, 3, 7, 4, 6, 4, 9, 8, 0, 1, 3, 5, 0, 0, 5, 8,\n",
       "       0, 0, 0, 4, 9, 0, 8, 5, 9, 4, 9, 5, 6, 1, 6, 4, 5, 2, 7, 9, 1, 2,\n",
       "       9, 8, 7, 4, 2, 3, 8, 2, 9, 5, 5, 6, 2, 9, 1, 8, 6, 8, 2, 5, 4, 5,\n",
       "       2, 2, 4, 5, 2, 0, 1, 5, 6, 1, 9, 1, 1, 5, 2, 9, 3, 6, 4, 1, 2, 6,\n",
       "       6, 9, 8, 9, 1, 4, 2, 3, 5, 0, 4, 3, 0, 4, 8, 7, 4, 9, 0, 9, 3, 1,\n",
       "       4, 8, 2, 9, 7, 1, 1, 7, 7, 1, 5, 2, 1, 0, 2, 4, 6, 3, 4, 8, 4, 7,\n",
       "       1, 9, 2, 1, 4, 2, 3, 6, 8, 4, 9, 3, 4, 5, 4, 7, 6, 0, 7, 1, 6, 2,\n",
       "       4, 5, 1, 0, 2, 2, 1, 4, 2, 3, 5, 1, 7, 2, 4, 9, 8, 0, 9, 8, 3, 6,\n",
       "       2, 0, 7, 6, 7, 0, 2, 6, 7, 8, 5, 7, 6, 3, 3, 6, 9, 7, 9, 8, 5, 5,\n",
       "       4, 3, 0, 2, 7, 4, 4, 6, 1, 8, 5, 1, 2, 1, 8, 7, 3, 9, 9, 8, 6, 1,\n",
       "       0, 0, 5, 6, 7, 3, 4, 7, 3, 6, 0, 1, 6, 9, 2, 0, 9, 4, 2, 4, 9, 9,\n",
       "       3, 0, 6, 2, 2, 4, 2, 8, 3, 6, 0, 5, 5, 0, 0, 9, 0, 3, 9, 3, 0, 7,\n",
       "       4, 4, 4, 3, 3, 2, 0, 8])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T09:53:22.938107Z",
     "start_time": "2024-07-10T09:53:22.925582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score,precision_score, recall_score, f1_score, confusion_matrix\n",
    "print(f\"Accuracy score: {accuracy_score(true_labels,predicted_labels)}\")\n",
    "print(precision_score(true_labels, predicted_labels, average='macro'))\n",
    "print(recall_score(true_labels, predicted_labels, average='macro'))\n",
    "print(confusion_matrix(true_labels, predicted_labels))\n",
    "print(f1_score(true_labels, predicted_labels, average='macro'))"
   ],
   "id": "be6dde8e05d1d5a3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.9277777777777778\n",
      "0.9268386766212853\n",
      "0.9223740604038158\n",
      "[[43  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 30  2  0  0  0  0  0  0  1]\n",
      " [ 0  0 41  0  0  0  0  0  0  0]\n",
      " [ 0  0  2 32  0  0  0  0  0  2]\n",
      " [ 1  2  0  0 45  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 28  0  0  0  1]\n",
      " [ 0  0  1  0  0  0 32  0  0  0]\n",
      " [ 0  0  0  0  1  0  0 34  0  1]\n",
      " [ 0  3  0  1  0  0  0  0 23  4]\n",
      " [ 0  0  0  0  0  0  0  0  4 26]]\n",
      "0.9229974009000482\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T09:53:25.980538Z",
     "start_time": "2024-07-10T09:53:25.728682Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define network architecture\n",
    "batch_size = 2\n",
    "sizes = [batch_size, 5,1]\n",
    "\n",
    "# Initialize network\n",
    "nn = NeuralNetwork(sizes, sigmoid, sigmoid_derivative,init_func=lambda x,y: xavier_initialization((x,y)),use_batch_norm=False)\n",
    "nn.set_loss_function(mean_squared_error, mean_squared_error_derivative)\n",
    "nn.set_output_function(sigmoid,sigmoid_derivative)\n",
    "\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y = np.array([[0],[0],[0],[1]])\n",
    "nn.train(X,y,epochs=1000,learning_rate=10e-1,batch_size=2)\n",
    "nn.forward(X)"
   ],
   "id": "11b0b19ca766da1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Loss : 0.23534449897541018\n",
      "Iteration: 1, Loss : 0.17951921927881992\n",
      "Iteration: 2, Loss : 0.1689733809766195\n",
      "Iteration: 3, Loss : 0.1616249405912087\n",
      "Iteration: 4, Loss : 0.15880304636731019\n",
      "Iteration: 5, Loss : 0.1570854558812189\n",
      "Iteration: 6, Loss : 0.15510839106795843\n",
      "Iteration: 7, Loss : 0.153118303169114\n",
      "Iteration: 8, Loss : 0.1511461331985599\n",
      "Iteration: 9, Loss : 0.149134005950864\n",
      "Iteration: 10, Loss : 0.14731058110607181\n",
      "Iteration: 11, Loss : 0.14576858539122858\n",
      "Iteration: 12, Loss : 0.14291104483186165\n",
      "Iteration: 13, Loss : 0.14058706897508277\n",
      "Iteration: 14, Loss : 0.1385003389314597\n",
      "Iteration: 15, Loss : 0.13630031078604932\n",
      "Iteration: 16, Loss : 0.13428549020706992\n",
      "Iteration: 17, Loss : 0.13228550958273463\n",
      "Iteration: 18, Loss : 0.13030561374763427\n",
      "Iteration: 19, Loss : 0.127683571569716\n",
      "Iteration: 20, Loss : 0.1256437685227509\n",
      "Iteration: 21, Loss : 0.12383097244465181\n",
      "Iteration: 22, Loss : 0.12185585133522858\n",
      "Iteration: 23, Loss : 0.11920234682457181\n",
      "Iteration: 24, Loss : 0.11777640216632518\n",
      "Iteration: 25, Loss : 0.11646270346631413\n",
      "Iteration: 26, Loss : 0.11405860970795897\n",
      "Iteration: 27, Loss : 0.11105874362754078\n",
      "Iteration: 28, Loss : 0.1098344364297365\n",
      "Iteration: 29, Loss : 0.10860184027044414\n",
      "Iteration: 30, Loss : 0.1061450719937746\n",
      "Iteration: 31, Loss : 0.10326040897143207\n",
      "Iteration: 32, Loss : 0.10153774817683127\n",
      "Iteration: 33, Loss : 0.0996134455236429\n",
      "Iteration: 34, Loss : 0.09772024984619093\n",
      "Iteration: 35, Loss : 0.09667394388883654\n",
      "Iteration: 36, Loss : 0.09411161993107484\n",
      "Iteration: 37, Loss : 0.09245104971385987\n",
      "Iteration: 38, Loss : 0.0915230058705685\n",
      "Iteration: 39, Loss : 0.09035484218796999\n",
      "Iteration: 40, Loss : 0.08768428768294345\n",
      "Iteration: 41, Loss : 0.08535819597352118\n",
      "Iteration: 42, Loss : 0.0837262623146618\n",
      "Iteration: 43, Loss : 0.0821484765487959\n",
      "Iteration: 44, Loss : 0.08126873274190438\n",
      "Iteration: 45, Loss : 0.07877060632056848\n",
      "Iteration: 46, Loss : 0.07748055335363446\n",
      "Iteration: 47, Loss : 0.07594650130684069\n",
      "Iteration: 48, Loss : 0.07505347948408786\n",
      "Iteration: 49, Loss : 0.07383751520760884\n",
      "Iteration: 50, Loss : 0.07088276641881722\n",
      "Iteration: 51, Loss : 0.0696861444648339\n",
      "Iteration: 52, Loss : 0.06789848105834945\n",
      "Iteration: 53, Loss : 0.06668473926927214\n",
      "Iteration: 54, Loss : 0.06555293941886667\n",
      "Iteration: 55, Loss : 0.06372540276689348\n",
      "Iteration: 56, Loss : 0.06272725720268084\n",
      "Iteration: 57, Loss : 0.06149484692598404\n",
      "Iteration: 58, Loss : 0.0597673630275941\n",
      "Iteration: 59, Loss : 0.05837634181603424\n",
      "Iteration: 60, Loss : 0.05747766171910278\n",
      "Iteration: 61, Loss : 0.05578982286080732\n",
      "Iteration: 62, Loss : 0.054790435501255785\n",
      "Iteration: 63, Loss : 0.053532352992637554\n",
      "Iteration: 64, Loss : 0.0527378153676011\n",
      "Iteration: 65, Loss : 0.05094796433731053\n",
      "Iteration: 66, Loss : 0.05007643373961997\n",
      "Iteration: 67, Loss : 0.04909086712083299\n",
      "Iteration: 68, Loss : 0.04764824259190825\n",
      "Iteration: 69, Loss : 0.04658949026359283\n",
      "Iteration: 70, Loss : 0.04556960552416721\n",
      "Iteration: 71, Loss : 0.044745372132852665\n",
      "Iteration: 72, Loss : 0.04358551458705502\n",
      "Iteration: 73, Loss : 0.04265582402888969\n",
      "Iteration: 74, Loss : 0.0417555840027634\n",
      "Iteration: 75, Loss : 0.04091290266923272\n",
      "Iteration: 76, Loss : 0.03989619066731199\n",
      "Iteration: 77, Loss : 0.039064057690892856\n",
      "Iteration: 78, Loss : 0.038259723083637676\n",
      "Iteration: 79, Loss : 0.03762235633717241\n",
      "Iteration: 80, Loss : 0.036671448384045145\n",
      "Iteration: 81, Loss : 0.03585469782823419\n",
      "Iteration: 82, Loss : 0.035249353428955986\n",
      "Iteration: 83, Loss : 0.0343654260706065\n",
      "Iteration: 84, Loss : 0.0336238726826106\n",
      "Iteration: 85, Loss : 0.03285464073297018\n",
      "Iteration: 86, Loss : 0.03213354767730012\n",
      "Iteration: 87, Loss : 0.0314935525526224\n",
      "Iteration: 88, Loss : 0.03088426712823839\n",
      "Iteration: 89, Loss : 0.030359441751283633\n",
      "Iteration: 90, Loss : 0.029549740930764873\n",
      "Iteration: 91, Loss : 0.02897508168206338\n",
      "Iteration: 92, Loss : 0.028386016143000136\n",
      "Iteration: 93, Loss : 0.027863777482880075\n",
      "Iteration: 94, Loss : 0.027370916953112813\n",
      "Iteration: 95, Loss : 0.026773743465921603\n",
      "Iteration: 96, Loss : 0.026301448055483432\n",
      "Iteration: 97, Loss : 0.025840227673245134\n",
      "Iteration: 98, Loss : 0.025377141902542352\n",
      "Iteration: 99, Loss : 0.024919954181843028\n",
      "Iteration: 100, Loss : 0.024459894456646293\n",
      "Iteration: 101, Loss : 0.02401038405317222\n",
      "Iteration: 102, Loss : 0.023486489985950716\n",
      "Iteration: 103, Loss : 0.023091155930182662\n",
      "Iteration: 104, Loss : 0.022640837289228066\n",
      "Iteration: 105, Loss : 0.022262964418098483\n",
      "Iteration: 106, Loss : 0.02184194847548894\n",
      "Iteration: 107, Loss : 0.021481684133084607\n",
      "Iteration: 108, Loss : 0.021087949180658594\n",
      "Iteration: 109, Loss : 0.02074315392898871\n",
      "Iteration: 110, Loss : 0.020407456404573726\n",
      "Iteration: 111, Loss : 0.020071350402165115\n",
      "Iteration: 112, Loss : 0.019749201106723832\n",
      "Iteration: 113, Loss : 0.01933876240834345\n",
      "Iteration: 114, Loss : 0.01903500529859748\n",
      "Iteration: 115, Loss : 0.018734798745385292\n",
      "Iteration: 116, Loss : 0.018388655175769757\n",
      "Iteration: 117, Loss : 0.018104704051560505\n",
      "Iteration: 118, Loss : 0.01782631243193939\n",
      "Iteration: 119, Loss : 0.017456665677462485\n",
      "Iteration: 120, Loss : 0.017177842028000018\n",
      "Iteration: 121, Loss : 0.016906006626137067\n",
      "Iteration: 122, Loss : 0.0166326342664377\n",
      "Iteration: 123, Loss : 0.016386503745335283\n",
      "Iteration: 124, Loss : 0.01612935120300223\n",
      "Iteration: 125, Loss : 0.015893769683579112\n",
      "Iteration: 126, Loss : 0.015647231210842768\n",
      "Iteration: 127, Loss : 0.015411991695243564\n",
      "Iteration: 128, Loss : 0.015183631626454055\n",
      "Iteration: 129, Loss : 0.01495815515135278\n",
      "Iteration: 130, Loss : 0.014739706177686357\n",
      "Iteration: 131, Loss : 0.014518880154314509\n",
      "Iteration: 132, Loss : 0.014307949482753301\n",
      "Iteration: 133, Loss : 0.014103890282401183\n",
      "Iteration: 134, Loss : 0.013905110194473463\n",
      "Iteration: 135, Loss : 0.01371101381584507\n",
      "Iteration: 136, Loss : 0.013515916292230902\n",
      "Iteration: 137, Loss : 0.013331389858755045\n",
      "Iteration: 138, Loss : 0.013148074425752611\n",
      "Iteration: 139, Loss : 0.012962964737438895\n",
      "Iteration: 140, Loss : 0.012790508699619925\n",
      "Iteration: 141, Loss : 0.012619104386209164\n",
      "Iteration: 142, Loss : 0.012444133918791043\n",
      "Iteration: 143, Loss : 0.012278629653071253\n",
      "Iteration: 144, Loss : 0.012118719178424546\n",
      "Iteration: 145, Loss : 0.0119500931671888\n",
      "Iteration: 146, Loss : 0.011797680294445\n",
      "Iteration: 147, Loss : 0.01165076255667428\n",
      "Iteration: 148, Loss : 0.011503315778782743\n",
      "Iteration: 149, Loss : 0.01135515615347608\n",
      "Iteration: 150, Loss : 0.011212622623201152\n",
      "Iteration: 151, Loss : 0.011076768949277201\n",
      "Iteration: 152, Loss : 0.010942212820895386\n",
      "Iteration: 153, Loss : 0.010812427046529852\n",
      "Iteration: 154, Loss : 0.01068459231083924\n",
      "Iteration: 155, Loss : 0.010548554834211306\n",
      "Iteration: 156, Loss : 0.010426875565269302\n",
      "Iteration: 157, Loss : 0.01029302984964399\n",
      "Iteration: 158, Loss : 0.010173125702526153\n",
      "Iteration: 159, Loss : 0.01005575007331145\n",
      "Iteration: 160, Loss : 0.009939559204392021\n",
      "Iteration: 161, Loss : 0.00982420137769072\n",
      "Iteration: 162, Loss : 0.00971165018810638\n",
      "Iteration: 163, Loss : 0.009604516732143183\n",
      "Iteration: 164, Loss : 0.009495233907937254\n",
      "Iteration: 165, Loss : 0.009390139240315157\n",
      "Iteration: 166, Loss : 0.009287393339583415\n",
      "Iteration: 167, Loss : 0.009183813807571585\n",
      "Iteration: 168, Loss : 0.009084530553135003\n",
      "Iteration: 169, Loss : 0.008981401031384645\n",
      "Iteration: 170, Loss : 0.008886694450863094\n",
      "Iteration: 171, Loss : 0.008787448373793878\n",
      "Iteration: 172, Loss : 0.00869520283815908\n",
      "Iteration: 173, Loss : 0.008602879000724036\n",
      "Iteration: 174, Loss : 0.00851350749125356\n",
      "Iteration: 175, Loss : 0.008425716642420233\n",
      "Iteration: 176, Loss : 0.00833861224638156\n",
      "Iteration: 177, Loss : 0.008253024145893907\n",
      "Iteration: 178, Loss : 0.008169271602224328\n",
      "Iteration: 179, Loss : 0.008087113627963756\n",
      "Iteration: 180, Loss : 0.008006422295930873\n",
      "Iteration: 181, Loss : 0.00792240382385161\n",
      "Iteration: 182, Loss : 0.007844955382753843\n",
      "Iteration: 183, Loss : 0.00776881886029596\n",
      "Iteration: 184, Loss : 0.007693772178099752\n",
      "Iteration: 185, Loss : 0.007619890642901628\n",
      "Iteration: 186, Loss : 0.0075476919436803635\n",
      "Iteration: 187, Loss : 0.0074755428904253185\n",
      "Iteration: 188, Loss : 0.007405727430305361\n",
      "Iteration: 189, Loss : 0.00733545011771827\n",
      "Iteration: 190, Loss : 0.007262697975819663\n",
      "Iteration: 191, Loss : 0.007196254524544077\n",
      "Iteration: 192, Loss : 0.007129512286938503\n",
      "Iteration: 193, Loss : 0.007064889550810019\n",
      "Iteration: 194, Loss : 0.00700003352506301\n",
      "Iteration: 195, Loss : 0.006932768675712983\n",
      "Iteration: 196, Loss : 0.006871150383758237\n",
      "Iteration: 197, Loss : 0.006807168137583199\n",
      "Iteration: 198, Loss : 0.006747327390166355\n",
      "Iteration: 199, Loss : 0.006685908107573985\n",
      "Iteration: 200, Loss : 0.006625734404943391\n",
      "Iteration: 201, Loss : 0.006566145749656745\n",
      "Iteration: 202, Loss : 0.006509638418566015\n",
      "Iteration: 203, Loss : 0.006452129245303615\n",
      "Iteration: 204, Loss : 0.006397106978800382\n",
      "Iteration: 205, Loss : 0.006343304569053294\n",
      "Iteration: 206, Loss : 0.006287972973435333\n",
      "Iteration: 207, Loss : 0.00623553541874425\n",
      "Iteration: 208, Loss : 0.006183541229608712\n",
      "Iteration: 209, Loss : 0.00613280491811104\n",
      "Iteration: 210, Loss : 0.006082903240664022\n",
      "Iteration: 211, Loss : 0.0060326561976012305\n",
      "Iteration: 212, Loss : 0.005981911929627091\n",
      "Iteration: 213, Loss : 0.005932321471113814\n",
      "Iteration: 214, Loss : 0.005883033966484822\n",
      "Iteration: 215, Loss : 0.005835010340719912\n",
      "Iteration: 216, Loss : 0.00578870690240067\n",
      "Iteration: 217, Loss : 0.005742540176823522\n",
      "Iteration: 218, Loss : 0.005696491233281296\n",
      "Iteration: 219, Loss : 0.005652055216581013\n",
      "Iteration: 220, Loss : 0.005607480513476889\n",
      "Iteration: 221, Loss : 0.005563381331391034\n",
      "Iteration: 222, Loss : 0.005520446683878175\n",
      "Iteration: 223, Loss : 0.00547832584149111\n",
      "Iteration: 224, Loss : 0.0054367821468144575\n",
      "Iteration: 225, Loss : 0.005395760397117929\n",
      "Iteration: 226, Loss : 0.005355352457963887\n",
      "Iteration: 227, Loss : 0.005314461675277723\n",
      "Iteration: 228, Loss : 0.005275036116638406\n",
      "Iteration: 229, Loss : 0.005235922399136059\n",
      "Iteration: 230, Loss : 0.005197568423359805\n",
      "Iteration: 231, Loss : 0.005159544320129694\n",
      "Iteration: 232, Loss : 0.005122142824820452\n",
      "Iteration: 233, Loss : 0.005084253074913921\n",
      "Iteration: 234, Loss : 0.005046600115147377\n",
      "Iteration: 235, Loss : 0.005009732626332562\n",
      "Iteration: 236, Loss : 0.004974186406538135\n",
      "Iteration: 237, Loss : 0.004937879760107093\n",
      "Iteration: 238, Loss : 0.004902406270452797\n",
      "Iteration: 239, Loss : 0.004868057586389329\n",
      "Iteration: 240, Loss : 0.004834375462249296\n",
      "Iteration: 241, Loss : 0.00480113122950826\n",
      "Iteration: 242, Loss : 0.004766823084015957\n",
      "Iteration: 243, Loss : 0.004733287314831329\n",
      "Iteration: 244, Loss : 0.004700951618490035\n",
      "Iteration: 245, Loss : 0.004669197051585352\n",
      "Iteration: 246, Loss : 0.004637791340331246\n",
      "Iteration: 247, Loss : 0.004606779999765908\n",
      "Iteration: 248, Loss : 0.0045755731161019105\n",
      "Iteration: 249, Loss : 0.0045452703538008945\n",
      "Iteration: 250, Loss : 0.004515299467391962\n",
      "Iteration: 251, Loss : 0.004485654339701436\n",
      "Iteration: 252, Loss : 0.004455872382931852\n",
      "Iteration: 253, Loss : 0.00442688243983835\n",
      "Iteration: 254, Loss : 0.004396486490891505\n",
      "Iteration: 255, Loss : 0.004367785977316491\n",
      "Iteration: 256, Loss : 0.004339894570894157\n",
      "Iteration: 257, Loss : 0.004311990451583681\n",
      "Iteration: 258, Loss : 0.004284154348921497\n",
      "Iteration: 259, Loss : 0.004257127022097485\n",
      "Iteration: 260, Loss : 0.0042303872653896134\n",
      "Iteration: 261, Loss : 0.0042039735385795245\n",
      "Iteration: 262, Loss : 0.004177791731186917\n",
      "Iteration: 263, Loss : 0.004151482897664609\n",
      "Iteration: 264, Loss : 0.004125447431886188\n",
      "Iteration: 265, Loss : 0.004099568315407326\n",
      "Iteration: 266, Loss : 0.0040741129840096875\n",
      "Iteration: 267, Loss : 0.004049277755746131\n",
      "Iteration: 268, Loss : 0.004023407556943227\n",
      "Iteration: 269, Loss : 0.003998842235085315\n",
      "Iteration: 270, Loss : 0.003974457744882213\n",
      "Iteration: 271, Loss : 0.003950496856447847\n",
      "Iteration: 272, Loss : 0.003927028641191713\n",
      "Iteration: 273, Loss : 0.003903801152691911\n",
      "Iteration: 274, Loss : 0.00388049814931947\n",
      "Iteration: 275, Loss : 0.0038569424401218378\n",
      "Iteration: 276, Loss : 0.003834297067024847\n",
      "Iteration: 277, Loss : 0.00381202645543017\n",
      "Iteration: 278, Loss : 0.0037899588751022454\n",
      "Iteration: 279, Loss : 0.003767748574754084\n",
      "Iteration: 280, Loss : 0.003745372661485208\n",
      "Iteration: 281, Loss : 0.003723599534894082\n",
      "Iteration: 282, Loss : 0.0037023062469074405\n",
      "Iteration: 283, Loss : 0.003681347895050766\n",
      "Iteration: 284, Loss : 0.0036602577682647093\n",
      "Iteration: 285, Loss : 0.0036394875759688684\n",
      "Iteration: 286, Loss : 0.0036189781309501345\n",
      "Iteration: 287, Loss : 0.0035987090928053848\n",
      "Iteration: 288, Loss : 0.003578755730066244\n",
      "Iteration: 289, Loss : 0.003558446591760152\n",
      "Iteration: 290, Loss : 0.003538431807923726\n",
      "Iteration: 291, Loss : 0.0035186352066654642\n",
      "Iteration: 292, Loss : 0.0034994771834832676\n",
      "Iteration: 293, Loss : 0.003480516351808005\n",
      "Iteration: 294, Loss : 0.0034617105586842323\n",
      "Iteration: 295, Loss : 0.0034429906022003247\n",
      "Iteration: 296, Loss : 0.0034245714161726362\n",
      "Iteration: 297, Loss : 0.003406297275546801\n",
      "Iteration: 298, Loss : 0.003388224704402626\n",
      "Iteration: 299, Loss : 0.003370167614959334\n",
      "Iteration: 300, Loss : 0.0033518720540827622\n",
      "Iteration: 301, Loss : 0.0033341888518613953\n",
      "Iteration: 302, Loss : 0.003316352480078548\n",
      "Iteration: 303, Loss : 0.00329903058185023\n",
      "Iteration: 304, Loss : 0.0032815472644675313\n",
      "Iteration: 305, Loss : 0.003264599611647263\n",
      "Iteration: 306, Loss : 0.0032477884959418238\n",
      "Iteration: 307, Loss : 0.0032311660628568644\n",
      "Iteration: 308, Loss : 0.003214220495695461\n",
      "Iteration: 309, Loss : 0.0031978905470036955\n",
      "Iteration: 310, Loss : 0.0031816466486832918\n",
      "Iteration: 311, Loss : 0.0031655581273691037\n",
      "Iteration: 312, Loss : 0.003149629890894054\n",
      "Iteration: 313, Loss : 0.0031337875479126413\n",
      "Iteration: 314, Loss : 0.003118170573812127\n",
      "Iteration: 315, Loss : 0.0031026396433506405\n",
      "Iteration: 316, Loss : 0.0030872216722900615\n",
      "Iteration: 317, Loss : 0.0030719808029529205\n",
      "Iteration: 318, Loss : 0.003056828499782915\n",
      "Iteration: 319, Loss : 0.003041817981389159\n",
      "Iteration: 320, Loss : 0.003026726182122779\n",
      "Iteration: 321, Loss : 0.0030116951351860538\n",
      "Iteration: 322, Loss : 0.0029970831688640965\n",
      "Iteration: 323, Loss : 0.0029826078816638877\n",
      "Iteration: 324, Loss : 0.0029682699304537175\n",
      "Iteration: 325, Loss : 0.002953759115672694\n",
      "Iteration: 326, Loss : 0.002939643555046701\n",
      "Iteration: 327, Loss : 0.002925651041781467\n",
      "Iteration: 328, Loss : 0.0029115616512800525\n",
      "Iteration: 329, Loss : 0.002897829196024095\n",
      "Iteration: 330, Loss : 0.00288421143848038\n",
      "Iteration: 331, Loss : 0.002870680267842848\n",
      "Iteration: 332, Loss : 0.002857041362599481\n",
      "Iteration: 333, Loss : 0.002843516546793294\n",
      "Iteration: 334, Loss : 0.002830345224669927\n",
      "Iteration: 335, Loss : 0.002817282957517866\n",
      "Iteration: 336, Loss : 0.002804111410191997\n",
      "Iteration: 337, Loss : 0.0027912846924729295\n",
      "Iteration: 338, Loss : 0.0027785282551858114\n",
      "Iteration: 339, Loss : 0.002765703666286719\n",
      "Iteration: 340, Loss : 0.00275312219384329\n",
      "Iteration: 341, Loss : 0.002740711255562971\n",
      "Iteration: 342, Loss : 0.0027283571574523964\n",
      "Iteration: 343, Loss : 0.0027160846776774304\n",
      "Iteration: 344, Loss : 0.0027039657490293086\n",
      "Iteration: 345, Loss : 0.002691887626948995\n",
      "Iteration: 346, Loss : 0.0026797165148434955\n",
      "Iteration: 347, Loss : 0.0026678555591978905\n",
      "Iteration: 348, Loss : 0.002655886824844319\n",
      "Iteration: 349, Loss : 0.0026440345201671116\n",
      "Iteration: 350, Loss : 0.002632256266271837\n",
      "Iteration: 351, Loss : 0.002620760597672715\n",
      "Iteration: 352, Loss : 0.0026093532661776043\n",
      "Iteration: 353, Loss : 0.0025978619361751104\n",
      "Iteration: 354, Loss : 0.002586475642555871\n",
      "Iteration: 355, Loss : 0.0025751471284630326\n",
      "Iteration: 356, Loss : 0.0025640937732706644\n",
      "Iteration: 357, Loss : 0.002552958880234575\n",
      "Iteration: 358, Loss : 0.0025420831902363376\n",
      "Iteration: 359, Loss : 0.002531295408408959\n",
      "Iteration: 360, Loss : 0.0025205797892842285\n",
      "Iteration: 361, Loss : 0.002509970953558655\n",
      "Iteration: 362, Loss : 0.0024994150982168488\n",
      "Iteration: 363, Loss : 0.00248894197271713\n",
      "Iteration: 364, Loss : 0.0024785682528313466\n",
      "Iteration: 365, Loss : 0.0024682744775138656\n",
      "Iteration: 366, Loss : 0.0024578462927675286\n",
      "Iteration: 367, Loss : 0.0024475164782999694\n",
      "Iteration: 368, Loss : 0.0024372854731257547\n",
      "Iteration: 369, Loss : 0.0024272762832716847\n",
      "Iteration: 370, Loss : 0.002417354803442586\n",
      "Iteration: 371, Loss : 0.0024074901457064235\n",
      "Iteration: 372, Loss : 0.0023976834966055703\n",
      "Iteration: 373, Loss : 0.002387961698601459\n",
      "Iteration: 374, Loss : 0.0023782956930251745\n",
      "Iteration: 375, Loss : 0.0023687174191640878\n",
      "Iteration: 376, Loss : 0.0023590605963303445\n",
      "Iteration: 377, Loss : 0.0023496091584949393\n",
      "Iteration: 378, Loss : 0.002340231385741677\n",
      "Iteration: 379, Loss : 0.0023309107600790425\n",
      "Iteration: 380, Loss : 0.0023215296152514723\n",
      "Iteration: 381, Loss : 0.0023123464592452094\n",
      "Iteration: 382, Loss : 0.0023031020494881094\n",
      "Iteration: 383, Loss : 0.0022940508066148406\n",
      "Iteration: 384, Loss : 0.002285057762687048\n",
      "Iteration: 385, Loss : 0.002276146609756813\n",
      "Iteration: 386, Loss : 0.0022673016932694993\n",
      "Iteration: 387, Loss : 0.0022585211656369827\n",
      "Iteration: 388, Loss : 0.002249791739353983\n",
      "Iteration: 389, Loss : 0.002241099883814499\n",
      "Iteration: 390, Loss : 0.0022325022969992515\n",
      "Iteration: 391, Loss : 0.0022239649790144287\n",
      "Iteration: 392, Loss : 0.0022154485186304477\n",
      "Iteration: 393, Loss : 0.0022070078842732653\n",
      "Iteration: 394, Loss : 0.0021986433016118498\n",
      "Iteration: 395, Loss : 0.0021901842044029416\n",
      "Iteration: 396, Loss : 0.0021819317390464115\n",
      "Iteration: 397, Loss : 0.0021736256107526214\n",
      "Iteration: 398, Loss : 0.0021653805828346314\n",
      "Iteration: 399, Loss : 0.0021571946125241307\n",
      "Iteration: 400, Loss : 0.0021491577150290217\n",
      "Iteration: 401, Loss : 0.0021410841426471477\n",
      "Iteration: 402, Loss : 0.0021331260932088503\n",
      "Iteration: 403, Loss : 0.002125226767185315\n",
      "Iteration: 404, Loss : 0.002117313560751498\n",
      "Iteration: 405, Loss : 0.002109454497499681\n",
      "Iteration: 406, Loss : 0.002101648406305378\n",
      "Iteration: 407, Loss : 0.0020939767677362322\n",
      "Iteration: 408, Loss : 0.0020863104191500706\n",
      "Iteration: 409, Loss : 0.0020787404593958746\n",
      "Iteration: 410, Loss : 0.0020711382857293162\n",
      "Iteration: 411, Loss : 0.0020635063445930877\n",
      "Iteration: 412, Loss : 0.0020560040353625204\n",
      "Iteration: 413, Loss : 0.002048639000256647\n",
      "Iteration: 414, Loss : 0.002041234416059762\n",
      "Iteration: 415, Loss : 0.002033913902471601\n",
      "Iteration: 416, Loss : 0.0020265399649188392\n",
      "Iteration: 417, Loss : 0.0020192776627470433\n",
      "Iteration: 418, Loss : 0.002012063222864253\n",
      "Iteration: 419, Loss : 0.0020048331967290507\n",
      "Iteration: 420, Loss : 0.0019977959336851684\n",
      "Iteration: 421, Loss : 0.0019907679144241905\n",
      "Iteration: 422, Loss : 0.0019836926008741386\n",
      "Iteration: 423, Loss : 0.0019767734452421\n",
      "Iteration: 424, Loss : 0.0019698894785388733\n",
      "Iteration: 425, Loss : 0.0019630693135591834\n",
      "Iteration: 426, Loss : 0.001956280679378507\n",
      "Iteration: 427, Loss : 0.0019494410180674882\n",
      "Iteration: 428, Loss : 0.0019426506474598646\n",
      "Iteration: 429, Loss : 0.0019359084189224957\n",
      "Iteration: 430, Loss : 0.0019292829726775154\n",
      "Iteration: 431, Loss : 0.0019226210189560201\n",
      "Iteration: 432, Loss : 0.0019160789569798325\n",
      "Iteration: 433, Loss : 0.001909584680461397\n",
      "Iteration: 434, Loss : 0.0019031303546629794\n",
      "Iteration: 435, Loss : 0.0018967162810509773\n",
      "Iteration: 436, Loss : 0.00189034308039866\n",
      "Iteration: 437, Loss : 0.0018839317202172146\n",
      "Iteration: 438, Loss : 0.001877631476338294\n",
      "Iteration: 439, Loss : 0.0018713573661014364\n",
      "Iteration: 440, Loss : 0.0018650653299050629\n",
      "Iteration: 441, Loss : 0.0018588865822288313\n",
      "Iteration: 442, Loss : 0.001852738043389594\n",
      "Iteration: 443, Loss : 0.0018466093788552262\n",
      "Iteration: 444, Loss : 0.0018404700064913989\n",
      "Iteration: 445, Loss : 0.0018344139781600003\n",
      "Iteration: 446, Loss : 0.001828321051090185\n",
      "Iteration: 447, Loss : 0.0018223595131664693\n",
      "Iteration: 448, Loss : 0.0018163428083808374\n",
      "Iteration: 449, Loss : 0.0018104508203576\n",
      "Iteration: 450, Loss : 0.0018045856288201535\n",
      "Iteration: 451, Loss : 0.0017986941583545382\n",
      "Iteration: 452, Loss : 0.0017928193015341316\n",
      "Iteration: 453, Loss : 0.001786996844723874\n",
      "Iteration: 454, Loss : 0.0017812677480120416\n",
      "Iteration: 455, Loss : 0.0017755726648333412\n",
      "Iteration: 456, Loss : 0.0017699111415311044\n",
      "Iteration: 457, Loss : 0.0017642739903202505\n",
      "Iteration: 458, Loss : 0.0017586773404318786\n",
      "Iteration: 459, Loss : 0.0017530434082655227\n",
      "Iteration: 460, Loss : 0.001747445671765198\n",
      "Iteration: 461, Loss : 0.0017419412205553358\n",
      "Iteration: 462, Loss : 0.0017364084485651365\n",
      "Iteration: 463, Loss : 0.001730908046601104\n",
      "Iteration: 464, Loss : 0.00172549936362673\n",
      "Iteration: 465, Loss : 0.0017201180656177845\n",
      "Iteration: 466, Loss : 0.001714770750710584\n",
      "Iteration: 467, Loss : 0.0017094544924288156\n",
      "Iteration: 468, Loss : 0.0017041029986966756\n",
      "Iteration: 469, Loss : 0.001698845838078894\n",
      "Iteration: 470, Loss : 0.0016935560243399091\n",
      "Iteration: 471, Loss : 0.0016883627076528383\n",
      "Iteration: 472, Loss : 0.001683193511152772\n",
      "Iteration: 473, Loss : 0.0016779914801708668\n",
      "Iteration: 474, Loss : 0.0016728216490058232\n",
      "Iteration: 475, Loss : 0.0016677411518132047\n",
      "Iteration: 476, Loss : 0.0016626832234900918\n",
      "Iteration: 477, Loss : 0.0016576031159831603\n",
      "Iteration: 478, Loss : 0.0016525534115218203\n",
      "Iteration: 479, Loss : 0.0016475816874686063\n",
      "Iteration: 480, Loss : 0.0016425884431620625\n",
      "Iteration: 481, Loss : 0.001637676954339753\n",
      "Iteration: 482, Loss : 0.0016327402229345644\n",
      "Iteration: 483, Loss : 0.0016278775363812576\n",
      "Iteration: 484, Loss : 0.0016230299085970424\n",
      "Iteration: 485, Loss : 0.0016182194722154323\n",
      "Iteration: 486, Loss : 0.0016133900215680413\n",
      "Iteration: 487, Loss : 0.0016085872909581697\n",
      "Iteration: 488, Loss : 0.0016038108779397636\n",
      "Iteration: 489, Loss : 0.0015991029770773219\n",
      "Iteration: 490, Loss : 0.0015944048913698445\n",
      "Iteration: 491, Loss : 0.001589733834149907\n",
      "Iteration: 492, Loss : 0.0015851009759371727\n",
      "Iteration: 493, Loss : 0.001580450678395683\n",
      "Iteration: 494, Loss : 0.0015758665352830384\n",
      "Iteration: 495, Loss : 0.0015713063718722573\n",
      "Iteration: 496, Loss : 0.0015667297946293287\n",
      "Iteration: 497, Loss : 0.0015622077242944394\n",
      "Iteration: 498, Loss : 0.001557705242648776\n",
      "Iteration: 499, Loss : 0.0015532336606508298\n",
      "Iteration: 500, Loss : 0.001548786561510895\n",
      "Iteration: 501, Loss : 0.001544367989592016\n",
      "Iteration: 502, Loss : 0.0015399619631910146\n",
      "Iteration: 503, Loss : 0.001535593092805524\n",
      "Iteration: 504, Loss : 0.001531203640422905\n",
      "Iteration: 505, Loss : 0.001526864969631505\n",
      "Iteration: 506, Loss : 0.0015225593146004084\n",
      "Iteration: 507, Loss : 0.0015182657120777333\n",
      "Iteration: 508, Loss : 0.0015139659864124044\n",
      "Iteration: 509, Loss : 0.0015097261635361775\n",
      "Iteration: 510, Loss : 0.0015054975209093523\n",
      "Iteration: 511, Loss : 0.0015012964754212792\n",
      "Iteration: 512, Loss : 0.0014971127741125836\n",
      "Iteration: 513, Loss : 0.0014929511540737383\n",
      "Iteration: 514, Loss : 0.001488817400666675\n",
      "Iteration: 515, Loss : 0.0014847083267002534\n",
      "Iteration: 516, Loss : 0.0014805663352422567\n",
      "Iteration: 517, Loss : 0.0014764582272979773\n",
      "Iteration: 518, Loss : 0.0014724109441279836\n",
      "Iteration: 519, Loss : 0.0014683802229972396\n",
      "Iteration: 520, Loss : 0.0014643618378927476\n",
      "Iteration: 521, Loss : 0.00146037106815463\n",
      "Iteration: 522, Loss : 0.0014563971482067354\n",
      "Iteration: 523, Loss : 0.0014524392234665482\n",
      "Iteration: 524, Loss : 0.0014485056889691574\n",
      "Iteration: 525, Loss : 0.001444596403973414\n",
      "Iteration: 526, Loss : 0.0014406585525494231\n",
      "Iteration: 527, Loss : 0.0014367799331318658\n",
      "Iteration: 528, Loss : 0.0014328837181686988\n",
      "Iteration: 529, Loss : 0.0014290119119860058\n",
      "Iteration: 530, Loss : 0.0014251906724024091\n",
      "Iteration: 531, Loss : 0.0014213525358733325\n",
      "Iteration: 532, Loss : 0.0014175741671969523\n",
      "Iteration: 533, Loss : 0.0014138090127113747\n",
      "Iteration: 534, Loss : 0.0014100277910591235\n",
      "Iteration: 535, Loss : 0.0014062997269038324\n",
      "Iteration: 536, Loss : 0.001402592999047489\n",
      "Iteration: 537, Loss : 0.001398900843289831\n",
      "Iteration: 538, Loss : 0.0013951941329914004\n",
      "Iteration: 539, Loss : 0.001391506095261002\n",
      "Iteration: 540, Loss : 0.0013878701827154854\n",
      "Iteration: 541, Loss : 0.0013842515567228711\n",
      "Iteration: 542, Loss : 0.0013806479212352246\n",
      "Iteration: 543, Loss : 0.0013770589347064934\n",
      "Iteration: 544, Loss : 0.0013734909507752686\n",
      "Iteration: 545, Loss : 0.0013699402676333639\n",
      "Iteration: 546, Loss : 0.001366372479358162\n",
      "Iteration: 547, Loss : 0.0013628513244692132\n",
      "Iteration: 548, Loss : 0.0013593506655611466\n",
      "Iteration: 549, Loss : 0.0013558660847117346\n",
      "Iteration: 550, Loss : 0.0013523979808200387\n",
      "Iteration: 551, Loss : 0.001348913237412402\n",
      "Iteration: 552, Loss : 0.0013454772852859011\n",
      "Iteration: 553, Loss : 0.001342024399987252\n",
      "Iteration: 554, Loss : 0.0013385885378086717\n",
      "Iteration: 555, Loss : 0.0013352007610236854\n",
      "Iteration: 556, Loss : 0.0013317975885452594\n",
      "Iteration: 557, Loss : 0.0013284072936602244\n",
      "Iteration: 558, Loss : 0.0013250635422610794\n",
      "Iteration: 559, Loss : 0.0013217371746932416\n",
      "Iteration: 560, Loss : 0.0013184239085052118\n",
      "Iteration: 561, Loss : 0.0013151282259748617\n",
      "Iteration: 562, Loss : 0.0013118480303331809\n",
      "Iteration: 563, Loss : 0.00130858313652963\n",
      "Iteration: 564, Loss : 0.0013053292879298124\n",
      "Iteration: 565, Loss : 0.001302062662406326\n",
      "Iteration: 566, Loss : 0.0012988119489979089\n",
      "Iteration: 567, Loss : 0.001295603449630968\n",
      "Iteration: 568, Loss : 0.0012924081598435888\n",
      "Iteration: 569, Loss : 0.0012892301916986324\n",
      "Iteration: 570, Loss : 0.0012860638033263079\n",
      "Iteration: 571, Loss : 0.001282885260745689\n",
      "Iteration: 572, Loss : 0.0012797476258899843\n",
      "Iteration: 573, Loss : 0.0012766263979140284\n",
      "Iteration: 574, Loss : 0.0012735165794457394\n",
      "Iteration: 575, Loss : 0.00127042065571978\n",
      "Iteration: 576, Loss : 0.0012673388513844246\n",
      "Iteration: 577, Loss : 0.0012642710079870617\n",
      "Iteration: 578, Loss : 0.001261218091124035\n",
      "Iteration: 579, Loss : 0.001258178722587611\n",
      "Iteration: 580, Loss : 0.0012551512837397783\n",
      "Iteration: 581, Loss : 0.0012521358726762355\n",
      "Iteration: 582, Loss : 0.0012491338373088723\n",
      "Iteration: 583, Loss : 0.0012461204017959185\n",
      "Iteration: 584, Loss : 0.0012431450456072799\n",
      "Iteration: 585, Loss : 0.0012401827562401337\n",
      "Iteration: 586, Loss : 0.0012372331810212027\n",
      "Iteration: 587, Loss : 0.0012342967823299768\n",
      "Iteration: 588, Loss : 0.0012313488487707622\n",
      "Iteration: 589, Loss : 0.0012284146033368377\n",
      "Iteration: 590, Loss : 0.001225519024556245\n",
      "Iteration: 591, Loss : 0.0012226350201018658\n",
      "Iteration: 592, Loss : 0.00121976348598171\n",
      "Iteration: 593, Loss : 0.001216902805113045\n",
      "Iteration: 594, Loss : 0.0012140557576456779\n",
      "Iteration: 595, Loss : 0.0012112186159042855\n",
      "Iteration: 596, Loss : 0.0012083936972898267\n",
      "Iteration: 597, Loss : 0.0012055595108934592\n",
      "Iteration: 598, Loss : 0.0012027608247560488\n",
      "Iteration: 599, Loss : 0.001199949444888553\n",
      "Iteration: 600, Loss : 0.0011971747726687194\n",
      "Iteration: 601, Loss : 0.0011943884347462878\n",
      "Iteration: 602, Loss : 0.0011916357248639908\n",
      "Iteration: 603, Loss : 0.0011888969507662368\n",
      "Iteration: 604, Loss : 0.0011861661905411777\n",
      "Iteration: 605, Loss : 0.0011834276573094848\n",
      "Iteration: 606, Loss : 0.0011807196933691921\n",
      "Iteration: 607, Loss : 0.0011780267900977084\n",
      "Iteration: 608, Loss : 0.0011753230852887975\n",
      "Iteration: 609, Loss : 0.0011726486385012916\n",
      "Iteration: 610, Loss : 0.0011699640032755843\n",
      "Iteration: 611, Loss : 0.0011673123794560553\n",
      "Iteration: 612, Loss : 0.0011646739838427596\n",
      "Iteration: 613, Loss : 0.0011620239015269328\n",
      "Iteration: 614, Loss : 0.0011594055500470132\n",
      "Iteration: 615, Loss : 0.001156800049109793\n",
      "Iteration: 616, Loss : 0.0011542052872510833\n",
      "Iteration: 617, Loss : 0.0011515997796564005\n",
      "Iteration: 618, Loss : 0.0011490262369670372\n",
      "Iteration: 619, Loss : 0.0011464636813503667\n",
      "Iteration: 620, Loss : 0.0011439090727749976\n",
      "Iteration: 621, Loss : 0.0011413649610276084\n",
      "Iteration: 622, Loss : 0.0011388335805290481\n",
      "Iteration: 623, Loss : 0.001136290824607292\n",
      "Iteration: 624, Loss : 0.0011337780327100597\n",
      "Iteration: 625, Loss : 0.00113127749496472\n",
      "Iteration: 626, Loss : 0.0011287864301866525\n",
      "Iteration: 627, Loss : 0.0011262860710523973\n",
      "Iteration: 628, Loss : 0.0011238141875160998\n",
      "Iteration: 629, Loss : 0.0011213319831816328\n",
      "Iteration: 630, Loss : 0.0011188789662302107\n",
      "Iteration: 631, Loss : 0.0011164384137441325\n",
      "Iteration: 632, Loss : 0.0011140049459174594\n",
      "Iteration: 633, Loss : 0.0011115812823593797\n",
      "Iteration: 634, Loss : 0.001109168002025107\n",
      "Iteration: 635, Loss : 0.0011067454777917553\n",
      "Iteration: 636, Loss : 0.0011043330965018403\n",
      "Iteration: 637, Loss : 0.0011019500106589641\n",
      "Iteration: 638, Loss : 0.001099574783693434\n",
      "Iteration: 639, Loss : 0.001097210816303412\n",
      "Iteration: 640, Loss : 0.0010948545797102938\n",
      "Iteration: 641, Loss : 0.001092506601669688\n",
      "Iteration: 642, Loss : 0.0010901688941204876\n",
      "Iteration: 643, Loss : 0.0010878421174174418\n",
      "Iteration: 644, Loss : 0.001085504047618447\n",
      "Iteration: 645, Loss : 0.001083192799804721\n",
      "Iteration: 646, Loss : 0.0010808737252006818\n",
      "Iteration: 647, Loss : 0.001078564966089666\n",
      "Iteration: 648, Loss : 0.0010762825224174012\n",
      "Iteration: 649, Loss : 0.0010740095999743115\n",
      "Iteration: 650, Loss : 0.0010717430868395926\n",
      "Iteration: 651, Loss : 0.001069487750677528\n",
      "Iteration: 652, Loss : 0.0010672386894727123\n",
      "Iteration: 653, Loss : 0.0010649991660929999\n",
      "Iteration: 654, Loss : 0.0010627690197818054\n",
      "Iteration: 655, Loss : 0.00106054585422863\n",
      "Iteration: 656, Loss : 0.001058332899164654\n",
      "Iteration: 657, Loss : 0.0010561121967506586\n",
      "Iteration: 658, Loss : 0.0010539144874754931\n",
      "Iteration: 659, Loss : 0.0010517275481092046\n",
      "Iteration: 660, Loss : 0.0010495481369429981\n",
      "Iteration: 661, Loss : 0.0010473770593772192\n",
      "Iteration: 662, Loss : 0.0010452126973200609\n",
      "Iteration: 663, Loss : 0.0010430424474708346\n",
      "Iteration: 664, Loss : 0.0010408944177427027\n",
      "Iteration: 665, Loss : 0.0010387393478650895\n",
      "Iteration: 666, Loss : 0.001036593669047413\n",
      "Iteration: 667, Loss : 0.0010344700434140133\n",
      "Iteration: 668, Loss : 0.0010323395612304902\n",
      "Iteration: 669, Loss : 0.0010302180661805496\n",
      "Iteration: 670, Loss : 0.0010281204836449015\n",
      "Iteration: 671, Loss : 0.0010260138448335196\n",
      "Iteration: 672, Loss : 0.0010239299775211441\n",
      "Iteration: 673, Loss : 0.0010218402819814588\n",
      "Iteration: 674, Loss : 0.0010197574285036903\n",
      "Iteration: 675, Loss : 0.001017698672291265\n",
      "Iteration: 676, Loss : 0.0010156456995591009\n",
      "Iteration: 677, Loss : 0.0010135871232730066\n",
      "Iteration: 678, Loss : 0.0010115513662424238\n",
      "Iteration: 679, Loss : 0.001009520962942996\n",
      "Iteration: 680, Loss : 0.0010074995250687166\n",
      "Iteration: 681, Loss : 0.0010054714656844998\n",
      "Iteration: 682, Loss : 0.0010034648288242832\n",
      "Iteration: 683, Loss : 0.0010014503324935978\n",
      "Iteration: 684, Loss : 0.00099944457286803\n",
      "Iteration: 685, Loss : 0.0009974600364298994\n",
      "Iteration: 686, Loss : 0.0009954678312962379\n",
      "Iteration: 687, Loss : 0.0009934970036644035\n",
      "Iteration: 688, Loss : 0.000991534727786438\n",
      "Iteration: 689, Loss : 0.0009895642332130854\n",
      "Iteration: 690, Loss : 0.0009876143971806351\n",
      "Iteration: 691, Loss : 0.0009856720406221117\n",
      "Iteration: 692, Loss : 0.0009837375683987846\n",
      "Iteration: 693, Loss : 0.0009818092582564557\n",
      "Iteration: 694, Loss : 0.0009798755371546392\n",
      "Iteration: 695, Loss : 0.0009779617989857804\n",
      "Iteration: 696, Loss : 0.000976053660657333\n",
      "Iteration: 697, Loss : 0.0009741530563098513\n",
      "Iteration: 698, Loss : 0.0009722585464202961\n",
      "Iteration: 699, Loss : 0.0009703708664944695\n",
      "Iteration: 700, Loss : 0.0009684912324001726\n",
      "Iteration: 701, Loss : 0.0009666170706114701\n",
      "Iteration: 702, Loss : 0.000964750014961206\n",
      "Iteration: 703, Loss : 0.0009628903631023369\n",
      "Iteration: 704, Loss : 0.0009610361017873634\n",
      "Iteration: 705, Loss : 0.0009591888245814842\n",
      "Iteration: 706, Loss : 0.000957335478273427\n",
      "Iteration: 707, Loss : 0.0009555009228129345\n",
      "Iteration: 708, Loss : 0.0009536730877390058\n",
      "Iteration: 709, Loss : 0.0009518517686704698\n",
      "Iteration: 710, Loss : 0.0009500245163436892\n",
      "Iteration: 711, Loss : 0.0009482168250672307\n",
      "Iteration: 712, Loss : 0.0009464157020428719\n",
      "Iteration: 713, Loss : 0.00094460779912957\n",
      "Iteration: 714, Loss : 0.0009428190623889407\n",
      "Iteration: 715, Loss : 0.0009410240541905053\n",
      "Iteration: 716, Loss : 0.0009392478331863351\n",
      "Iteration: 717, Loss : 0.0009374766918096432\n",
      "Iteration: 718, Loss : 0.0009357117073294007\n",
      "Iteration: 719, Loss : 0.0009339526500577401\n",
      "Iteration: 720, Loss : 0.0009322008913510186\n",
      "Iteration: 721, Loss : 0.0009304552694527387\n",
      "Iteration: 722, Loss : 0.0009287146588973108\n",
      "Iteration: 723, Loss : 0.0009269693588890868\n",
      "Iteration: 724, Loss : 0.0009252419722234266\n",
      "Iteration: 725, Loss : 0.0009235089392947355\n",
      "Iteration: 726, Loss : 0.00092179246852339\n",
      "Iteration: 727, Loss : 0.0009200698767397053\n",
      "Iteration: 728, Loss : 0.0009183655500727545\n",
      "Iteration: 729, Loss : 0.0009166676132699488\n",
      "Iteration: 730, Loss : 0.000914974859466787\n",
      "Iteration: 731, Loss : 0.0009132878910447856\n",
      "Iteration: 732, Loss : 0.000911595968100059\n",
      "Iteration: 733, Loss : 0.0009099085759534877\n",
      "Iteration: 734, Loss : 0.0009082382877427725\n",
      "Iteration: 735, Loss : 0.0009065746906810725\n",
      "Iteration: 736, Loss : 0.0009049157376543246\n",
      "Iteration: 737, Loss : 0.000903261593717671\n",
      "Iteration: 738, Loss : 0.0009016025821874114\n",
      "Iteration: 739, Loss : 0.0008999494015324494\n",
      "Iteration: 740, Loss : 0.0008983019983110601\n",
      "Iteration: 741, Loss : 0.0008966705955231263\n",
      "Iteration: 742, Loss : 0.0008950343207359834\n",
      "Iteration: 743, Loss : 0.0008934146175353151\n",
      "Iteration: 744, Loss : 0.0008917897478099514\n",
      "Iteration: 745, Loss : 0.000890181080780149\n",
      "Iteration: 746, Loss : 0.0008885665636508878\n",
      "Iteration: 747, Loss : 0.0008869580510625872\n",
      "Iteration: 748, Loss : 0.0008853654582833856\n",
      "Iteration: 749, Loss : 0.0008837678437673808\n",
      "Iteration: 750, Loss : 0.0008821853470918635\n",
      "Iteration: 751, Loss : 0.0008805973509833412\n",
      "Iteration: 752, Loss : 0.0008790248267504686\n",
      "Iteration: 753, Loss : 0.000877458438788207\n",
      "Iteration: 754, Loss : 0.0008758871788229505\n",
      "Iteration: 755, Loss : 0.0008743311591104285\n",
      "Iteration: 756, Loss : 0.0008727785983205037\n",
      "Iteration: 757, Loss : 0.0008712321370435987\n",
      "Iteration: 758, Loss : 0.0008696814786268573\n",
      "Iteration: 759, Loss : 0.0008681457395959046\n",
      "Iteration: 760, Loss : 0.0008666150132976766\n",
      "Iteration: 761, Loss : 0.0008650780439199322\n",
      "Iteration: 762, Loss : 0.0008635463141888851\n",
      "Iteration: 763, Loss : 0.0008620304355416591\n",
      "Iteration: 764, Loss : 0.0008605186158363197\n",
      "Iteration: 765, Loss : 0.0008590117068882795\n",
      "Iteration: 766, Loss : 0.0008575089283339609\n",
      "Iteration: 767, Loss : 0.0008560110461352939\n",
      "Iteration: 768, Loss : 0.0008545186446010159\n",
      "Iteration: 769, Loss : 0.0008530313009967242\n",
      "Iteration: 770, Loss : 0.0008515484745363164\n",
      "Iteration: 771, Loss : 0.0008500712564793346\n",
      "Iteration: 772, Loss : 0.0008485979468467718\n",
      "Iteration: 773, Loss : 0.0008471301835792744\n",
      "Iteration: 774, Loss : 0.0008456580967157535\n",
      "Iteration: 775, Loss : 0.0008441979665768174\n",
      "Iteration: 776, Loss : 0.0008427425492290037\n",
      "Iteration: 777, Loss : 0.000841292711256153\n",
      "Iteration: 778, Loss : 0.0008398378988713358\n",
      "Iteration: 779, Loss : 0.0008383965180457536\n",
      "Iteration: 780, Loss : 0.0008369602741318654\n",
      "Iteration: 781, Loss : 0.0008355205911537592\n",
      "Iteration: 782, Loss : 0.0008340936513176014\n",
      "Iteration: 783, Loss : 0.0008326718076034492\n",
      "Iteration: 784, Loss : 0.0008312536640841313\n",
      "Iteration: 785, Loss : 0.0008298392461384648\n",
      "Iteration: 786, Loss : 0.0008284307682880621\n",
      "Iteration: 787, Loss : 0.0008270259402638771\n",
      "Iteration: 788, Loss : 0.0008256262825384727\n",
      "Iteration: 789, Loss : 0.0008242302748760438\n",
      "Iteration: 790, Loss : 0.0008228386609865914\n",
      "Iteration: 791, Loss : 0.0008214505816068992\n",
      "Iteration: 792, Loss : 0.0008200668685103403\n",
      "Iteration: 793, Loss : 0.0008186805223922163\n",
      "Iteration: 794, Loss : 0.0008172972941798151\n",
      "Iteration: 795, Loss : 0.0008159185427663713\n",
      "Iteration: 796, Loss : 0.0008145442313171842\n",
      "Iteration: 797, Loss : 0.000813182118158465\n",
      "Iteration: 798, Loss : 0.0008118252555648269\n",
      "Iteration: 799, Loss : 0.0008104718628916433\n",
      "Iteration: 800, Loss : 0.0008091233924906905\n",
      "Iteration: 801, Loss : 0.0008077779032413209\n",
      "Iteration: 802, Loss : 0.0008064289057524428\n",
      "Iteration: 803, Loss : 0.0008050925174349009\n",
      "Iteration: 804, Loss : 0.0008037519616889014\n",
      "Iteration: 805, Loss : 0.0008024159655344355\n",
      "Iteration: 806, Loss : 0.0008010836539515095\n",
      "Iteration: 807, Loss : 0.0007997558629330465\n",
      "Iteration: 808, Loss : 0.0007984393750607929\n",
      "Iteration: 809, Loss : 0.000797126594943832\n",
      "Iteration: 810, Loss : 0.0007958181028349615\n",
      "Iteration: 811, Loss : 0.0007945065745245297\n",
      "Iteration: 812, Loss : 0.000793205662052489\n",
      "Iteration: 813, Loss : 0.0007919094338636649\n",
      "Iteration: 814, Loss : 0.0007906164133681111\n",
      "Iteration: 815, Loss : 0.0007893276141166079\n",
      "Iteration: 816, Loss : 0.0007880358521539477\n",
      "Iteration: 817, Loss : 0.0007867555456561755\n",
      "Iteration: 818, Loss : 0.0007854779517093033\n",
      "Iteration: 819, Loss : 0.0007842042189820711\n",
      "Iteration: 820, Loss : 0.0007829343279545727\n",
      "Iteration: 821, Loss : 0.000781668259452552\n",
      "Iteration: 822, Loss : 0.0007804062993211026\n",
      "Iteration: 823, Loss : 0.000779148460670591\n",
      "Iteration: 824, Loss : 0.0007778940196936415\n",
      "Iteration: 825, Loss : 0.0007766429612992556\n",
      "Iteration: 826, Loss : 0.000775395643038718\n",
      "Iteration: 827, Loss : 0.0007741529809731904\n",
      "Iteration: 828, Loss : 0.0007729068654224548\n",
      "Iteration: 829, Loss : 0.0007716645169624991\n",
      "Iteration: 830, Loss : 0.0007704318468312585\n",
      "Iteration: 831, Loss : 0.0007692028441243606\n",
      "Iteration: 832, Loss : 0.0007679774905114031\n",
      "Iteration: 833, Loss : 0.000766749756057786\n",
      "Iteration: 834, Loss : 0.0007655320210764045\n",
      "Iteration: 835, Loss : 0.0007643174127274913\n",
      "Iteration: 836, Loss : 0.0007631070488028477\n",
      "Iteration: 837, Loss : 0.0007619002440068335\n",
      "Iteration: 838, Loss : 0.000760696981729079\n",
      "Iteration: 839, Loss : 0.0007594907260177848\n",
      "Iteration: 840, Loss : 0.0007582948235150952\n",
      "Iteration: 841, Loss : 0.0007570956922336016\n",
      "Iteration: 842, Loss : 0.0007559061865745886\n",
      "Iteration: 843, Loss : 0.0007547131136441715\n",
      "Iteration: 844, Loss : 0.0007535311018208749\n",
      "Iteration: 845, Loss : 0.0007523520989823891\n",
      "Iteration: 846, Loss : 0.0007511769182249583\n",
      "Iteration: 847, Loss : 0.0007500045552658518\n",
      "Iteration: 848, Loss : 0.0007488355835837388\n",
      "Iteration: 849, Loss : 0.0007476641108467394\n",
      "Iteration: 850, Loss : 0.0007464960604274029\n",
      "Iteration: 851, Loss : 0.0007453377761773589\n",
      "Iteration: 852, Loss : 0.0007441822292651411\n",
      "Iteration: 853, Loss : 0.0007430300119279973\n",
      "Iteration: 854, Loss : 0.0007418816572208366\n",
      "Iteration: 855, Loss : 0.0007407360252443715\n",
      "Iteration: 856, Loss : 0.0007395942210571039\n",
      "Iteration: 857, Loss : 0.0007384494966349952\n",
      "Iteration: 858, Loss : 0.0007373129009939375\n",
      "Iteration: 859, Loss : 0.0007361747173106814\n",
      "Iteration: 860, Loss : 0.0007350458833882134\n",
      "Iteration: 861, Loss : 0.0007339141989346542\n",
      "Iteration: 862, Loss : 0.0007327857525895552\n",
      "Iteration: 863, Loss : 0.0007316659647612514\n",
      "Iteration: 864, Loss : 0.0007305493387873058\n",
      "Iteration: 865, Loss : 0.0007294358615447865\n",
      "Iteration: 866, Loss : 0.00072831851763291\n",
      "Iteration: 867, Loss : 0.0007272113383710065\n",
      "Iteration: 868, Loss : 0.000726100429672926\n",
      "Iteration: 869, Loss : 0.0007249928064108634\n",
      "Iteration: 870, Loss : 0.0007238943437039447\n",
      "Iteration: 871, Loss : 0.000722799591398823\n",
      "Iteration: 872, Loss : 0.0007217079033230463\n",
      "Iteration: 873, Loss : 0.000720619220363957\n",
      "Iteration: 874, Loss : 0.00071952723615717\n",
      "Iteration: 875, Loss : 0.0007184384291990659\n",
      "Iteration: 876, Loss : 0.000717358999347129\n",
      "Iteration: 877, Loss : 0.0007162764097781377\n",
      "Iteration: 878, Loss : 0.0007152033660624627\n",
      "Iteration: 879, Loss : 0.0007141333041634466\n",
      "Iteration: 880, Loss : 0.0007130597493144376\n",
      "Iteration: 881, Loss : 0.0007119899246498648\n",
      "Iteration: 882, Loss : 0.0007109284028663787\n",
      "Iteration: 883, Loss : 0.0007098698257503459\n",
      "Iteration: 884, Loss : 0.0007088082236189081\n",
      "Iteration: 885, Loss : 0.0007077557830423363\n",
      "Iteration: 886, Loss : 0.0007067054413341552\n",
      "Iteration: 887, Loss : 0.0007056585263703447\n",
      "Iteration: 888, Loss : 0.000704609174570217\n",
      "Iteration: 889, Loss : 0.0007035677718450586\n",
      "Iteration: 890, Loss : 0.0007025288966671002\n",
      "Iteration: 891, Loss : 0.0007014936692864976\n",
      "Iteration: 892, Loss : 0.0007004552148247277\n",
      "Iteration: 893, Loss : 0.0006994196830547756\n",
      "Iteration: 894, Loss : 0.0006983927007925745\n",
      "Iteration: 895, Loss : 0.0006973632852361748\n",
      "Iteration: 896, Loss : 0.0006963420953558406\n",
      "Iteration: 897, Loss : 0.0006953232092473574\n",
      "Iteration: 898, Loss : 0.0006943022532137464\n",
      "Iteration: 899, Loss : 0.0006932841061689783\n",
      "Iteration: 900, Loss : 0.0006922735293233648\n",
      "Iteration: 901, Loss : 0.0006912656855577401\n",
      "Iteration: 902, Loss : 0.000690255042795891\n",
      "Iteration: 903, Loss : 0.0006892526310888364\n",
      "Iteration: 904, Loss : 0.0006882482243204002\n",
      "Iteration: 905, Loss : 0.0006872512191914881\n",
      "Iteration: 906, Loss : 0.0006862564529840965\n",
      "Iteration: 907, Loss : 0.0006852652391713806\n",
      "Iteration: 908, Loss : 0.000684270813134638\n",
      "Iteration: 909, Loss : 0.0006832840826027846\n",
      "Iteration: 910, Loss : 0.0006823004927437397\n",
      "Iteration: 911, Loss : 0.000681319380445156\n",
      "Iteration: 912, Loss : 0.0006803363288336846\n",
      "Iteration: 913, Loss : 0.0006793605631880942\n",
      "Iteration: 914, Loss : 0.0006783820884592065\n",
      "Iteration: 915, Loss : 0.0006774062982109396\n",
      "Iteration: 916, Loss : 0.0006764379190433454\n",
      "Iteration: 917, Loss : 0.0006754721188462257\n",
      "Iteration: 918, Loss : 0.0006745042040036628\n",
      "Iteration: 919, Loss : 0.0006735440048228365\n",
      "Iteration: 920, Loss : 0.0006725863442303844\n",
      "Iteration: 921, Loss : 0.0006716261805996728\n",
      "Iteration: 922, Loss : 0.0006706687886498993\n",
      "Iteration: 923, Loss : 0.000669718257806814\n",
      "Iteration: 924, Loss : 0.0006687707781309258\n",
      "Iteration: 925, Loss : 0.0006678209811779132\n",
      "Iteration: 926, Loss : 0.0006668778981615453\n",
      "Iteration: 927, Loss : 0.0006659378787649774\n",
      "Iteration: 928, Loss : 0.0006649997194319461\n",
      "Iteration: 929, Loss : 0.000664064605826547\n",
      "Iteration: 930, Loss : 0.0006631319391059924\n",
      "Iteration: 931, Loss : 0.0006621966254374052\n",
      "Iteration: 932, Loss : 0.000661268810035097\n",
      "Iteration: 933, Loss : 0.0006603387638089986\n",
      "Iteration: 934, Loss : 0.0006594107421529485\n",
      "Iteration: 935, Loss : 0.0006584899946173975\n",
      "Iteration: 936, Loss : 0.0006575668622531251\n",
      "Iteration: 937, Loss : 0.0006566506197004141\n",
      "Iteration: 938, Loss : 0.0006557367563008838\n",
      "Iteration: 939, Loss : 0.0006548256628536565\n",
      "Iteration: 940, Loss : 0.0006539163289139659\n",
      "Iteration: 941, Loss : 0.0006530050716706516\n",
      "Iteration: 942, Loss : 0.0006520962171797415\n",
      "Iteration: 943, Loss : 0.0006511899487240935\n",
      "Iteration: 944, Loss : 0.0006502903996804907\n",
      "Iteration: 945, Loss : 0.0006493927545000978\n",
      "Iteration: 946, Loss : 0.0006484975251647017\n",
      "Iteration: 947, Loss : 0.0006476045967907066\n",
      "Iteration: 948, Loss : 0.000646713959508285\n",
      "Iteration: 949, Loss : 0.0006458259886227759\n",
      "Iteration: 950, Loss : 0.0006449355645016448\n",
      "Iteration: 951, Loss : 0.0006440515726782977\n",
      "Iteration: 952, Loss : 0.0006431698367885772\n",
      "Iteration: 953, Loss : 0.0006422903478237301\n",
      "Iteration: 954, Loss : 0.000641409060258828\n",
      "Iteration: 955, Loss : 0.0006405344408843675\n",
      "Iteration: 956, Loss : 0.0006396620989930984\n",
      "Iteration: 957, Loss : 0.0006387875296356477\n",
      "Iteration: 958, Loss : 0.0006379195918222325\n",
      "Iteration: 959, Loss : 0.000637049611014044\n",
      "Iteration: 960, Loss : 0.0006361857111468153\n",
      "Iteration: 961, Loss : 0.0006353242452357852\n",
      "Iteration: 962, Loss : 0.0006344650564832773\n",
      "Iteration: 963, Loss : 0.0006336075274675276\n",
      "Iteration: 964, Loss : 0.0006327482579527547\n",
      "Iteration: 965, Loss : 0.0006318950548253253\n",
      "Iteration: 966, Loss : 0.0006310443611508025\n",
      "Iteration: 967, Loss : 0.0006301916031980875\n",
      "Iteration: 968, Loss : 0.0006293410965227337\n",
      "Iteration: 969, Loss : 0.0006284964853500467\n",
      "Iteration: 970, Loss : 0.0006276542615715943\n",
      "Iteration: 971, Loss : 0.0006268138560865228\n",
      "Iteration: 972, Loss : 0.0006259759037163183\n",
      "Iteration: 973, Loss : 0.0006251396871638463\n",
      "Iteration: 974, Loss : 0.0006243059029671159\n",
      "Iteration: 975, Loss : 0.0006234740492935481\n",
      "Iteration: 976, Loss : 0.0006226442577756781\n",
      "Iteration: 977, Loss : 0.0006218161600796313\n",
      "Iteration: 978, Loss : 0.0006209904714573482\n",
      "Iteration: 979, Loss : 0.0006201665773204398\n",
      "Iteration: 980, Loss : 0.0006193449482216607\n",
      "Iteration: 981, Loss : 0.0006185251087378106\n",
      "Iteration: 982, Loss : 0.0006177076325070408\n",
      "Iteration: 983, Loss : 0.0006168921684264369\n",
      "Iteration: 984, Loss : 0.0006160748712338861\n",
      "Iteration: 985, Loss : 0.0006152630850004225\n",
      "Iteration: 986, Loss : 0.0006144530830942796\n",
      "Iteration: 987, Loss : 0.0006136418082303534\n",
      "Iteration: 988, Loss : 0.0006128363061273499\n",
      "Iteration: 989, Loss : 0.0006120285912257961\n",
      "Iteration: 990, Loss : 0.0006112228973575021\n",
      "Iteration: 991, Loss : 0.0006104192139304923\n",
      "Iteration: 992, Loss : 0.0006096175306421004\n",
      "Iteration: 993, Loss : 0.0006088178374612476\n",
      "Iteration: 994, Loss : 0.000608020278751089\n",
      "Iteration: 995, Loss : 0.0006072283916253771\n",
      "Iteration: 996, Loss : 0.0006064381036831028\n",
      "Iteration: 997, Loss : 0.0006056500544861464\n",
      "Iteration: 998, Loss : 0.0006048599743928556\n",
      "Iteration: 999, Loss : 0.0006040720794050465\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[6.84523978e-04],\n",
       "       [2.58524231e-02],\n",
       "       [2.48995743e-02],\n",
       "       [9.66421984e-01]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T15:32:37.797773Z",
     "start_time": "2024-07-09T15:32:37.754394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 1\n",
    "X = np.array([[-1],[-2],[-3],[1],[2],[3]])\n",
    "y = np.array([[1],[2],[3],[-1],[-2],[-3]])\n",
    "size = [1,2,1]\n",
    "nn = NeuralNetwork(size,lambda x: x,lambda x: 1,init_func=lambda x,y: xavier_initialization((x,y)),use_batch_norm=False)\n",
    "nn.set_loss_function(mean_squared_error, mean_squared_error_derivative)\n",
    "nn.train(X,y,epochs=100,learning_rate=0.01, batch_size=3)\n",
    "nn.forward([[-25],[5],[-4],[4]])"
   ],
   "id": "4553d76d7dbaf899",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Loss : 4.6256037596674915\n",
      "Iteration: 1, Loss : 3.141815460215979\n",
      "Iteration: 2, Loss : 2.092505233622914\n",
      "Iteration: 3, Loss : 1.3166255515480165\n",
      "Iteration: 4, Loss : 0.7967779476806705\n",
      "Iteration: 5, Loss : 0.4655553478348294\n",
      "Iteration: 6, Loss : 0.2894731666914195\n",
      "Iteration: 7, Loss : 0.191451458174326\n",
      "Iteration: 8, Loss : 0.1288762906705466\n",
      "Iteration: 9, Loss : 0.08452455767479518\n",
      "Iteration: 10, Loss : 0.05761672905320724\n",
      "Iteration: 11, Loss : 0.037987432208739944\n",
      "Iteration: 12, Loss : 0.02503346082537029\n",
      "Iteration: 13, Loss : 0.016873306370959542\n",
      "Iteration: 14, Loss : 0.011381623919114528\n",
      "Iteration: 15, Loss : 0.007664189935640949\n",
      "Iteration: 16, Loss : 0.005091499444436252\n",
      "Iteration: 17, Loss : 0.0033698303616038357\n",
      "Iteration: 18, Loss : 0.0022501270643953356\n",
      "Iteration: 19, Loss : 0.0014576617698522046\n",
      "Iteration: 20, Loss : 0.0009734078843813263\n",
      "Iteration: 21, Loss : 0.000650290137643332\n",
      "Iteration: 22, Loss : 0.000435692508433223\n",
      "Iteration: 23, Loss : 0.00028791034728484444\n",
      "Iteration: 24, Loss : 0.0001920321413088998\n",
      "Iteration: 25, Loss : 0.00012803972533678444\n",
      "Iteration: 26, Loss : 8.570702740062207e-05\n",
      "Iteration: 27, Loss : 5.6604408626491695e-05\n",
      "Iteration: 28, Loss : 3.765316990512103e-05\n",
      "Iteration: 29, Loss : 2.5185186486630673e-05\n",
      "Iteration: 30, Loss : 1.6627093788126393e-05\n",
      "Iteration: 31, Loss : 1.1114188852164744e-05\n",
      "Iteration: 32, Loss : 7.402196085756756e-06\n",
      "Iteration: 33, Loss : 4.949040613866954e-06\n",
      "Iteration: 34, Loss : 3.3102670517480473e-06\n",
      "Iteration: 35, Loss : 2.187174655006823e-06\n",
      "Iteration: 36, Loss : 1.4555356273043275e-06\n",
      "Iteration: 37, Loss : 9.589876383671529e-07\n",
      "Iteration: 38, Loss : 6.340159880631157e-07\n",
      "Iteration: 39, Loss : 4.2369266840824933e-07\n",
      "Iteration: 40, Loss : 2.8024491204360304e-07\n",
      "Iteration: 41, Loss : 1.8700993861345736e-07\n",
      "Iteration: 42, Loss : 1.2502058526184048e-07\n",
      "Iteration: 43, Loss : 8.232349607390986e-08\n",
      "Iteration: 44, Loss : 5.308542928407902e-08\n",
      "Iteration: 45, Loss : 3.530737118914232e-08\n",
      "Iteration: 46, Loss : 2.3292864551976347e-08\n",
      "Iteration: 47, Loss : 1.5530304349657174e-08\n",
      "Iteration: 48, Loss : 1.0343728689075468e-08\n",
      "Iteration: 49, Loss : 6.887525990080346e-09\n",
      "Iteration: 50, Loss : 4.600405822929949e-09\n",
      "Iteration: 51, Loss : 3.075619882408608e-09\n",
      "Iteration: 52, Loss : 2.056841468178721e-09\n",
      "Iteration: 53, Loss : 1.3760957708674467e-09\n",
      "Iteration: 54, Loss : 9.204128277015507e-10\n",
      "Iteration: 55, Loss : 6.1255986377379e-10\n",
      "Iteration: 56, Loss : 4.0832408496556007e-10\n",
      "Iteration: 57, Loss : 2.635163981881047e-10\n",
      "Iteration: 58, Loss : 1.7352366627089286e-10\n",
      "Iteration: 59, Loss : 1.1606359183345315e-10\n",
      "Iteration: 60, Loss : 7.764538369148733e-11\n",
      "Iteration: 61, Loss : 5.1752843921576023e-11\n",
      "Iteration: 62, Loss : 3.449777161710927e-11\n",
      "Iteration: 63, Loss : 2.2713369087634792e-11\n",
      "Iteration: 64, Loss : 1.511520071762544e-11\n",
      "Iteration: 65, Loss : 1.0075269771557666e-11\n",
      "Iteration: 66, Loss : 6.715688456464311e-12\n",
      "Iteration: 67, Loss : 4.473997927245835e-12\n",
      "Iteration: 68, Loss : 2.94743133194031e-12\n",
      "Iteration: 69, Loss : 1.947068049010767e-12\n",
      "Iteration: 70, Loss : 1.3023545115007848e-12\n",
      "Iteration: 71, Loss : 8.601781809664559e-13\n",
      "Iteration: 72, Loss : 5.748864256195953e-13\n",
      "Iteration: 73, Loss : 3.7967439673702825e-13\n",
      "Iteration: 74, Loss : 2.5293343238415073e-13\n",
      "Iteration: 75, Loss : 1.6324400723646742e-13\n",
      "Iteration: 76, Loss : 1.0920270735676764e-13\n",
      "Iteration: 77, Loss : 7.278460996713798e-14\n",
      "Iteration: 78, Loss : 4.8692169409660485e-14\n",
      "Iteration: 79, Loss : 3.257943940183851e-14\n",
      "Iteration: 80, Loss : 2.179081618037506e-14\n",
      "Iteration: 81, Loss : 1.4398252999018824e-14\n",
      "Iteration: 82, Loss : 9.593829890524973e-15\n",
      "Iteration: 83, Loss : 6.3946726977433306e-15\n",
      "Iteration: 84, Loss : 4.133661067840673e-15\n",
      "Iteration: 85, Loss : 2.748103424790064e-15\n",
      "Iteration: 86, Loss : 1.7752330175982703e-15\n",
      "Iteration: 87, Loss : 1.1831406874137268e-15\n",
      "Iteration: 88, Loss : 7.911551990613302e-16\n",
      "Iteration: 89, Loss : 5.273397756446665e-16\n",
      "Iteration: 90, Loss : 3.527558120451576e-16\n",
      "Iteration: 91, Loss : 2.3261501403522043e-16\n",
      "Iteration: 92, Loss : 1.5485578818881615e-16\n",
      "Iteration: 93, Loss : 1.0198162501119285e-16\n",
      "Iteration: 94, Loss : 6.797872754331607e-17\n",
      "Iteration: 95, Loss : 4.408124884120094e-17\n",
      "Iteration: 96, Loss : 2.898242513746125e-17\n",
      "Iteration: 97, Loss : 1.935873423957493e-17\n",
      "Iteration: 98, Loss : 1.2778282432570212e-17\n",
      "Iteration: 99, Loss : 8.403881495954702e-18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[24.99999998],\n",
       "       [-5.        ],\n",
       "       [ 4.        ],\n",
       "       [-4.        ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T15:33:44.800675Z",
     "start_time": "2024-07-09T15:33:44.139320Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 1 \n",
    "size = [2,3,1]\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y = np.array([[0],[1],[1],[0]])\n",
    "nn = NeuralNetwork(size,sigmoid,sigmoid_derivative,use_batch_norm=False)\n",
    "nn.set_loss_function(mean_squared_error,mean_squared_error_derivative)\n",
    "nn.train(X,y,epochs=1000,learning_rate=10e-3,L2=None,clipping_mode='norm',adam_optimizer=True,batch_size=batch_size)\n",
    "nn.forward(X)"
   ],
   "id": "8926d131faf82493",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Loss : 0.3273192813941069\n",
      "Iteration: 1, Loss : 0.3231620457619328\n",
      "Iteration: 2, Loss : 0.31847123627134194\n",
      "Iteration: 3, Loss : 0.31358156521103636\n",
      "Iteration: 4, Loss : 0.3092275355094654\n",
      "Iteration: 5, Loss : 0.3036810266499065\n",
      "Iteration: 6, Loss : 0.2991717185124966\n",
      "Iteration: 7, Loss : 0.2942886192030614\n",
      "Iteration: 8, Loss : 0.29004389345731557\n",
      "Iteration: 9, Loss : 0.2875134780849379\n",
      "Iteration: 10, Loss : 0.2841809256441531\n",
      "Iteration: 11, Loss : 0.2805468531976735\n",
      "Iteration: 12, Loss : 0.27776684040649385\n",
      "Iteration: 13, Loss : 0.2759967293962527\n",
      "Iteration: 14, Loss : 0.2730880384465447\n",
      "Iteration: 15, Loss : 0.2716348755939501\n",
      "Iteration: 16, Loss : 0.2700635307896075\n",
      "Iteration: 17, Loss : 0.2679957896384892\n",
      "Iteration: 18, Loss : 0.2663799644503365\n",
      "Iteration: 19, Loss : 0.26428193523530746\n",
      "Iteration: 20, Loss : 0.26306270837359724\n",
      "Iteration: 21, Loss : 0.26170355635380943\n",
      "Iteration: 22, Loss : 0.26088993749106487\n",
      "Iteration: 23, Loss : 0.260213684144212\n",
      "Iteration: 24, Loss : 0.2593237838827837\n",
      "Iteration: 25, Loss : 0.2585002767339008\n",
      "Iteration: 26, Loss : 0.25756039039709605\n",
      "Iteration: 27, Loss : 0.2569078893955381\n",
      "Iteration: 28, Loss : 0.256189592937506\n",
      "Iteration: 29, Loss : 0.25557426255059945\n",
      "Iteration: 30, Loss : 0.25502316143308684\n",
      "Iteration: 31, Loss : 0.25479690386948045\n",
      "Iteration: 32, Loss : 0.2541775693054381\n",
      "Iteration: 33, Loss : 0.25388420157199065\n",
      "Iteration: 34, Loss : 0.25360534751716496\n",
      "Iteration: 35, Loss : 0.25324296983568656\n",
      "Iteration: 36, Loss : 0.25293013714000856\n",
      "Iteration: 37, Loss : 0.2526808136383133\n",
      "Iteration: 38, Loss : 0.2522848433772677\n",
      "Iteration: 39, Loss : 0.25221150673578363\n",
      "Iteration: 40, Loss : 0.25218460941793025\n",
      "Iteration: 41, Loss : 0.25204250950823504\n",
      "Iteration: 42, Loss : 0.2518374539872043\n",
      "Iteration: 43, Loss : 0.2517800520311399\n",
      "Iteration: 44, Loss : 0.2515220320624141\n",
      "Iteration: 45, Loss : 0.2513314243548752\n",
      "Iteration: 46, Loss : 0.2512837659552549\n",
      "Iteration: 47, Loss : 0.25118225745103123\n",
      "Iteration: 48, Loss : 0.25099109914534096\n",
      "Iteration: 49, Loss : 0.2509401687086075\n",
      "Iteration: 50, Loss : 0.25084179091445463\n",
      "Iteration: 51, Loss : 0.2508072936155276\n",
      "Iteration: 52, Loss : 0.25077059799804485\n",
      "Iteration: 53, Loss : 0.2507604577329337\n",
      "Iteration: 54, Loss : 0.2507045502688334\n",
      "Iteration: 55, Loss : 0.25064725029392393\n",
      "Iteration: 56, Loss : 0.25066607483672443\n",
      "Iteration: 57, Loss : 0.25059631268223337\n",
      "Iteration: 58, Loss : 0.25050989707562854\n",
      "Iteration: 59, Loss : 0.2504212169688043\n",
      "Iteration: 60, Loss : 0.2504321939493599\n",
      "Iteration: 61, Loss : 0.25035785806015737\n",
      "Iteration: 62, Loss : 0.25032475774800683\n",
      "Iteration: 63, Loss : 0.250298143294084\n",
      "Iteration: 64, Loss : 0.25033377434846227\n",
      "Iteration: 65, Loss : 0.25030935353177336\n",
      "Iteration: 66, Loss : 0.25024471841293006\n",
      "Iteration: 67, Loss : 0.2502722419346191\n",
      "Iteration: 68, Loss : 0.2502087726501309\n",
      "Iteration: 69, Loss : 0.25021371320066393\n",
      "Iteration: 70, Loss : 0.25023002224331176\n",
      "Iteration: 71, Loss : 0.250215735396444\n",
      "Iteration: 72, Loss : 0.250218901854308\n",
      "Iteration: 73, Loss : 0.2501906689485051\n",
      "Iteration: 74, Loss : 0.2501868338001429\n",
      "Iteration: 75, Loss : 0.25012114117221496\n",
      "Iteration: 76, Loss : 0.25012532176195457\n",
      "Iteration: 77, Loss : 0.25007717699050397\n",
      "Iteration: 78, Loss : 0.25006789380603256\n",
      "Iteration: 79, Loss : 0.25004399547564626\n",
      "Iteration: 80, Loss : 0.2500491131119674\n",
      "Iteration: 81, Loss : 0.25001647463290627\n",
      "Iteration: 82, Loss : 0.25000780209287204\n",
      "Iteration: 83, Loss : 0.24997492878681188\n",
      "Iteration: 84, Loss : 0.24995241215832142\n",
      "Iteration: 85, Loss : 0.24994057889369295\n",
      "Iteration: 86, Loss : 0.2499302666003636\n",
      "Iteration: 87, Loss : 0.249919036521226\n",
      "Iteration: 88, Loss : 0.249903620234539\n",
      "Iteration: 89, Loss : 0.24990587786714702\n",
      "Iteration: 90, Loss : 0.24988916357473825\n",
      "Iteration: 91, Loss : 0.249860687758349\n",
      "Iteration: 92, Loss : 0.24985155874781909\n",
      "Iteration: 93, Loss : 0.2498434643368606\n",
      "Iteration: 94, Loss : 0.2498204551234598\n",
      "Iteration: 95, Loss : 0.24980605264816316\n",
      "Iteration: 96, Loss : 0.24979278380404535\n",
      "Iteration: 97, Loss : 0.24977563595907218\n",
      "Iteration: 98, Loss : 0.24976524387335536\n",
      "Iteration: 99, Loss : 0.24975633432935107\n",
      "Iteration: 100, Loss : 0.24974030526593455\n",
      "Iteration: 101, Loss : 0.2497266203466384\n",
      "Iteration: 102, Loss : 0.24971232464677268\n",
      "Iteration: 103, Loss : 0.24970153937675293\n",
      "Iteration: 104, Loss : 0.2496952694659662\n",
      "Iteration: 105, Loss : 0.24967243257028765\n",
      "Iteration: 106, Loss : 0.2496568332029503\n",
      "Iteration: 107, Loss : 0.24964126254159152\n",
      "Iteration: 108, Loss : 0.24962449969979728\n",
      "Iteration: 109, Loss : 0.24961602768346192\n",
      "Iteration: 110, Loss : 0.24960302440353066\n",
      "Iteration: 111, Loss : 0.24958369115962742\n",
      "Iteration: 112, Loss : 0.24956387132497293\n",
      "Iteration: 113, Loss : 0.2495458770981289\n",
      "Iteration: 114, Loss : 0.24953476512505318\n",
      "Iteration: 115, Loss : 0.24951879970087876\n",
      "Iteration: 116, Loss : 0.24950303041591354\n",
      "Iteration: 117, Loss : 0.2494844970005542\n",
      "Iteration: 118, Loss : 0.2494697574923455\n",
      "Iteration: 119, Loss : 0.24945599324242984\n",
      "Iteration: 120, Loss : 0.24944941584846314\n",
      "Iteration: 121, Loss : 0.2494182077007989\n",
      "Iteration: 122, Loss : 0.24939676606860153\n",
      "Iteration: 123, Loss : 0.2493784684371949\n",
      "Iteration: 124, Loss : 0.2493636629454526\n",
      "Iteration: 125, Loss : 0.24935277300970932\n",
      "Iteration: 126, Loss : 0.2493318384358325\n",
      "Iteration: 127, Loss : 0.24931048738401285\n",
      "Iteration: 128, Loss : 0.24928985437980497\n",
      "Iteration: 129, Loss : 0.24925891953527424\n",
      "Iteration: 130, Loss : 0.249240398616601\n",
      "Iteration: 131, Loss : 0.24921835219172397\n",
      "Iteration: 132, Loss : 0.24918992754630437\n",
      "Iteration: 133, Loss : 0.2491706547919888\n",
      "Iteration: 134, Loss : 0.24914364553556523\n",
      "Iteration: 135, Loss : 0.24912301568476597\n",
      "Iteration: 136, Loss : 0.24910269355315107\n",
      "Iteration: 137, Loss : 0.24908700323411276\n",
      "Iteration: 138, Loss : 0.24904957066170857\n",
      "Iteration: 139, Loss : 0.2490219115653337\n",
      "Iteration: 140, Loss : 0.24900049765505727\n",
      "Iteration: 141, Loss : 0.24897259292337937\n",
      "Iteration: 142, Loss : 0.24894810191647926\n",
      "Iteration: 143, Loss : 0.2489135837295836\n",
      "Iteration: 144, Loss : 0.24889197469567453\n",
      "Iteration: 145, Loss : 0.24887050228357283\n",
      "Iteration: 146, Loss : 0.24883499217271238\n",
      "Iteration: 147, Loss : 0.24880490078865528\n",
      "Iteration: 148, Loss : 0.24877389678423342\n",
      "Iteration: 149, Loss : 0.24873935475846187\n",
      "Iteration: 150, Loss : 0.2486989216787496\n",
      "Iteration: 151, Loss : 0.24866718986989417\n",
      "Iteration: 152, Loss : 0.24863011144137276\n",
      "Iteration: 153, Loss : 0.24860166448061463\n",
      "Iteration: 154, Loss : 0.24855198066342882\n",
      "Iteration: 155, Loss : 0.24852292501408152\n",
      "Iteration: 156, Loss : 0.248487447546521\n",
      "Iteration: 157, Loss : 0.24845591798815025\n",
      "Iteration: 158, Loss : 0.24840062344373204\n",
      "Iteration: 159, Loss : 0.24836419007552307\n",
      "Iteration: 160, Loss : 0.24832492479573093\n",
      "Iteration: 161, Loss : 0.24828538025132124\n",
      "Iteration: 162, Loss : 0.24824421306800815\n",
      "Iteration: 163, Loss : 0.24819702906522584\n",
      "Iteration: 164, Loss : 0.24813903961590886\n",
      "Iteration: 165, Loss : 0.2480845375343149\n",
      "Iteration: 166, Loss : 0.24804143589459127\n",
      "Iteration: 167, Loss : 0.2479984855073702\n",
      "Iteration: 168, Loss : 0.24795137891586255\n",
      "Iteration: 169, Loss : 0.24790095261835315\n",
      "Iteration: 170, Loss : 0.24783085809922664\n",
      "Iteration: 171, Loss : 0.24778327374386672\n",
      "Iteration: 172, Loss : 0.24773505831668663\n",
      "Iteration: 173, Loss : 0.24765761002657555\n",
      "Iteration: 174, Loss : 0.24760642785967346\n",
      "Iteration: 175, Loss : 0.24754464555623973\n",
      "Iteration: 176, Loss : 0.2474723647257152\n",
      "Iteration: 177, Loss : 0.24741503136970622\n",
      "Iteration: 178, Loss : 0.24735705618559367\n",
      "Iteration: 179, Loss : 0.24728613514239545\n",
      "Iteration: 180, Loss : 0.2472171425621391\n",
      "Iteration: 181, Loss : 0.24714631002295445\n",
      "Iteration: 182, Loss : 0.24707384559577553\n",
      "Iteration: 183, Loss : 0.2469827290014635\n",
      "Iteration: 184, Loss : 0.246895721632041\n",
      "Iteration: 185, Loss : 0.2468032483925643\n",
      "Iteration: 186, Loss : 0.2467183427683849\n",
      "Iteration: 187, Loss : 0.24664390391401037\n",
      "Iteration: 188, Loss : 0.24656396987056337\n",
      "Iteration: 189, Loss : 0.24648289556138336\n",
      "Iteration: 190, Loss : 0.24638946467573714\n",
      "Iteration: 191, Loss : 0.24629414789685555\n",
      "Iteration: 192, Loss : 0.24617353374452514\n",
      "Iteration: 193, Loss : 0.24605859824239246\n",
      "Iteration: 194, Loss : 0.2459386263299866\n",
      "Iteration: 195, Loss : 0.2458212103808517\n",
      "Iteration: 196, Loss : 0.24571946867457617\n",
      "Iteration: 197, Loss : 0.24563488483970686\n",
      "Iteration: 198, Loss : 0.24554157933130377\n",
      "Iteration: 199, Loss : 0.24544056847745377\n",
      "Iteration: 200, Loss : 0.24531153406815254\n",
      "Iteration: 201, Loss : 0.2451983566517697\n",
      "Iteration: 202, Loss : 0.24504093356736473\n",
      "Iteration: 203, Loss : 0.2448972572451915\n",
      "Iteration: 204, Loss : 0.24475571358869563\n",
      "Iteration: 205, Loss : 0.24463737611762074\n",
      "Iteration: 206, Loss : 0.24448285754621019\n",
      "Iteration: 207, Loss : 0.24433763916257528\n",
      "Iteration: 208, Loss : 0.2441880692256193\n",
      "Iteration: 209, Loss : 0.24401594685512887\n",
      "Iteration: 210, Loss : 0.2438506159968971\n",
      "Iteration: 211, Loss : 0.24371864634948373\n",
      "Iteration: 212, Loss : 0.24357800862683704\n",
      "Iteration: 213, Loss : 0.24337252706365492\n",
      "Iteration: 214, Loss : 0.2431694028765254\n",
      "Iteration: 215, Loss : 0.24297123915015723\n",
      "Iteration: 216, Loss : 0.24282762239184363\n",
      "Iteration: 217, Loss : 0.24266468981292136\n",
      "Iteration: 218, Loss : 0.24244435844641182\n",
      "Iteration: 219, Loss : 0.24227260709471077\n",
      "Iteration: 220, Loss : 0.2420881888336877\n",
      "Iteration: 221, Loss : 0.24185775687814898\n",
      "Iteration: 222, Loss : 0.24162146002662227\n",
      "Iteration: 223, Loss : 0.241374943202687\n",
      "Iteration: 224, Loss : 0.24114041605382563\n",
      "Iteration: 225, Loss : 0.24095021058843336\n",
      "Iteration: 226, Loss : 0.24075320880939938\n",
      "Iteration: 227, Loss : 0.24048664900930825\n",
      "Iteration: 228, Loss : 0.2402310587326952\n",
      "Iteration: 229, Loss : 0.23994901235731364\n",
      "Iteration: 230, Loss : 0.23970837481532672\n",
      "Iteration: 231, Loss : 0.23949335543981143\n",
      "Iteration: 232, Loss : 0.23920189306031017\n",
      "Iteration: 233, Loss : 0.23890468320277425\n",
      "Iteration: 234, Loss : 0.23868467155495504\n",
      "Iteration: 235, Loss : 0.2384113744443449\n",
      "Iteration: 236, Loss : 0.238104993495327\n",
      "Iteration: 237, Loss : 0.2378058634269259\n",
      "Iteration: 238, Loss : 0.23753834862290912\n",
      "Iteration: 239, Loss : 0.2372236230310869\n",
      "Iteration: 240, Loss : 0.23693374823134755\n",
      "Iteration: 241, Loss : 0.23662485491638846\n",
      "Iteration: 242, Loss : 0.2362369424697751\n",
      "Iteration: 243, Loss : 0.23592411709260186\n",
      "Iteration: 244, Loss : 0.23557313968752824\n",
      "Iteration: 245, Loss : 0.23525322025152606\n",
      "Iteration: 246, Loss : 0.23489103860726712\n",
      "Iteration: 247, Loss : 0.23451583843632695\n",
      "Iteration: 248, Loss : 0.2341746802460012\n",
      "Iteration: 249, Loss : 0.23372945233713888\n",
      "Iteration: 250, Loss : 0.2333349448213284\n",
      "Iteration: 251, Loss : 0.23298720411951507\n",
      "Iteration: 252, Loss : 0.2325427049541181\n",
      "Iteration: 253, Loss : 0.23213437493019956\n",
      "Iteration: 254, Loss : 0.23174868286652783\n",
      "Iteration: 255, Loss : 0.23135328021514193\n",
      "Iteration: 256, Loss : 0.23088734023530338\n",
      "Iteration: 257, Loss : 0.23053966310785373\n",
      "Iteration: 258, Loss : 0.23017770130234633\n",
      "Iteration: 259, Loss : 0.229696829198497\n",
      "Iteration: 260, Loss : 0.2292776230593547\n",
      "Iteration: 261, Loss : 0.22878423655595656\n",
      "Iteration: 262, Loss : 0.22828876103780568\n",
      "Iteration: 263, Loss : 0.22787899107243187\n",
      "Iteration: 264, Loss : 0.22745209148321643\n",
      "Iteration: 265, Loss : 0.22695028507456727\n",
      "Iteration: 266, Loss : 0.22650020679939464\n",
      "Iteration: 267, Loss : 0.22597467822482512\n",
      "Iteration: 268, Loss : 0.2255405965329363\n",
      "Iteration: 269, Loss : 0.22501733817657166\n",
      "Iteration: 270, Loss : 0.22449960320263002\n",
      "Iteration: 271, Loss : 0.2239811627556677\n",
      "Iteration: 272, Loss : 0.22352151174641252\n",
      "Iteration: 273, Loss : 0.22295214045305167\n",
      "Iteration: 274, Loss : 0.22247750162411473\n",
      "Iteration: 275, Loss : 0.22198715753354567\n",
      "Iteration: 276, Loss : 0.22137943880477998\n",
      "Iteration: 277, Loss : 0.22084394332854732\n",
      "Iteration: 278, Loss : 0.22028976280667323\n",
      "Iteration: 279, Loss : 0.21981052626496195\n",
      "Iteration: 280, Loss : 0.21927063528516444\n",
      "Iteration: 281, Loss : 0.2187997672418659\n",
      "Iteration: 282, Loss : 0.21820766051148707\n",
      "Iteration: 283, Loss : 0.21769445549626737\n",
      "Iteration: 284, Loss : 0.21718762761694585\n",
      "Iteration: 285, Loss : 0.2166472073975821\n",
      "Iteration: 286, Loss : 0.2160888635447269\n",
      "Iteration: 287, Loss : 0.21553187036722005\n",
      "Iteration: 288, Loss : 0.21503024319653824\n",
      "Iteration: 289, Loss : 0.2144815140624107\n",
      "Iteration: 290, Loss : 0.2138013909049952\n",
      "Iteration: 291, Loss : 0.21317790150378868\n",
      "Iteration: 292, Loss : 0.212597266792181\n",
      "Iteration: 293, Loss : 0.21210412382847968\n",
      "Iteration: 294, Loss : 0.21158048753256006\n",
      "Iteration: 295, Loss : 0.2109875115729254\n",
      "Iteration: 296, Loss : 0.210410970112426\n",
      "Iteration: 297, Loss : 0.2098125875610261\n",
      "Iteration: 298, Loss : 0.20927869084056333\n",
      "Iteration: 299, Loss : 0.20867055162001674\n",
      "Iteration: 300, Loss : 0.20815477507958122\n",
      "Iteration: 301, Loss : 0.2076837312205304\n",
      "Iteration: 302, Loss : 0.2071498750591099\n",
      "Iteration: 303, Loss : 0.20652205405917926\n",
      "Iteration: 304, Loss : 0.20603342477171294\n",
      "Iteration: 305, Loss : 0.2055482088186577\n",
      "Iteration: 306, Loss : 0.20498962760611616\n",
      "Iteration: 307, Loss : 0.20433340658514187\n",
      "Iteration: 308, Loss : 0.20372150105257303\n",
      "Iteration: 309, Loss : 0.2032242119400411\n",
      "Iteration: 310, Loss : 0.2027395403360855\n",
      "Iteration: 311, Loss : 0.2022165945243493\n",
      "Iteration: 312, Loss : 0.20171351918495445\n",
      "Iteration: 313, Loss : 0.20110448600902892\n",
      "Iteration: 314, Loss : 0.2005627364695057\n",
      "Iteration: 315, Loss : 0.20000644677985746\n",
      "Iteration: 316, Loss : 0.19945360615794655\n",
      "Iteration: 317, Loss : 0.19898443179913206\n",
      "Iteration: 318, Loss : 0.1984679654869222\n",
      "Iteration: 319, Loss : 0.1980007990065362\n",
      "Iteration: 320, Loss : 0.19749579421211155\n",
      "Iteration: 321, Loss : 0.1970307222713482\n",
      "Iteration: 322, Loss : 0.19657314350432525\n",
      "Iteration: 323, Loss : 0.19613354110533818\n",
      "Iteration: 324, Loss : 0.19565093447660126\n",
      "Iteration: 325, Loss : 0.1951725961205888\n",
      "Iteration: 326, Loss : 0.1946417043312526\n",
      "Iteration: 327, Loss : 0.19414735875633066\n",
      "Iteration: 328, Loss : 0.19368767392654965\n",
      "Iteration: 329, Loss : 0.19324085195302992\n",
      "Iteration: 330, Loss : 0.1927858028346351\n",
      "Iteration: 331, Loss : 0.19233449941294423\n",
      "Iteration: 332, Loss : 0.19185226895105606\n",
      "Iteration: 333, Loss : 0.19134380791571542\n",
      "Iteration: 334, Loss : 0.1909417352120541\n",
      "Iteration: 335, Loss : 0.19051598552750712\n",
      "Iteration: 336, Loss : 0.19013290713269745\n",
      "Iteration: 337, Loss : 0.18970260852095533\n",
      "Iteration: 338, Loss : 0.1893098441032026\n",
      "Iteration: 339, Loss : 0.18890629830383884\n",
      "Iteration: 340, Loss : 0.18838765103414024\n",
      "Iteration: 341, Loss : 0.18790859614657351\n",
      "Iteration: 342, Loss : 0.1874663742625904\n",
      "Iteration: 343, Loss : 0.18714350992839723\n",
      "Iteration: 344, Loss : 0.18676789344961622\n",
      "Iteration: 345, Loss : 0.18641584652653162\n",
      "Iteration: 346, Loss : 0.18601754378151292\n",
      "Iteration: 347, Loss : 0.18562457472607452\n",
      "Iteration: 348, Loss : 0.18527445968538697\n",
      "Iteration: 349, Loss : 0.18491951632772832\n",
      "Iteration: 350, Loss : 0.18448799952523381\n",
      "Iteration: 351, Loss : 0.18404390492553638\n",
      "Iteration: 352, Loss : 0.18363942809760972\n",
      "Iteration: 353, Loss : 0.18333771722701725\n",
      "Iteration: 354, Loss : 0.18299756579238505\n",
      "Iteration: 355, Loss : 0.1826226025033157\n",
      "Iteration: 356, Loss : 0.1822624266175653\n",
      "Iteration: 357, Loss : 0.18190516099838655\n",
      "Iteration: 358, Loss : 0.18157810246535955\n",
      "Iteration: 359, Loss : 0.1811944595543707\n",
      "Iteration: 360, Loss : 0.18090679028533685\n",
      "Iteration: 361, Loss : 0.18055724264728096\n",
      "Iteration: 362, Loss : 0.18018606972729634\n",
      "Iteration: 363, Loss : 0.17991198441685752\n",
      "Iteration: 364, Loss : 0.17957827005607296\n",
      "Iteration: 365, Loss : 0.1793035329405463\n",
      "Iteration: 366, Loss : 0.17898997070684164\n",
      "Iteration: 367, Loss : 0.17870443175916417\n",
      "Iteration: 368, Loss : 0.17835497736484207\n",
      "Iteration: 369, Loss : 0.1779801605973961\n",
      "Iteration: 370, Loss : 0.1776423744302798\n",
      "Iteration: 371, Loss : 0.17734643253529922\n",
      "Iteration: 372, Loss : 0.17708469053305906\n",
      "Iteration: 373, Loss : 0.17679876341863976\n",
      "Iteration: 374, Loss : 0.17648394183690652\n",
      "Iteration: 375, Loss : 0.17615454846677583\n",
      "Iteration: 376, Loss : 0.17584438293678525\n",
      "Iteration: 377, Loss : 0.1755924980130884\n",
      "Iteration: 378, Loss : 0.17530268520022702\n",
      "Iteration: 379, Loss : 0.175022575214347\n",
      "Iteration: 380, Loss : 0.17476038830254853\n",
      "Iteration: 381, Loss : 0.17446529996919696\n",
      "Iteration: 382, Loss : 0.17420099205093825\n",
      "Iteration: 383, Loss : 0.1739182657255472\n",
      "Iteration: 384, Loss : 0.17361973229559177\n",
      "Iteration: 385, Loss : 0.17335815305918462\n",
      "Iteration: 386, Loss : 0.17308072133605876\n",
      "Iteration: 387, Loss : 0.17282752919135358\n",
      "Iteration: 388, Loss : 0.17255907202176163\n",
      "Iteration: 389, Loss : 0.17228476615036098\n",
      "Iteration: 390, Loss : 0.17197150611223833\n",
      "Iteration: 391, Loss : 0.1717216674879123\n",
      "Iteration: 392, Loss : 0.17145902929333012\n",
      "Iteration: 393, Loss : 0.17114953795300272\n",
      "Iteration: 394, Loss : 0.1708781483863585\n",
      "Iteration: 395, Loss : 0.1706270477311438\n",
      "Iteration: 396, Loss : 0.17036169492242587\n",
      "Iteration: 397, Loss : 0.17007895822439867\n",
      "Iteration: 398, Loss : 0.16983250043795187\n",
      "Iteration: 399, Loss : 0.16955312235022274\n",
      "Iteration: 400, Loss : 0.16929273024374653\n",
      "Iteration: 401, Loss : 0.16904936392980963\n",
      "Iteration: 402, Loss : 0.16877297835317678\n",
      "Iteration: 403, Loss : 0.1685215698571353\n",
      "Iteration: 404, Loss : 0.16826816856864452\n",
      "Iteration: 405, Loss : 0.16801799846272267\n",
      "Iteration: 406, Loss : 0.1677460760173867\n",
      "Iteration: 407, Loss : 0.16748901736863225\n",
      "Iteration: 408, Loss : 0.16724112697321628\n",
      "Iteration: 409, Loss : 0.1669707424646688\n",
      "Iteration: 410, Loss : 0.16671195874265307\n",
      "Iteration: 411, Loss : 0.16646489858981597\n",
      "Iteration: 412, Loss : 0.16620525065238875\n",
      "Iteration: 413, Loss : 0.16596401788151957\n",
      "Iteration: 414, Loss : 0.16569494172334104\n",
      "Iteration: 415, Loss : 0.16545528738241738\n",
      "Iteration: 416, Loss : 0.16520668773844427\n",
      "Iteration: 417, Loss : 0.16492271906454498\n",
      "Iteration: 418, Loss : 0.16465448189814\n",
      "Iteration: 419, Loss : 0.16440785839047117\n",
      "Iteration: 420, Loss : 0.164149020132886\n",
      "Iteration: 421, Loss : 0.16388676797111565\n",
      "Iteration: 422, Loss : 0.1636287350091336\n",
      "Iteration: 423, Loss : 0.16336913789195787\n",
      "Iteration: 424, Loss : 0.16311574143060262\n",
      "Iteration: 425, Loss : 0.1628523372580768\n",
      "Iteration: 426, Loss : 0.16259694664939786\n",
      "Iteration: 427, Loss : 0.16233640070864286\n",
      "Iteration: 428, Loss : 0.16207723110017289\n",
      "Iteration: 429, Loss : 0.1618182067676383\n",
      "Iteration: 430, Loss : 0.16154210242826167\n",
      "Iteration: 431, Loss : 0.1612809783052891\n",
      "Iteration: 432, Loss : 0.1610141843522066\n",
      "Iteration: 433, Loss : 0.16074430778909982\n",
      "Iteration: 434, Loss : 0.16047975360648767\n",
      "Iteration: 435, Loss : 0.1602157097695857\n",
      "Iteration: 436, Loss : 0.15994798352947243\n",
      "Iteration: 437, Loss : 0.1596816132283328\n",
      "Iteration: 438, Loss : 0.15941513552391798\n",
      "Iteration: 439, Loss : 0.15915207226928357\n",
      "Iteration: 440, Loss : 0.1588855889208788\n",
      "Iteration: 441, Loss : 0.15859498389674803\n",
      "Iteration: 442, Loss : 0.1583200713538972\n",
      "Iteration: 443, Loss : 0.15805690114138368\n",
      "Iteration: 444, Loss : 0.1577798914149858\n",
      "Iteration: 445, Loss : 0.1575039038745305\n",
      "Iteration: 446, Loss : 0.1572194575965541\n",
      "Iteration: 447, Loss : 0.1569430933198619\n",
      "Iteration: 448, Loss : 0.15665692736145406\n",
      "Iteration: 449, Loss : 0.1563835959831022\n",
      "Iteration: 450, Loss : 0.15610231002481656\n",
      "Iteration: 451, Loss : 0.1558100211627928\n",
      "Iteration: 452, Loss : 0.155514582464121\n",
      "Iteration: 453, Loss : 0.15522904551816735\n",
      "Iteration: 454, Loss : 0.15495898809576011\n",
      "Iteration: 455, Loss : 0.15467818528174065\n",
      "Iteration: 456, Loss : 0.15438725120549157\n",
      "Iteration: 457, Loss : 0.15408117621170686\n",
      "Iteration: 458, Loss : 0.15380050414184307\n",
      "Iteration: 459, Loss : 0.1534991678264866\n",
      "Iteration: 460, Loss : 0.1532081791162402\n",
      "Iteration: 461, Loss : 0.15289629304008473\n",
      "Iteration: 462, Loss : 0.15259124299496585\n",
      "Iteration: 463, Loss : 0.1523079417841132\n",
      "Iteration: 464, Loss : 0.15200612845379644\n",
      "Iteration: 465, Loss : 0.15169578452523957\n",
      "Iteration: 466, Loss : 0.15141346706319397\n",
      "Iteration: 467, Loss : 0.1510996841599393\n",
      "Iteration: 468, Loss : 0.15079791240468798\n",
      "Iteration: 469, Loss : 0.15050626176135878\n",
      "Iteration: 470, Loss : 0.15018537915482177\n",
      "Iteration: 471, Loss : 0.14985366806456454\n",
      "Iteration: 472, Loss : 0.14953995137551288\n",
      "Iteration: 473, Loss : 0.14922710294412977\n",
      "Iteration: 474, Loss : 0.14892727938150574\n",
      "Iteration: 475, Loss : 0.14860561864362612\n",
      "Iteration: 476, Loss : 0.14831003313870347\n",
      "Iteration: 477, Loss : 0.14800495310443557\n",
      "Iteration: 478, Loss : 0.14765713836875022\n",
      "Iteration: 479, Loss : 0.14735755517864163\n",
      "Iteration: 480, Loss : 0.14701168392035788\n",
      "Iteration: 481, Loss : 0.14665821015582606\n",
      "Iteration: 482, Loss : 0.14635593653700626\n",
      "Iteration: 483, Loss : 0.1460099348542701\n",
      "Iteration: 484, Loss : 0.1456900237043908\n",
      "Iteration: 485, Loss : 0.14537752404620602\n",
      "Iteration: 486, Loss : 0.1450377404492436\n",
      "Iteration: 487, Loss : 0.14468152833422307\n",
      "Iteration: 488, Loss : 0.14434960526237794\n",
      "Iteration: 489, Loss : 0.1440304399060003\n",
      "Iteration: 490, Loss : 0.1436789627996295\n",
      "Iteration: 491, Loss : 0.1433541014298932\n",
      "Iteration: 492, Loss : 0.1429957618505278\n",
      "Iteration: 493, Loss : 0.14266505204404034\n",
      "Iteration: 494, Loss : 0.14232536332754722\n",
      "Iteration: 495, Loss : 0.14193572325307602\n",
      "Iteration: 496, Loss : 0.14159924129153478\n",
      "Iteration: 497, Loss : 0.14121741512456198\n",
      "Iteration: 498, Loss : 0.1408619200553911\n",
      "Iteration: 499, Loss : 0.1404768289585185\n",
      "Iteration: 500, Loss : 0.14014197312496082\n",
      "Iteration: 501, Loss : 0.13976392592043949\n",
      "Iteration: 502, Loss : 0.13939711947967257\n",
      "Iteration: 503, Loss : 0.13904284716674853\n",
      "Iteration: 504, Loss : 0.1386749779389286\n",
      "Iteration: 505, Loss : 0.13828165868213685\n",
      "Iteration: 506, Loss : 0.13792202390970457\n",
      "Iteration: 507, Loss : 0.13755093614641756\n",
      "Iteration: 508, Loss : 0.13720661900979972\n",
      "Iteration: 509, Loss : 0.13681448932916412\n",
      "Iteration: 510, Loss : 0.13644129780223374\n",
      "Iteration: 511, Loss : 0.13603008181053922\n",
      "Iteration: 512, Loss : 0.13568043974269256\n",
      "Iteration: 513, Loss : 0.13532418830816884\n",
      "Iteration: 514, Loss : 0.13490936764036898\n",
      "Iteration: 515, Loss : 0.13454951212812777\n",
      "Iteration: 516, Loss : 0.13415395513025843\n",
      "Iteration: 517, Loss : 0.1337251322030056\n",
      "Iteration: 518, Loss : 0.13334352828319007\n",
      "Iteration: 519, Loss : 0.13294950387928478\n",
      "Iteration: 520, Loss : 0.1325265860845703\n",
      "Iteration: 521, Loss : 0.1321363339864397\n",
      "Iteration: 522, Loss : 0.131738899177606\n",
      "Iteration: 523, Loss : 0.13132960607705746\n",
      "Iteration: 524, Loss : 0.1309651204875759\n",
      "Iteration: 525, Loss : 0.13058830352637557\n",
      "Iteration: 526, Loss : 0.1302006790801412\n",
      "Iteration: 527, Loss : 0.12980493960217357\n",
      "Iteration: 528, Loss : 0.12937520269020392\n",
      "Iteration: 529, Loss : 0.12894648596559932\n",
      "Iteration: 530, Loss : 0.12848828263360879\n",
      "Iteration: 531, Loss : 0.12805846007262084\n",
      "Iteration: 532, Loss : 0.12765795530992585\n",
      "Iteration: 533, Loss : 0.1272236807209796\n",
      "Iteration: 534, Loss : 0.12684190673212017\n",
      "Iteration: 535, Loss : 0.12644330710146612\n",
      "Iteration: 536, Loss : 0.1259948833890335\n",
      "Iteration: 537, Loss : 0.12554745755783026\n",
      "Iteration: 538, Loss : 0.12508967450492448\n",
      "Iteration: 539, Loss : 0.12466324891044314\n",
      "Iteration: 540, Loss : 0.12426663195359111\n",
      "Iteration: 541, Loss : 0.12384078568322543\n",
      "Iteration: 542, Loss : 0.1233735837236613\n",
      "Iteration: 543, Loss : 0.12295884110991347\n",
      "Iteration: 544, Loss : 0.12252345780749428\n",
      "Iteration: 545, Loss : 0.1220906791914296\n",
      "Iteration: 546, Loss : 0.12168433707982018\n",
      "Iteration: 547, Loss : 0.12126683785365086\n",
      "Iteration: 548, Loss : 0.12076942489735834\n",
      "Iteration: 549, Loss : 0.12030517016036206\n",
      "Iteration: 550, Loss : 0.1198963545283175\n",
      "Iteration: 551, Loss : 0.1194267448497438\n",
      "Iteration: 552, Loss : 0.11896024101123154\n",
      "Iteration: 553, Loss : 0.11849571886706796\n",
      "Iteration: 554, Loss : 0.11804065973983428\n",
      "Iteration: 555, Loss : 0.1175676785733369\n",
      "Iteration: 556, Loss : 0.11716650197612197\n",
      "Iteration: 557, Loss : 0.11668292567585545\n",
      "Iteration: 558, Loss : 0.1162312276352401\n",
      "Iteration: 559, Loss : 0.11578462796873984\n",
      "Iteration: 560, Loss : 0.11534797245040751\n",
      "Iteration: 561, Loss : 0.11489448053665763\n",
      "Iteration: 562, Loss : 0.1143955125499729\n",
      "Iteration: 563, Loss : 0.11395446822872274\n",
      "Iteration: 564, Loss : 0.11351501300163969\n",
      "Iteration: 565, Loss : 0.11304303395801985\n",
      "Iteration: 566, Loss : 0.11259498376394474\n",
      "Iteration: 567, Loss : 0.11209511541494527\n",
      "Iteration: 568, Loss : 0.11159775551888709\n",
      "Iteration: 569, Loss : 0.1111427501315492\n",
      "Iteration: 570, Loss : 0.11065777561472596\n",
      "Iteration: 571, Loss : 0.11016326924143925\n",
      "Iteration: 572, Loss : 0.10972023146085062\n",
      "Iteration: 573, Loss : 0.10930721705532336\n",
      "Iteration: 574, Loss : 0.1088820868821371\n",
      "Iteration: 575, Loss : 0.1084397288836474\n",
      "Iteration: 576, Loss : 0.10788105694184577\n",
      "Iteration: 577, Loss : 0.10741021834985412\n",
      "Iteration: 578, Loss : 0.10697292736723817\n",
      "Iteration: 579, Loss : 0.10643844745843038\n",
      "Iteration: 580, Loss : 0.10592428838351513\n",
      "Iteration: 581, Loss : 0.10549543819864243\n",
      "Iteration: 582, Loss : 0.10498994692467625\n",
      "Iteration: 583, Loss : 0.10455798784340511\n",
      "Iteration: 584, Loss : 0.10404645044099001\n",
      "Iteration: 585, Loss : 0.10358114475848074\n",
      "Iteration: 586, Loss : 0.10313583311507077\n",
      "Iteration: 587, Loss : 0.10267388670763594\n",
      "Iteration: 588, Loss : 0.10211588020651705\n",
      "Iteration: 589, Loss : 0.101673175177417\n",
      "Iteration: 590, Loss : 0.10116329363699776\n",
      "Iteration: 591, Loss : 0.10064627205630039\n",
      "Iteration: 592, Loss : 0.10020536239853217\n",
      "Iteration: 593, Loss : 0.09974426829569932\n",
      "Iteration: 594, Loss : 0.09928056374531631\n",
      "Iteration: 595, Loss : 0.09871631114823858\n",
      "Iteration: 596, Loss : 0.09820273176129425\n",
      "Iteration: 597, Loss : 0.09771453274795851\n",
      "Iteration: 598, Loss : 0.09721059105410362\n",
      "Iteration: 599, Loss : 0.09671209780512197\n",
      "Iteration: 600, Loss : 0.09622747471258733\n",
      "Iteration: 601, Loss : 0.09577716812061216\n",
      "Iteration: 602, Loss : 0.09528282110500064\n",
      "Iteration: 603, Loss : 0.09483188441703969\n",
      "Iteration: 604, Loss : 0.0943252701858733\n",
      "Iteration: 605, Loss : 0.093865898745115\n",
      "Iteration: 606, Loss : 0.09335130334983172\n",
      "Iteration: 607, Loss : 0.09285266735140864\n",
      "Iteration: 608, Loss : 0.09239398713890465\n",
      "Iteration: 609, Loss : 0.09193171232672889\n",
      "Iteration: 610, Loss : 0.0914673067735036\n",
      "Iteration: 611, Loss : 0.0909914931998367\n",
      "Iteration: 612, Loss : 0.09045041484135412\n",
      "Iteration: 613, Loss : 0.08992414012494769\n",
      "Iteration: 614, Loss : 0.08944392551084773\n",
      "Iteration: 615, Loss : 0.08896456566715877\n",
      "Iteration: 616, Loss : 0.08845930996650159\n",
      "Iteration: 617, Loss : 0.08799137547291437\n",
      "Iteration: 618, Loss : 0.0875288313196845\n",
      "Iteration: 619, Loss : 0.08703546080193279\n",
      "Iteration: 620, Loss : 0.08650396787328418\n",
      "Iteration: 621, Loss : 0.085976570933048\n",
      "Iteration: 622, Loss : 0.08553421587361455\n",
      "Iteration: 623, Loss : 0.08508027370694951\n",
      "Iteration: 624, Loss : 0.08461977214749308\n",
      "Iteration: 625, Loss : 0.08414369934002784\n",
      "Iteration: 626, Loss : 0.08358567850280357\n",
      "Iteration: 627, Loss : 0.08311326640291145\n",
      "Iteration: 628, Loss : 0.08260229632580018\n",
      "Iteration: 629, Loss : 0.08213183115827148\n",
      "Iteration: 630, Loss : 0.08162804163702976\n",
      "Iteration: 631, Loss : 0.08116066476313638\n",
      "Iteration: 632, Loss : 0.08066734834624839\n",
      "Iteration: 633, Loss : 0.08017917681100586\n",
      "Iteration: 634, Loss : 0.07969330287301114\n",
      "Iteration: 635, Loss : 0.07920709270329135\n",
      "Iteration: 636, Loss : 0.07870571176888114\n",
      "Iteration: 637, Loss : 0.07827253093333639\n",
      "Iteration: 638, Loss : 0.07781501133609602\n",
      "Iteration: 639, Loss : 0.07730454827511739\n",
      "Iteration: 640, Loss : 0.07684515837961453\n",
      "Iteration: 641, Loss : 0.0764098423088064\n",
      "Iteration: 642, Loss : 0.07596927536030357\n",
      "Iteration: 643, Loss : 0.0754564229298414\n",
      "Iteration: 644, Loss : 0.0749746840658253\n",
      "Iteration: 645, Loss : 0.07450439457983228\n",
      "Iteration: 646, Loss : 0.07400491015549832\n",
      "Iteration: 647, Loss : 0.07350939647716453\n",
      "Iteration: 648, Loss : 0.07305312906073465\n",
      "Iteration: 649, Loss : 0.07259115025915674\n",
      "Iteration: 650, Loss : 0.072168699683713\n",
      "Iteration: 651, Loss : 0.0716689701627813\n",
      "Iteration: 652, Loss : 0.07119790601695548\n",
      "Iteration: 653, Loss : 0.07071141734610342\n",
      "Iteration: 654, Loss : 0.07026247432804897\n",
      "Iteration: 655, Loss : 0.0697864236188266\n",
      "Iteration: 656, Loss : 0.06931637951809022\n",
      "Iteration: 657, Loss : 0.06885093276621211\n",
      "Iteration: 658, Loss : 0.0684360434981941\n",
      "Iteration: 659, Loss : 0.06794858628092638\n",
      "Iteration: 660, Loss : 0.06748472830005424\n",
      "Iteration: 661, Loss : 0.06705279815451509\n",
      "Iteration: 662, Loss : 0.0666303621675779\n",
      "Iteration: 663, Loss : 0.06621116194611709\n",
      "Iteration: 664, Loss : 0.06574283653773566\n",
      "Iteration: 665, Loss : 0.06528270731266975\n",
      "Iteration: 666, Loss : 0.06478888959469692\n",
      "Iteration: 667, Loss : 0.06432965336465873\n",
      "Iteration: 668, Loss : 0.06389831409878997\n",
      "Iteration: 669, Loss : 0.06344385063554638\n",
      "Iteration: 670, Loss : 0.06301166472028888\n",
      "Iteration: 671, Loss : 0.06258792380632362\n",
      "Iteration: 672, Loss : 0.06217122080128298\n",
      "Iteration: 673, Loss : 0.06169967780118342\n",
      "Iteration: 674, Loss : 0.061258138711740656\n",
      "Iteration: 675, Loss : 0.06083126608820759\n",
      "Iteration: 676, Loss : 0.060369969898928344\n",
      "Iteration: 677, Loss : 0.059973600549503396\n",
      "Iteration: 678, Loss : 0.059569687979670896\n",
      "Iteration: 679, Loss : 0.05909892854854122\n",
      "Iteration: 680, Loss : 0.05869779946240912\n",
      "Iteration: 681, Loss : 0.05826455907999283\n",
      "Iteration: 682, Loss : 0.05781851321128601\n",
      "Iteration: 683, Loss : 0.0573545766266611\n",
      "Iteration: 684, Loss : 0.056957248867956\n",
      "Iteration: 685, Loss : 0.056503276853316964\n",
      "Iteration: 686, Loss : 0.05610581348727377\n",
      "Iteration: 687, Loss : 0.05567876793054588\n",
      "Iteration: 688, Loss : 0.055280817894576076\n",
      "Iteration: 689, Loss : 0.05485296894208968\n",
      "Iteration: 690, Loss : 0.054408848341325804\n",
      "Iteration: 691, Loss : 0.053963280235680755\n",
      "Iteration: 692, Loss : 0.05354158583333319\n",
      "Iteration: 693, Loss : 0.053125988445996766\n",
      "Iteration: 694, Loss : 0.052718155479464124\n",
      "Iteration: 695, Loss : 0.052305760115634356\n",
      "Iteration: 696, Loss : 0.05189626207894231\n",
      "Iteration: 697, Loss : 0.05152525135473953\n",
      "Iteration: 698, Loss : 0.05108824780984032\n",
      "Iteration: 699, Loss : 0.05072240632994855\n",
      "Iteration: 700, Loss : 0.050345195973290346\n",
      "Iteration: 701, Loss : 0.04993097103732351\n",
      "Iteration: 702, Loss : 0.04949682001744143\n",
      "Iteration: 703, Loss : 0.04909787128717369\n",
      "Iteration: 704, Loss : 0.048683809926042695\n",
      "Iteration: 705, Loss : 0.04827397482441595\n",
      "Iteration: 706, Loss : 0.047876518274190205\n",
      "Iteration: 707, Loss : 0.04749649142407808\n",
      "Iteration: 708, Loss : 0.047102323400191805\n",
      "Iteration: 709, Loss : 0.04665036683822578\n",
      "Iteration: 710, Loss : 0.04626642778162453\n",
      "Iteration: 711, Loss : 0.0458326613370978\n",
      "Iteration: 712, Loss : 0.045408942797097035\n",
      "Iteration: 713, Loss : 0.04497991268385196\n",
      "Iteration: 714, Loss : 0.0445819309810377\n",
      "Iteration: 715, Loss : 0.04417906610284505\n",
      "Iteration: 716, Loss : 0.043745785820503875\n",
      "Iteration: 717, Loss : 0.043315688628876046\n",
      "Iteration: 718, Loss : 0.04292121031390583\n",
      "Iteration: 719, Loss : 0.04251718421046902\n",
      "Iteration: 720, Loss : 0.042104181501576854\n",
      "Iteration: 721, Loss : 0.04164279134471002\n",
      "Iteration: 722, Loss : 0.04122348772206029\n",
      "Iteration: 723, Loss : 0.040804401534442404\n",
      "Iteration: 724, Loss : 0.040381586964643224\n",
      "Iteration: 725, Loss : 0.0399403045438948\n",
      "Iteration: 726, Loss : 0.03946631408501941\n",
      "Iteration: 727, Loss : 0.03905866991149484\n",
      "Iteration: 728, Loss : 0.03857460244701537\n",
      "Iteration: 729, Loss : 0.03812120041866774\n",
      "Iteration: 730, Loss : 0.03764663160862497\n",
      "Iteration: 731, Loss : 0.03718747605730415\n",
      "Iteration: 732, Loss : 0.036726192217922685\n",
      "Iteration: 733, Loss : 0.036268422400340894\n",
      "Iteration: 734, Loss : 0.03579605395495284\n",
      "Iteration: 735, Loss : 0.035323415216649764\n",
      "Iteration: 736, Loss : 0.03489236449732736\n",
      "Iteration: 737, Loss : 0.034432714299142325\n",
      "Iteration: 738, Loss : 0.03400799745859136\n",
      "Iteration: 739, Loss : 0.03351181594965723\n",
      "Iteration: 740, Loss : 0.033052884124157994\n",
      "Iteration: 741, Loss : 0.032589426035358596\n",
      "Iteration: 742, Loss : 0.032089341285035045\n",
      "Iteration: 743, Loss : 0.031606254987696064\n",
      "Iteration: 744, Loss : 0.031115112306500003\n",
      "Iteration: 745, Loss : 0.03062496842311549\n",
      "Iteration: 746, Loss : 0.03020129846610634\n",
      "Iteration: 747, Loss : 0.029735154072547725\n",
      "Iteration: 748, Loss : 0.029254056689796196\n",
      "Iteration: 749, Loss : 0.028821256319782954\n",
      "Iteration: 750, Loss : 0.028356064537730614\n",
      "Iteration: 751, Loss : 0.02791849937628818\n",
      "Iteration: 752, Loss : 0.027447792482536874\n",
      "Iteration: 753, Loss : 0.026961475426186765\n",
      "Iteration: 754, Loss : 0.026511105170106016\n",
      "Iteration: 755, Loss : 0.02605538085818381\n",
      "Iteration: 756, Loss : 0.02562581862355385\n",
      "Iteration: 757, Loss : 0.025236316721273664\n",
      "Iteration: 758, Loss : 0.024851628363535716\n",
      "Iteration: 759, Loss : 0.024436679935035288\n",
      "Iteration: 760, Loss : 0.024016350840532802\n",
      "Iteration: 761, Loss : 0.023638250276466568\n",
      "Iteration: 762, Loss : 0.023251181810744807\n",
      "Iteration: 763, Loss : 0.022853190587626013\n",
      "Iteration: 764, Loss : 0.022488977886279037\n",
      "Iteration: 765, Loss : 0.022153016290564775\n",
      "Iteration: 766, Loss : 0.02178077831801943\n",
      "Iteration: 767, Loss : 0.021456492572830103\n",
      "Iteration: 768, Loss : 0.021097281779291654\n",
      "Iteration: 769, Loss : 0.02075240039830172\n",
      "Iteration: 770, Loss : 0.020415411216256427\n",
      "Iteration: 771, Loss : 0.020102237624477083\n",
      "Iteration: 772, Loss : 0.019803724634121736\n",
      "Iteration: 773, Loss : 0.0194967765051131\n",
      "Iteration: 774, Loss : 0.019184423563259796\n",
      "Iteration: 775, Loss : 0.018875630833237206\n",
      "Iteration: 776, Loss : 0.0185833510039771\n",
      "Iteration: 777, Loss : 0.01832804530740284\n",
      "Iteration: 778, Loss : 0.018024982063166457\n",
      "Iteration: 779, Loss : 0.0177355849162819\n",
      "Iteration: 780, Loss : 0.017494565496604865\n",
      "Iteration: 781, Loss : 0.017211471358890278\n",
      "Iteration: 782, Loss : 0.016966280701459745\n",
      "Iteration: 783, Loss : 0.01669123377975578\n",
      "Iteration: 784, Loss : 0.01645407366465829\n",
      "Iteration: 785, Loss : 0.016223801425764465\n",
      "Iteration: 786, Loss : 0.015953421221211943\n",
      "Iteration: 787, Loss : 0.015733114583551536\n",
      "Iteration: 788, Loss : 0.015494200806473794\n",
      "Iteration: 789, Loss : 0.015273022214722907\n",
      "Iteration: 790, Loss : 0.015029571263325162\n",
      "Iteration: 791, Loss : 0.014812291336077353\n",
      "Iteration: 792, Loss : 0.014561019546452187\n",
      "Iteration: 793, Loss : 0.014356382543085067\n",
      "Iteration: 794, Loss : 0.01412735093895716\n",
      "Iteration: 795, Loss : 0.013902105669034468\n",
      "Iteration: 796, Loss : 0.013685569082441024\n",
      "Iteration: 797, Loss : 0.013471392520815669\n",
      "Iteration: 798, Loss : 0.013261403782803407\n",
      "Iteration: 799, Loss : 0.013070570079130919\n",
      "Iteration: 800, Loss : 0.01286981163606581\n",
      "Iteration: 801, Loss : 0.012679541732022405\n",
      "Iteration: 802, Loss : 0.012479255364073124\n",
      "Iteration: 803, Loss : 0.012301990410026869\n",
      "Iteration: 804, Loss : 0.012126326218774046\n",
      "Iteration: 805, Loss : 0.011927475480610454\n",
      "Iteration: 806, Loss : 0.011737253027069737\n",
      "Iteration: 807, Loss : 0.011571157511237764\n",
      "Iteration: 808, Loss : 0.011388500042392913\n",
      "Iteration: 809, Loss : 0.01121007402221261\n",
      "Iteration: 810, Loss : 0.01104594543903546\n",
      "Iteration: 811, Loss : 0.010875933087543636\n",
      "Iteration: 812, Loss : 0.010696015144303999\n",
      "Iteration: 813, Loss : 0.010537341167290293\n",
      "Iteration: 814, Loss : 0.01036310566081893\n",
      "Iteration: 815, Loss : 0.010211717614140251\n",
      "Iteration: 816, Loss : 0.01004524866332339\n",
      "Iteration: 817, Loss : 0.00988458048451027\n",
      "Iteration: 818, Loss : 0.009727222542043012\n",
      "Iteration: 819, Loss : 0.009585797672429185\n",
      "Iteration: 820, Loss : 0.009433972895262387\n",
      "Iteration: 821, Loss : 0.009291783007246987\n",
      "Iteration: 822, Loss : 0.009138118528692198\n",
      "Iteration: 823, Loss : 0.008993134097731956\n",
      "Iteration: 824, Loss : 0.008864369860505646\n",
      "Iteration: 825, Loss : 0.008719034865385543\n",
      "Iteration: 826, Loss : 0.008593771022007209\n",
      "Iteration: 827, Loss : 0.008467876170885413\n",
      "Iteration: 828, Loss : 0.008330437096324774\n",
      "Iteration: 829, Loss : 0.00820075632896361\n",
      "Iteration: 830, Loss : 0.008065066927614037\n",
      "Iteration: 831, Loss : 0.007942721926399509\n",
      "Iteration: 832, Loss : 0.007823434168468254\n",
      "Iteration: 833, Loss : 0.007701138845928386\n",
      "Iteration: 834, Loss : 0.007584484998838502\n",
      "Iteration: 835, Loss : 0.007459695450833467\n",
      "Iteration: 836, Loss : 0.007333795404293984\n",
      "Iteration: 837, Loss : 0.007213903519262199\n",
      "Iteration: 838, Loss : 0.007099167842478776\n",
      "Iteration: 839, Loss : 0.0069868320405736765\n",
      "Iteration: 840, Loss : 0.006882314068403044\n",
      "Iteration: 841, Loss : 0.006770522799607619\n",
      "Iteration: 842, Loss : 0.0066676802895649645\n",
      "Iteration: 843, Loss : 0.006560948541066938\n",
      "Iteration: 844, Loss : 0.006453981726325755\n",
      "Iteration: 845, Loss : 0.006347597336133777\n",
      "Iteration: 846, Loss : 0.006252561171129334\n",
      "Iteration: 847, Loss : 0.006147641486933831\n",
      "Iteration: 848, Loss : 0.0060577307066349865\n",
      "Iteration: 849, Loss : 0.005957533440245654\n",
      "Iteration: 850, Loss : 0.005864770957267073\n",
      "Iteration: 851, Loss : 0.005771411128309565\n",
      "Iteration: 852, Loss : 0.005677507719822195\n",
      "Iteration: 853, Loss : 0.005586692704135949\n",
      "Iteration: 854, Loss : 0.005497645804971293\n",
      "Iteration: 855, Loss : 0.005414689353456232\n",
      "Iteration: 856, Loss : 0.005330457570834635\n",
      "Iteration: 857, Loss : 0.005245990725500493\n",
      "Iteration: 858, Loss : 0.005158328593324459\n",
      "Iteration: 859, Loss : 0.00507612549861545\n",
      "Iteration: 860, Loss : 0.00499669036444255\n",
      "Iteration: 861, Loss : 0.004922783163148049\n",
      "Iteration: 862, Loss : 0.004839553880131287\n",
      "Iteration: 863, Loss : 0.004766543812195791\n",
      "Iteration: 864, Loss : 0.004690971306832594\n",
      "Iteration: 865, Loss : 0.004610274575720695\n",
      "Iteration: 866, Loss : 0.004538762223461789\n",
      "Iteration: 867, Loss : 0.0044631660182130155\n",
      "Iteration: 868, Loss : 0.004397225474238384\n",
      "Iteration: 869, Loss : 0.004330798893186868\n",
      "Iteration: 870, Loss : 0.004256182812790589\n",
      "Iteration: 871, Loss : 0.004187322661750887\n",
      "Iteration: 872, Loss : 0.004116782229069417\n",
      "Iteration: 873, Loss : 0.00405108293421789\n",
      "Iteration: 874, Loss : 0.0039869323783493435\n",
      "Iteration: 875, Loss : 0.003921746842037822\n",
      "Iteration: 876, Loss : 0.003862429898601472\n",
      "Iteration: 877, Loss : 0.0037990468477174678\n",
      "Iteration: 878, Loss : 0.003743056362621607\n",
      "Iteration: 879, Loss : 0.0036816589004730173\n",
      "Iteration: 880, Loss : 0.0036203651415373456\n",
      "Iteration: 881, Loss : 0.0035637338630817206\n",
      "Iteration: 882, Loss : 0.003503802242019408\n",
      "Iteration: 883, Loss : 0.0034460085787352446\n",
      "Iteration: 884, Loss : 0.0033928516793755867\n",
      "Iteration: 885, Loss : 0.0033378210137879737\n",
      "Iteration: 886, Loss : 0.0032879732212553097\n",
      "Iteration: 887, Loss : 0.0032324273631851985\n",
      "Iteration: 888, Loss : 0.0031775048743551324\n",
      "Iteration: 889, Loss : 0.0031258570000445964\n",
      "Iteration: 890, Loss : 0.0030763117967770874\n",
      "Iteration: 891, Loss : 0.0030254099325587426\n",
      "Iteration: 892, Loss : 0.002977992082122147\n",
      "Iteration: 893, Loss : 0.0029302209357882645\n",
      "Iteration: 894, Loss : 0.00288788991148798\n",
      "Iteration: 895, Loss : 0.002844119613587608\n",
      "Iteration: 896, Loss : 0.0028007072577957083\n",
      "Iteration: 897, Loss : 0.0027504921556733004\n",
      "Iteration: 898, Loss : 0.0027083223900762204\n",
      "Iteration: 899, Loss : 0.0026646290541521836\n",
      "Iteration: 900, Loss : 0.002619723496517278\n",
      "Iteration: 901, Loss : 0.002576521274722296\n",
      "Iteration: 902, Loss : 0.002534482247577478\n",
      "Iteration: 903, Loss : 0.0024954432178175123\n",
      "Iteration: 904, Loss : 0.0024578353251045433\n",
      "Iteration: 905, Loss : 0.002417414864786948\n",
      "Iteration: 906, Loss : 0.0023801858758910575\n",
      "Iteration: 907, Loss : 0.0023418263166446805\n",
      "Iteration: 908, Loss : 0.0023013751140114245\n",
      "Iteration: 909, Loss : 0.0022623678200118045\n",
      "Iteration: 910, Loss : 0.0022272545800164022\n",
      "Iteration: 911, Loss : 0.0021932067758637565\n",
      "Iteration: 912, Loss : 0.0021581011187331175\n",
      "Iteration: 913, Loss : 0.0021227012908320328\n",
      "Iteration: 914, Loss : 0.0020864152307864233\n",
      "Iteration: 915, Loss : 0.0020522562927817976\n",
      "Iteration: 916, Loss : 0.002018638055005077\n",
      "Iteration: 917, Loss : 0.0019845768805188757\n",
      "Iteration: 918, Loss : 0.0019553404276069844\n",
      "Iteration: 919, Loss : 0.001921828247350517\n",
      "Iteration: 920, Loss : 0.0018903506482256139\n",
      "Iteration: 921, Loss : 0.0018613834351419373\n",
      "Iteration: 922, Loss : 0.0018313448791057994\n",
      "Iteration: 923, Loss : 0.0018008067812850588\n",
      "Iteration: 924, Loss : 0.0017715036683123497\n",
      "Iteration: 925, Loss : 0.0017425597168704615\n",
      "Iteration: 926, Loss : 0.0017155912454619728\n",
      "Iteration: 927, Loss : 0.0016888009639364912\n",
      "Iteration: 928, Loss : 0.0016634846214507338\n",
      "Iteration: 929, Loss : 0.0016366378072192006\n",
      "Iteration: 930, Loss : 0.0016090111646182722\n",
      "Iteration: 931, Loss : 0.0015840972714560339\n",
      "Iteration: 932, Loss : 0.0015579735970923823\n",
      "Iteration: 933, Loss : 0.0015333468506006946\n",
      "Iteration: 934, Loss : 0.0015065946385184456\n",
      "Iteration: 935, Loss : 0.001483598326888921\n",
      "Iteration: 936, Loss : 0.0014601808640246018\n",
      "Iteration: 937, Loss : 0.0014351826474300526\n",
      "Iteration: 938, Loss : 0.0014116122470167083\n",
      "Iteration: 939, Loss : 0.0013905125773449277\n",
      "Iteration: 940, Loss : 0.0013670788065102651\n",
      "Iteration: 941, Loss : 0.0013461460951493189\n",
      "Iteration: 942, Loss : 0.001323760800382433\n",
      "Iteration: 943, Loss : 0.0013027917569053095\n",
      "Iteration: 944, Loss : 0.0012805318218647613\n",
      "Iteration: 945, Loss : 0.0012591605440402096\n",
      "Iteration: 946, Loss : 0.0012380604139568523\n",
      "Iteration: 947, Loss : 0.0012196666651672588\n",
      "Iteration: 948, Loss : 0.0012002932936461604\n",
      "Iteration: 949, Loss : 0.0011813456011513955\n",
      "Iteration: 950, Loss : 0.0011611512584190184\n",
      "Iteration: 951, Loss : 0.0011414193671251006\n",
      "Iteration: 952, Loss : 0.001123181491099884\n",
      "Iteration: 953, Loss : 0.001105193927917892\n",
      "Iteration: 954, Loss : 0.0010874013141343416\n",
      "Iteration: 955, Loss : 0.0010689841855488777\n",
      "Iteration: 956, Loss : 0.0010527705929303478\n",
      "Iteration: 957, Loss : 0.0010364753947720716\n",
      "Iteration: 958, Loss : 0.0010197820987420262\n",
      "Iteration: 959, Loss : 0.0010046044872455621\n",
      "Iteration: 960, Loss : 0.000986762908233193\n",
      "Iteration: 961, Loss : 0.0009720039046305814\n",
      "Iteration: 962, Loss : 0.0009559419289507478\n",
      "Iteration: 963, Loss : 0.0009412947182221294\n",
      "Iteration: 964, Loss : 0.0009255960054290174\n",
      "Iteration: 965, Loss : 0.0009099091193852648\n",
      "Iteration: 966, Loss : 0.0008947729190412234\n",
      "Iteration: 967, Loss : 0.000881308745994213\n",
      "Iteration: 968, Loss : 0.0008660006033549873\n",
      "Iteration: 969, Loss : 0.0008519893122527081\n",
      "Iteration: 970, Loss : 0.0008390904987048533\n",
      "Iteration: 971, Loss : 0.000826119726718823\n",
      "Iteration: 972, Loss : 0.0008121230313527148\n",
      "Iteration: 973, Loss : 0.0007983577626272159\n",
      "Iteration: 974, Loss : 0.0007858009001751626\n",
      "Iteration: 975, Loss : 0.0007733164555567434\n",
      "Iteration: 976, Loss : 0.0007609600986794175\n",
      "Iteration: 977, Loss : 0.0007478731930810555\n",
      "Iteration: 978, Loss : 0.0007357702645812425\n",
      "Iteration: 979, Loss : 0.0007242063805892416\n",
      "Iteration: 980, Loss : 0.0007131278215858491\n",
      "Iteration: 981, Loss : 0.0007010581682556851\n",
      "Iteration: 982, Loss : 0.000690082281230097\n",
      "Iteration: 983, Loss : 0.0006785687022928877\n",
      "Iteration: 984, Loss : 0.0006673261456710966\n",
      "Iteration: 985, Loss : 0.0006561544491429955\n",
      "Iteration: 986, Loss : 0.0006463794769721553\n",
      "Iteration: 987, Loss : 0.0006362736518605513\n",
      "Iteration: 988, Loss : 0.0006257670268733243\n",
      "Iteration: 989, Loss : 0.0006158458999562188\n",
      "Iteration: 990, Loss : 0.0006055333122640104\n",
      "Iteration: 991, Loss : 0.0005962635451772275\n",
      "Iteration: 992, Loss : 0.0005868546480713025\n",
      "Iteration: 993, Loss : 0.0005771298912308185\n",
      "Iteration: 994, Loss : 0.0005683963098479458\n",
      "Iteration: 995, Loss : 0.0005591072857411177\n",
      "Iteration: 996, Loss : 0.0005501754030733296\n",
      "Iteration: 997, Loss : 0.0005410949497282866\n",
      "Iteration: 998, Loss : 0.0005319667971063011\n",
      "Iteration: 999, Loss : 0.0005238796288863231\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.00636636],\n",
       "       [0.97875838],\n",
       "       [0.97868883],\n",
       "       [0.03390598]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
